id,blob,channel_id,end,published,start,text,title,url
35Pdoyi6ZoQ-t0.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 74, 'published': '2021-07-06 13:00:03 UTC', 'start': 0, 'text': ""Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,74,2021-07-06 13:00:03 UTC,0,"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t18.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 94, 'published': '2021-07-06 13:00:03 UTC', 'start': 18, 'text': ""So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,94,2021-07-06 13:00:03 UTC,18,"So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t32.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 108, 'published': '2021-07-06 13:00:03 UTC', 'start': 32, 'text': ""So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,108,2021-07-06 13:00:03 UTC,32,"So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t51.519999999999996,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 125, 'published': '2021-07-06 13:00:03 UTC', 'start': 51, 'text': ""PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,125,2021-07-06 13:00:03 UTC,51,"PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t67.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 140, 'published': '2021-07-06 13:00:03 UTC', 'start': 67, 'text': ""So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,140,2021-07-06 13:00:03 UTC,67,"So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well. Same thing with Bert as well. We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t83.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 153, 'published': '2021-07-06 13:00:03 UTC', 'start': 83, 'text': ""We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away. So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,153,2021-07-06 13:00:03 UTC,83,"We have our input IDs, attention mask, and our labels. Our input IDs have roughly 15% of their values masked. So we can see that here we have these two tensors. These are the labels. And we have the real tokens in here, the token IDs. And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away. So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t102.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 177, 'published': '2021-07-06 13:00:03 UTC', 'start': 102, 'text': ""And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away. So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,177,2021-07-06 13:00:03 UTC,102,"And then in our input IDs tensor, we have these being replaced with mask tokens, the number fours. So that's the structure of our input data. We've created a Torch data set from it and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away. So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t119.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 201, 'published': '2021-07-06 13:00:03 UTC', 'start': 119, 'text': ""and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away. So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config. And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,201,2021-07-06 13:00:03 UTC,119,"and use that to create a Torch data loader. And with that, we can actually begin setting up our model for training. So there are a few things to that. We can't just begin training straight away. So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config. And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t134.35999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 218, 'published': '2021-07-06 13:00:03 UTC', 'start': 134, 'text': ""So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config. And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it. In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,218,2021-07-06 13:00:03 UTC,134,"So the first thing that we need to do is create a Roberta config object. And the config object is something that we use when we're initializing a transformer from scratch in order to initialize it with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config. And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it. In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t149.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 228, 'published': '2021-07-06 13:00:03 UTC', 'start': 149, 'text': ""with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config. And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it. In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522. So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,228,2021-07-06 13:00:03 UTC,149,"with a certain set of parameters. So we'll do that first. So we want from transformers import Roberta config. And to create that config object, we do this. So we do Roberta config. And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it. In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522. So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t167.92000000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 251, 'published': '2021-07-06 13:00:03 UTC', 'start': 167, 'text': ""And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it. In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522. So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size. So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,251,2021-07-06 13:00:03 UTC,167,"And then in here, we need to specify different parameters. Now, one of the main ones is the voc up size. Now, this needs to match to whichever voc up size we have already created in our tokenizer when we're initializing it. In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522. So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size. So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t187.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 268, 'published': '2021-07-06 13:00:03 UTC', 'start': 187, 'text': ""In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522. So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size. So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,268,2021-07-06 13:00:03 UTC,187,"In our tokenizer, when building our tokenizer. So I mean, for me, if I go all the way up here to here, this is where I created the tokenizer. I can see, OK, it's this number here. So 30,522. So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size. So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t209.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 289, 'published': '2021-07-06 13:00:03 UTC', 'start': 209, 'text': ""So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size. So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens. If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,289,2021-07-06 13:00:03 UTC,209,"So I'm going to set that. But if you don't have that, you can just write tokenizer voc up size. So here. And that will return your voc up size. So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens. If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t224.23999999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 307, 'published': '2021-07-06 13:00:03 UTC', 'start': 224, 'text': ""So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens. If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create. So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,307,2021-07-06 13:00:03 UTC,224,"So I mean, let's replace that. We'll do this. Now, as well as that, we want to also set this. So max position embedding. And this needs to be set to your max length plus two in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens. If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create. So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t246.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 334, 'published': '2021-07-06 13:00:03 UTC', 'start': 246, 'text': ""in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens. If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create. So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model. So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,334,2021-07-06 13:00:03 UTC,246,"in this case. So max length is set up here. So where is it? Max length here, 512. Plus two because we have these added special tokens. If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create. So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model. So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t259.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 352, 'published': '2021-07-06 13:00:03 UTC', 'start': 259, 'text': ""If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create. So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model. So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,352,2021-07-06 13:00:03 UTC,259,"If we don't do that, we'll end up with a index error because we're going beyond the embedding limits. Now we want our hidden size. So this is the size of the vectors that our embedding layers within Roberta will create. So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model. So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t277.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 363, 'published': '2021-07-06 13:00:03 UTC', 'start': 277, 'text': ""So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model. So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that. OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,363,2021-07-06 13:00:03 UTC,277,"So each token, so we have 514 or 12 tokens. And each one of those will be signed a vector of size 768. This is the typical number. So that's the originally came from the BERT based model. Then we set up the architecture of the internals of the model. So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that. OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t301.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 377, 'published': '2021-07-06 13:00:03 UTC', 'start': 301, 'text': ""So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that. OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta. And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,377,2021-07-06 13:00:03 UTC,301,"So we want the number of attention heads, which I'm going to set to 12. And also the number of hidden layers, which I... So the default for this is for Roberta, 12. But I'm going to go with six for the sake of keeping train times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that. OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta. And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t324.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 393, 'published': '2021-07-06 13:00:03 UTC', 'start': 324, 'text': ""times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that. OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta. And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights,"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,393,2021-07-06 13:00:03 UTC,324,"times a little shorter. Now we also need to add type, vocab, size, which is just one. So that's the different token types that we have. We just have one. Don't need to worry about that. OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta. And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights,",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t341.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 403, 'published': '2021-07-06 13:00:03 UTC', 'start': 341, 'text': ""OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta. And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights, and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,403,2021-07-06 13:00:03 UTC,341,"OK, so that's our configuration object ready. And we can import and initialize a Roberta model with that. So we went from transformers. This is kind of similar to what we usually do. Import Roberta. And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights, and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t359.28000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 419, 'published': '2021-07-06 13:00:03 UTC', 'start': 359, 'text': ""And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights, and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it. First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,419,2021-07-06 13:00:03 UTC,359,"And we're doing this for massed LL. So MLM, right? So we're training using MLM. So we want Roberta for massed LL. And we initialize our model using that Roberta for massed LL object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights, and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it. First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t374.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 440, 'published': '2021-07-06 13:00:03 UTC', 'start': 374, 'text': ""object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights, and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it. First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available. So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA,"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,440,2021-07-06 13:00:03 UTC,374,"object. And we just pass in our config. And this will... That's right there, initialize our Roberta model. So that's a plain Roberta model, randomly initialized weights, and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it. First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available. So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA,",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t387.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 464, 'published': '2021-07-06 13:00:03 UTC', 'start': 387, 'text': ""and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it. First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available. So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA, or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,464,2021-07-06 13:00:03 UTC,387,"and so on. And now we can move on to setting up everything for training. So we have our model. Now we need to prepare a few things before we train it. First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available. So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA, or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t400.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 478, 'published': '2021-07-06 13:00:03 UTC', 'start': 400, 'text': ""First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available. So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA, or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time. So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,478,2021-07-06 13:00:03 UTC,400,"First thing is we need to decide which device we're going to be training on. So whether that's CPU or a CUDA-enabled GPU. And to figure out if we have that, we write... Well, we can write torch CUDA is available. So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA, or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time. So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t416.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 490, 'published': '2021-07-06 13:00:03 UTC', 'start': 416, 'text': ""So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA, or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time. So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU. If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,490,2021-07-06 13:00:03 UTC,416,"So write this. And for me, it is. So the typical way that you would decide whether you're using CUDA or CPU or the typical line of code that will decide it for you is you write device and you do torch.CUDA, or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time. So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU. If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t434.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 506, 'published': '2021-07-06 13:00:03 UTC', 'start': 434, 'text': ""or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time. So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU. If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two. I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,506,2021-07-06 13:00:03 UTC,434,"or torch device, sorry. And then you write CUDA inside here. If it's available, otherwise we are going to use torch device CPU. Now, CPU takes... Yeah, it's just takes a really long time. So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU. If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two. I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t456.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 523, 'published': '2021-07-06 13:00:03 UTC', 'start': 456, 'text': ""So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU. If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two. I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway. So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,523,2021-07-06 13:00:03 UTC,456,"So if you are using CPU, you have to leave it overnight, for sure. Maybe even longer. Even if it's just like a little bit of data, it takes so long. So but hopefully you have a GPU. If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two. I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway. So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t474.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 534, 'published': '2021-07-06 13:00:03 UTC', 'start': 474, 'text': ""If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two. I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway. So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now. So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,534,2021-07-06 13:00:03 UTC,474,"If not, just you're going to have to be patient. That's all. Or if you could maybe try and use Google Colab, but you have to use a premium version because otherwise it's just going to shut off after like an hour or two. I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway. So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now. So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t489.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 541, 'published': '2021-07-06 13:00:03 UTC', 'start': 489, 'text': ""I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway. So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now. So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model. And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,541,2021-07-06 13:00:03 UTC,489,"I don't know. I don't really use it. So I don't know how long it will train for before just deciding that it's done. And the GPU is also not that good anyway. So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now. So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model. And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t499.64000000000004,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 551, 'published': '2021-07-06 13:00:03 UTC', 'start': 499, 'text': ""So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now. So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model. And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be. Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,551,2021-07-06 13:00:03 UTC,499,"So, yeah. However, you can do it. And then after that, we want to move our model to our device. So whether it's GPU or CPU, we move over there. We're going to get a really big output now. So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model. And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be. Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t519.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 568, 'published': '2021-07-06 13:00:03 UTC', 'start': 519, 'text': ""So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model. And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be. Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,568,2021-07-06 13:00:03 UTC,519,"So it's just our model. So this is like the structure of our model. So we can see a few interesting things. We've got Roberta for MLM. We have the Roberta model. And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be. Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t529.5600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 579, 'published': '2021-07-06 13:00:03 UTC', 'start': 529, 'text': ""And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be. Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay. And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,579,2021-07-06 13:00:03 UTC,529,"And then inside that we have our embeddings. And then we have our 12. Did I say 12? I think it was six. Six encoders should be. Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay. And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t538.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 595, 'published': '2021-07-06 13:00:03 UTC', 'start': 538, 'text': ""Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay. And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what? Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,595,2021-07-06 13:00:03 UTC,538,"Yeah. So it goes from 0 to 5. It's our six. And then we have the outputs here. And then our final bit, which is the language modeling head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay. And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what? Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t547.9200000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 610, 'published': '2021-07-06 13:00:03 UTC', 'start': 547, 'text': ""head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay. And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what? Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters. And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think,"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,610,2021-07-06 13:00:03 UTC,547,"head, the MLM head. So that's cool. Now we need our optimizer. So from transformers, import AdamW, which is Adam with weight and decay. And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what? Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters. And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think,",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t562.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 623, 'published': '2021-07-06 13:00:03 UTC', 'start': 562, 'text': ""And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what? Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters. And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think, from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,623,2021-07-06 13:00:03 UTC,562,"And what we're going to do is just going to activate the training mode of our model. It's going to give us loads of output again. So just, yeah. You know what? Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters. And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think, from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t575.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 641, 'published': '2021-07-06 13:00:03 UTC', 'start': 575, 'text': ""Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters. And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think, from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that. How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,641,2021-07-06 13:00:03 UTC,575,"Maybe I can just, let's just remove that. There we go. Easier. And then our optimizer is going to be AdamW. We need to pass in our model parameters. And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think, from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that. How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t589.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 660, 'published': '2021-07-06 13:00:03 UTC', 'start': 589, 'text': ""And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think, from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that. How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop. Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,660,2021-07-06 13:00:03 UTC,589,"And we need a learning rate. So from, I mean, I don't usually use Roberta. But looking online, this looks like a reasonable learning rate. I think you can go from sort of here to, I think, from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that. How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop. Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t605.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 672, 'published': '2021-07-06 13:00:03 UTC', 'start': 605, 'text': ""from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that. How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop. Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM. We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,672,2021-07-06 13:00:03 UTC,605,"from what I remember down to like here. That's the sort of typical range. But obviously, it's going to depend on how much data you have. And we don't want to do that. How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop. Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM. We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t614.8399999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 690, 'published': '2021-07-06 13:00:03 UTC', 'start': 614, 'text': ""How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop. Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM. We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool. Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,690,2021-07-06 13:00:03 UTC,614,"How much data you have and loads of different things, right? So that's what I'm going to go with. And that should be pretty much it. So that's our setup. Now we're just going to create our training loop. Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM. We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool. Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t633.0799999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 706, 'published': '2021-07-06 13:00:03 UTC', 'start': 633, 'text': ""Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM. We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool. Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style. So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,706,2021-07-06 13:00:03 UTC,633,"Now for the training loop, we want to import TQDM. So we can see how far through we are. We're going to train for two epochs. And we're going to initialize our loop object using TQDM. So TQDM. We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool. Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style. So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t656.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 720, 'published': '2021-07-06 13:00:03 UTC', 'start': 656, 'text': ""We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool. Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style. So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer. So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,720,2021-07-06 13:00:03 UTC,656,"We have our data loader. What is the name of that data loader? I'm not sure. Data loader. Cool. Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style. So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer. So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t667.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 738, 'published': '2021-07-06 13:00:03 UTC', 'start': 667, 'text': ""Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style. So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer. So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model. And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,738,2021-07-06 13:00:03 UTC,667,"Data loader. And we set leave equals true. But I need that. Sorry. I need that in the same style. So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer. So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model. And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t677.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 761, 'published': '2021-07-06 13:00:03 UTC', 'start': 677, 'text': ""So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer. So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model. And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors. So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,761,2021-07-06 13:00:03 UTC,677,"So for batch in loop. And then here, we run through each of the steps that we're going to perform for every single training loop. So the first thing we do is initialize the gradient in our optimizer. So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model. And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors. So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t700.9599999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 778, 'published': '2021-07-06 13:00:03 UTC', 'start': 700, 'text': ""So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model. And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors. So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three. So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,778,2021-07-06 13:00:03 UTC,700,"So zero grad. So the reason we do this is after the first loop, our optimizer is going to be assigned a set of gradients, which we're going to use to optimize our model. And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors. So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three. So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t714.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 796, 'published': '2021-07-06 13:00:03 UTC', 'start': 714, 'text': ""And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors. So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three. So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model. We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,796,2021-07-06 13:00:03 UTC,714,"And on the next loop, we don't want those residual gradients to still be there in our optimizer. We want to essentially reset it for the next loop. So that's what we're doing here. Then we want our tensors. So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three. So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model. We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t728.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 815, 'published': '2021-07-06 13:00:03 UTC', 'start': 728, 'text': ""So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three. So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model. We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs. Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,815,2021-07-06 13:00:03 UTC,728,"So we have input IDs. And that is going to be batch input IDs. And we also want to move that over to our GPU or CPU if you're on that. And this is pretty much the same for our three. So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model. We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs. Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t750.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 834, 'published': '2021-07-06 13:00:03 UTC', 'start': 750, 'text': ""So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model. We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs. Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model. We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,834,2021-07-06 13:00:03 UTC,750,"So mask labels. And this is just attention mask. So we've extracted our tensors. And we just need to feed them into our model now. So we're going to get outputs from the model. We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs. Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model. We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t770.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 850, 'published': '2021-07-06 13:00:03 UTC', 'start': 770, 'text': ""We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs. Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model. We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss. Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,850,2021-07-06 13:00:03 UTC,770,"We just do model. Input IDs, attention mask, which is going to be equal to mask. And our labels equal to labels. So everything has been fed into our model. We have our outputs. Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model. We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss. Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t790.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 869, 'published': '2021-07-06 13:00:03 UTC', 'start': 790, 'text': ""Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model. We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss. Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description. And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,869,2021-07-06 13:00:03 UTC,790,"Now we need to extract a few things from the output. So we need the loss. So we write loss equals outputs.loss. And from that, we want to calculate all of the different parameters in our model. We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss. Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description. And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t807.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 883, 'published': '2021-07-06 13:00:03 UTC', 'start': 807, 'text': ""We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss. Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description. And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss. So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,883,2021-07-06 13:00:03 UTC,807,"We need to calculate the loss for each one of those parameters. So we do this loss.backwards to backpropagate through all of those different values and get that loss. After we've done that, we use our optimizer, take a step, and optimize all those parameters based on that loss. Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description. And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss. So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t831.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 901, 'published': '2021-07-06 13:00:03 UTC', 'start': 831, 'text': ""Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description. And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss. So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work. OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,901,2021-07-06 13:00:03 UTC,831,"Then that's everything we need to train the model. And there's just a few things. So for the progress bar, I just want a little bit of information there just so I know what's going on. And I just write loop set description. And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss. So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work. OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t845.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 915, 'published': '2021-07-06 13:00:03 UTC', 'start': 845, 'text': ""And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss. So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work. OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors. One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU,"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,915,2021-07-06 13:00:03 UTC,845,"And I just want to print out the epoch. So write that. And then I want to set the postfix as well. So loop.set postfix. And here I just want to see the loss. So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work. OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors. One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU,",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t862.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 932, 'published': '2021-07-06 13:00:03 UTC', 'start': 862, 'text': ""So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work. OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors. One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU, and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,932,2021-07-06 13:00:03 UTC,862,"So we'll just do loss item like that. So that should be everything. Yeah, let's run that, see what happens. Hopefully it should work. No, it didn't work. OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors. One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU, and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t880.4000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 944, 'published': '2021-07-06 13:00:03 UTC', 'start': 880, 'text': ""OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors. One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU, and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself. So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,944,2021-07-06 13:00:03 UTC,880,"OK. Let me see. Oh, no, it's a CUDA error. So probably just need to refresh everything. I hate CUDA errors. One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU, and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself. So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t897.0400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 958, 'published': '2021-07-06 13:00:03 UTC', 'start': 897, 'text': ""One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU, and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself. So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,958,2021-07-06 13:00:03 UTC,897,"One moment. OK, so finally figured it out. Took so long. So a few tips anyway. When you do get a CUDA error, switch your device to CPU, and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself. So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t910.6800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 968, 'published': '2021-07-06 13:00:03 UTC', 'start': 910, 'text': ""and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself. So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it. It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,968,2021-07-06 13:00:03 UTC,910,"and then re-run everything. And you should get a more understandable error. So if we come down here, I've changed it to CPU. We see that we get an index error. Scroll down, index out of range itself. So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it. It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t922.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 986, 'published': '2021-07-06 13:00:03 UTC', 'start': 922, 'text': ""So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it. It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything. Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,986,2021-07-06 13:00:03 UTC,922,"So the reason for this is, so you get this error if you don't add the extra two tokens onto the end of here. But we added them. So I was pretty confused about that. And then it took me a really long time to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it. It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything. Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t940.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1005, 'published': '2021-07-06 13:00:03 UTC', 'start': 940, 'text': ""to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it. It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything. Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now. Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1005,2021-07-06 13:00:03 UTC,940,"to realize that this argument is wrong. And there should be an S on the end. So that was the error. So yeah, super, super cool. That was literally it. It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything. Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now. Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t952.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1029, 'published': '2021-07-06 13:00:03 UTC', 'start': 952, 'text': ""It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything. Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now. Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along. So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1029,2021-07-06 13:00:03 UTC,952,"It took me so long to figure that out. But now we have it. That's good. We just need to run everything again. So I'm just going to run through everything. Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now. Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along. So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t964.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1048, 'published': '2021-07-06 13:00:03 UTC', 'start': 964, 'text': ""Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now. Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along. So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was? Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1048,2021-07-06 13:00:03 UTC,964,"Remove this cell here where I changed it to CPU, because I don't need it now. And just re-execute all that. OK, so we're back. And we've finished training our model now. Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along. So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was? Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t981.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1063, 'published': '2021-07-06 13:00:03 UTC', 'start': 981, 'text': ""Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along. So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was? Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run. And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1063,2021-07-06 13:00:03 UTC,981,"Now, it has taken a long time. It's a few days later. And I made a few changes during training as well. So this definitely wasn't the cleanest training process, because I was kind of updating parameters as it was going along. So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was? Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run. And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t998.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1075, 'published': '2021-07-06 13:00:03 UTC', 'start': 998, 'text': ""So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was? Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run. And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute. So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1075,2021-07-06 13:00:03 UTC,998,"So initially, well, first, we've trained for like three and a bit epochs. And I've trained on the full data set as well. So if I come up here, I think, do I print out how much data it was? Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run. And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute. So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1022.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1089, 'published': '2021-07-06 13:00:03 UTC', 'start': 1022, 'text': ""Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run. And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute. So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs. Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1089,2021-07-06 13:00:03 UTC,1022,"Maybe in another file. So if we come down here, so yeah, there's a lot more data here. So we have 200, no, 20. Let me think, 2 million. OK, so 2 million samples in that final run. And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute. So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs. Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1040.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1103, 'published': '2021-07-06 13:00:03 UTC', 'start': 1040, 'text': ""And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute. So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs. Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting. So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1103,2021-07-06 13:00:03 UTC,1040,"And initially, when we started training, we started with a learning rate of 1e to the minus 5. Now, I looked into this a little bit. And it just was not really moving. And I'll show you in a minute. So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs. Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting. So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1054.6799999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1122, 'published': '2021-07-06 13:00:03 UTC', 'start': 1054, 'text': ""So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs. Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting. So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1122,2021-07-06 13:00:03 UTC,1054,"So for the second epoch, I moved it down to 1e to the minus 4. Or moved it up, sorry, to 1e to the minus 4. And that moved, started moving things a lot quicker. So that was good. And then in total, like I said, it was three and a bit epochs. Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting. So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1071.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1134, 'published': '2021-07-06 13:00:03 UTC', 'start': 1071, 'text': ""Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting. So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1134,2021-07-06 13:00:03 UTC,1071,"Other than that, I didn't really change anything. The only thing I did was I trained for one epoch at a time because I wanted to see how the results were looking after each epoch. And that was quite interesting. So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1084.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1150, 'published': '2021-07-06 13:00:03 UTC', 'start': 1084, 'text': ""So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right? Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1150,2021-07-06 13:00:03 UTC,1084,"So let me show you that. OK, so this is after the first epoch. So here, what I'm doing is I've got this fill, which is a pipeline fill object. And I'm entering chow and then putting in now mass and then va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right? Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1100.6399999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1166, 'published': '2021-07-06 13:00:03 UTC', 'start': 1100, 'text': ""va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right? Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva. I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1166,2021-07-06 13:00:03 UTC,1100,"va. And I wanted to say chow, come here, va. And in the middle, I wouldn't have to predict come. Now, this is after the first epoch. And we can see it's not, yeah, it's just putting like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right? Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva. I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1112.8799999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1182, 'published': '2021-07-06 13:00:03 UTC', 'start': 1112, 'text': ""like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right? Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva. I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words. But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1182,2021-07-06 13:00:03 UTC,1112,"like random characters. So question mark here, three dots here, chow and chow again here. Kind of weird. So yeah, not the best, right? Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva. I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words. But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1128.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1190, 'published': '2021-07-06 13:00:03 UTC', 'start': 1128, 'text': ""Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva. I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words. But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it. So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1190,2021-07-06 13:00:03 UTC,1128,"Then we move on to the second epoch. And it's getting, well, it's still rubbish. At least it's got words. So like here, we have a word, chow, kiva, or chiva. Kek, oh, chow, kiva. I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words. But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it. So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1145.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1207, 'published': '2021-07-06 13:00:03 UTC', 'start': 1145, 'text': ""I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words. But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it. So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted. So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1207,2021-07-06 13:00:03 UTC,1145,"I don't know if that's the way I always, the C-H in Italian, I always get messed up. If there's any Italians watching, I'm sorry. Chow, kiva. At least we're getting words. But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it. So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted. So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1158.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1221, 'published': '2021-07-06 13:00:03 UTC', 'start': 1158, 'text': ""But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it. So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted. So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy. So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1221,2021-07-06 13:00:03 UTC,1158,"But none of these, so it doesn't make any sense. OK, so no, still not good. Now, if we come across again, so this is this one. Yeah, this one. Now we get it. So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted. So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy. So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1173.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1232, 'published': '2021-07-06 13:00:03 UTC', 'start': 1173, 'text': ""So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted. So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy. So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1232,2021-07-06 13:00:03 UTC,1173,"So the rest of them are nonsense. OK, so the four here, ignore them. However, at the top, we get this score of 0.33. And we get chow, kiva. So that's what we wanted. So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy. So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1187.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1242, 'published': '2021-07-06 13:00:03 UTC', 'start': 1187, 'text': ""So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy. So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is. That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1242,2021-07-06 13:00:03 UTC,1187,"So that's good. It means it's working. This was after the third and a bit epoch. Let me show you the loss function as well. So this, I know this is really messy. So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is. That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1202.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1259, 'published': '2021-07-06 13:00:03 UTC', 'start': 1202, 'text': ""So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is. That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud. So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1259,2021-07-06 13:00:03 UTC,1202,"So here we have our, I don't know why this one's so short, actually. Why is that one so short? Strange, maybe I didn't. Yeah, for the last one, it doesn't look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is. That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud. So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1218.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1273, 'published': '2021-07-06 13:00:03 UTC', 'start': 1218, 'text': ""look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is. That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud. So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker. So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1273,2021-07-06 13:00:03 UTC,1218,"look like I finished training for the full epoch. So I thought I did. Maybe something happened. I'm not sure. But fine, that's what it is. That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud. So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker. So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1227.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1288, 'published': '2021-07-06 13:00:03 UTC', 'start': 1227, 'text': ""That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud. So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker. So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway. But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1288,2021-07-06 13:00:03 UTC,1227,"That's fine. So the first set of training I did was here. And you see in the middle, my computer went to sleep for a bit overnight because it was just so loud. So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker. So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway. But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1237.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1298, 'published': '2021-07-06 13:00:03 UTC', 'start': 1237, 'text': ""So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker. So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway. But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model. So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1298,2021-07-06 13:00:03 UTC,1237,"So I turned it off for a bit. And then continue going down. Now, this first epoch is when we were at 1.0 or 1e to the minus 5. And then here, I was testing the 1e to the minus 4. And you can see straight away, it goes down way quicker. So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway. But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model. So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1255.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1313, 'published': '2021-07-06 13:00:03 UTC', 'start': 1255, 'text': ""So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway. But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model. So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline. Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1313,2021-07-06 13:00:03 UTC,1255,"So I was like, OK, we're going to go with that. It's clearly a lot better. And then continued over here, the next epoch, and then the final one here, which didn't seem to change much anyway. But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model. So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline. Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1265.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1325, 'published': '2021-07-06 13:00:03 UTC', 'start': 1265, 'text': ""But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model. So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline. Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1325,2021-07-06 13:00:03 UTC,1265,"But there was still a pretty clear difference. So that's the loss over time. And yeah, I mean, we've seen the results from that. So now we have that. Let's move on to actually testing the model. So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline. Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1281.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1335, 'published': '2021-07-06 13:00:03 UTC', 'start': 1281, 'text': ""So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline. Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian. So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1335,2021-07-06 13:00:03 UTC,1281,"So I'm going to bring Lara in. I'm going to just open the file. OK, so this is the testing we're going to do. So we're using the file mask. We've got this pipeline. Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian. So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1295.6000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1349, 'published': '2021-07-06 13:00:03 UTC', 'start': 1295, 'text': ""Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian. So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao. OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1349,2021-07-06 13:00:03 UTC,1295,"Sorry, fill mask. I've got this pipeline. And what I'm going to do is just get Lara to come in and some Italian sentences and just add this random mask token in and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian. So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao. OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1306.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1360, 'published': '2021-07-06 13:00:03 UTC', 'start': 1306, 'text': ""and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian. So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao. OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word. And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1360,2021-07-06 13:00:03 UTC,1306,"and see if the results are bearable or not. So let's see. So I will see you in a minute. This is Lara. She can speak Italian. So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao. OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word. And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1318.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1370, 'published': '2021-07-06 13:00:03 UTC', 'start': 1318, 'text': ""So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao. OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word. And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet. And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1370,2021-07-06 13:00:03 UTC,1318,"So she's going to go through this and test it a few times and hopefully say it's good. Let's see. Hopefully. Ciao. OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word. And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet. And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1328.0800000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1379, 'published': '2021-07-06 13:00:03 UTC', 'start': 1328, 'text': ""OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word. And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet. And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence. And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1379,2021-07-06 13:00:03 UTC,1328,"OK, so all you need to do is we have a sentence here. And you just write some Italian. And then for one of the words in there, we want to replace it with this text here. And then that's going to mask that word. And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet. And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence. And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1343.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1393, 'published': '2021-07-06 13:00:03 UTC', 'start': 1343, 'text': ""And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet. And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence. And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots? Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1393,2021-07-06 13:00:03 UTC,1343,"And then the model is going to try and predict what is there. And hopefully it will predict. Let's see. So just write some Italian phrase. Not too difficult yet. And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence. And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots? Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1355.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1410, 'published': '2021-07-06 13:00:03 UTC', 'start': 1355, 'text': ""And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence. And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots? Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come? OK. And then. OK, so just cover it with the mask. And see what it says. So not this."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1410,2021-07-06 13:00:03 UTC,1355,"And see. So I don't have to write, ciao, va? No, no, no, no. You write. Just write a sentence. And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots? Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come? OK. And then. OK, so just cover it with the mask. And see what it says. So not this.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1362.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1420, 'published': '2021-07-06 13:00:03 UTC', 'start': 1362, 'text': ""And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots? Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come? OK. And then. OK, so just cover it with the mask. And see what it says. So not this. I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1420,2021-07-06 13:00:03 UTC,1362,"And we'll do that. Buongiorno, gente. No, buongiorno. Maybe a few words there. Can I put comma or dots? Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come? OK. And then. OK, so just cover it with the mask. And see what it says. So not this. I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1376.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1437, 'published': '2021-07-06 13:00:03 UTC', 'start': 1376, 'text': ""Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come? OK. And then. OK, so just cover it with the mask. And see what it says. So not this. I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va. Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian,"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1437,2021-07-06 13:00:03 UTC,1376,"Yeah, you can put comma. Buongiorno, come va? OK. And then, so which word should we cover? Come? OK. And then. OK, so just cover it with the mask. And see what it says. So not this. I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va. Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian,",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1389.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1445, 'published': '2021-07-06 13:00:03 UTC', 'start': 1389, 'text': ""OK. And then. OK, so just cover it with the mask. And see what it says. So not this. I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va. Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian, but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1445,2021-07-06 13:00:03 UTC,1389,"OK. And then. OK, so just cover it with the mask. And see what it says. So not this. I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va. Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian, but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1404.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1458, 'published': '2021-07-06 13:00:03 UTC', 'start': 1404, 'text': ""I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va. Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian, but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe. Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1458,2021-07-06 13:00:03 UTC,1404,"I seem to read on these as well. OK, so let's give it a moment. Chi va? Yeah, but the second one, come va? Come va. Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian, but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe. Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1417.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1466, 'published': '2021-07-06 13:00:03 UTC', 'start': 1417, 'text': ""Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian, but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe. Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No. Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1466,2021-07-06 13:00:03 UTC,1417,"Almost there. Does chi va mean anything? Like who? Yeah, it's like, is there someone? But I understand because I'm Italian, but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe. Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No. Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1430.8400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1476, 'published': '2021-07-06 13:00:03 UTC', 'start': 1430, 'text': ""but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe. Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No. Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect. Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1476,2021-07-06 13:00:03 UTC,1430,"but I don't think that we don't usually say that. I don't think. I'm going to take that as fine. I'm going to take that as it's good. So let's do it again, maybe. Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No. Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect. Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1443.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1495, 'published': '2021-07-06 13:00:03 UTC', 'start': 1443, 'text': ""Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No. Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect. Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence. So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1495,2021-07-06 13:00:03 UTC,1443,"Yeah. Try another one. Oh, wait, actually, what about these ones? Cos'va, no, non'va definitely not right. No. Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect. Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence. So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1455.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1500, 'published': '2021-07-06 13:00:03 UTC', 'start': 1455, 'text': ""Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect. Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence. So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace? Incontriamo, maybe? Or dove as well. OK. Yeah. So which one?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1500,2021-07-06 13:00:03 UTC,1455,"Che'va? No. No, OK. But it's just like after buongiorno, I wouldn't expect. Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence. So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace? Incontriamo, maybe? Or dove as well. OK. Yeah. So which one?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1464.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1509, 'published': '2021-07-06 13:00:03 UTC', 'start': 1464, 'text': ""Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence. So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace? Incontriamo, maybe? Or dove as well. OK. Yeah. So which one? You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1509,2021-07-06 13:00:03 UTC,1464,"Chi va? Chi va. Yeah, that's OK. So you can just put another one like where we put Phil again. You can write another sentence. So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace? Incontriamo, maybe? Or dove as well. OK. Yeah. So which one? You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1473.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1517, 'published': '2021-07-06 13:00:03 UTC', 'start': 1473, 'text': ""So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace? Incontriamo, maybe? Or dove as well. OK. Yeah. So which one? You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah. Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1517,2021-07-06 13:00:03 UTC,1473,"So we're here. Yeah. So we can write. Yeah. Yeah, and then what do you want to replace? Incontriamo, maybe? Or dove as well. OK. Yeah. So which one? You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah. Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1490.3600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1524, 'published': '2021-07-06 13:00:03 UTC', 'start': 1490, 'text': ""Incontriamo, maybe? Or dove as well. OK. Yeah. So which one? You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah. Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio? No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1524,2021-07-06 13:00:03 UTC,1490,"Incontriamo, maybe? Or dove as well. OK. Yeah. So which one? You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah. Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio? No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1499.1200000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1529, 'published': '2021-07-06 13:00:03 UTC', 'start': 1499, 'text': ""You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah. Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio? No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good. Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1529,2021-07-06 13:00:03 UTC,1499,"You decide. It's fine. Incontriamo is interesting. OK, let's try. Yeah, yeah. Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio? No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good. Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1507.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1551, 'published': '2021-07-06 13:00:03 UTC', 'start': 1507, 'text': ""Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio? No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good. Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah. Try. You can do it. OK. You can control Z, right? Yeah."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1551,2021-07-06 13:00:03 UTC,1507,"Oh, look at that. Yeah, that's good. Dove ci vediamo oggi pomeriggio? Dove ci incontriamo oggi pomeriggio? Dove ci siamo oggi pomeriggio? No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good. Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah. Try. You can do it. OK. You can control Z, right? Yeah.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1515.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1564, 'published': '2021-07-06 13:00:03 UTC', 'start': 1515, 'text': ""No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good. Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah. Try. You can do it. OK. You can control Z, right? Yeah. OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1564,2021-07-06 13:00:03 UTC,1515,"No. No. Dove ci troviamo oggi pomeriggio? Dove ci ritroviamo? Yeah, that's quite good. Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah. Try. You can do it. OK. You can control Z, right? Yeah. OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1522.1200000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1583, 'published': '2021-07-06 13:00:03 UTC', 'start': 1522, 'text': ""Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah. Try. You can do it. OK. You can control Z, right? Yeah. OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool. Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1583,2021-07-06 13:00:03 UTC,1522,"Yeah. OK. Should we try with dove? Like using the same phrase. Yeah, yeah. Try. You can do it. OK. You can control Z, right? Yeah. OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool. Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1527.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1587, 'published': '2021-07-06 13:00:03 UTC', 'start': 1527, 'text': ""Try. You can do it. OK. You can control Z, right? Yeah. OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool. Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera. OK. Let's remove pripare. Yeah. Control Z. Yeah."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1587,2021-07-06 13:00:03 UTC,1527,"Try. You can do it. OK. You can control Z, right? Yeah. OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool. Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera. OK. Let's remove pripare. Yeah. Control Z. Yeah.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1542.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1599, 'published': '2021-07-06 13:00:03 UTC', 'start': 1542, 'text': ""OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool. Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera. OK. Let's remove pripare. Yeah. Control Z. Yeah. Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1599,2021-07-06 13:00:03 UTC,1542,"OK, let's run it. Dove, the second one. Come ci incontriamo quando ci incontriamo is good. Si. That's cool. Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera. OK. Let's remove pripare. Yeah. Control Z. Yeah. Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1559.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1613, 'published': '2021-07-06 13:00:03 UTC', 'start': 1559, 'text': ""Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera. OK. Let's remove pripare. Yeah. Control Z. Yeah. Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah. Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before,"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1613,2021-07-06 13:00:03 UTC,1559,"Yeah, cosa, se. Let's try another one. Yeah. Yeah. Cosa, pripare, che cena stacera. OK. Let's remove pripare. Yeah. Control Z. Yeah. Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah. Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before,",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1581.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1624, 'published': '2021-07-06 13:00:03 UTC', 'start': 1581, 'text': ""OK. Let's remove pripare. Yeah. Control Z. Yeah. Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah. Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before, which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1624,2021-07-06 13:00:03 UTC,1581,"OK. Let's remove pripare. Yeah. Control Z. Yeah. Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah. Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before, which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1585.8799999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1648, 'published': '2021-07-06 13:00:03 UTC', 'start': 1585, 'text': ""Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah. Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before, which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si. Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know,"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1648,2021-07-06 13:00:03 UTC,1585,"Go. Run it. Yeah. Cosa fare per cena stacera, that's good. Cosa serve per cena stacera, yeah. Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before, which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si. Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know,",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1595.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1652, 'published': '2021-07-06 13:00:03 UTC', 'start': 1595, 'text': ""Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before, which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si. Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know, I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1652,2021-07-06 13:00:03 UTC,1595,"Cosa aspetta per cena stacera, ni. No. Cosa succede per cena stacera, ni. Cosa vedere per cena stacera, ni. So I didn't find what we said before, which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si. Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know, I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1609.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1656, 'published': '2021-07-06 13:00:03 UTC', 'start': 1609, 'text': ""which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si. Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know, I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace? Avessimo. Avessimo. Avess... How do you say? Avessimo."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1656,2021-07-06 13:00:03 UTC,1609,"which was cosa, pripare per cena stacera. Yeah. But does it make sense? Yeah, it makes sense. Cosa fare per cena stacera, cosa serve per cena stacera, si. Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know, I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace? Avessimo. Avessimo. Avess... How do you say? Avessimo.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1619.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1668, 'published': '2021-07-06 13:00:03 UTC', 'start': 1619, 'text': ""Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know, I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace? Avessimo. Avessimo. Avess... How do you say? Avessimo. Si. Avessimo. Avessimo. What does that mean? If we had."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1668,2021-07-06 13:00:03 UTC,1619,"Try something hard, like grammatically difficult. Mm. Cosa succederebbe. Is it hard? No, I'm thinking, I don't, when it's like that, you know, I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace? Avessimo. Avessimo. Avess... How do you say? Avessimo. Si. Avessimo. Avessimo. What does that mean? If we had.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1637.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1681, 'published': '2021-07-06 13:00:03 UTC', 'start': 1637, 'text': ""I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace? Avessimo. Avessimo. Avess... How do you say? Avessimo. Si. Avessimo. Avessimo. What does that mean? If we had. Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1681,2021-07-06 13:00:03 UTC,1637,"I don't know, like, doesn't come to my mind. Cosa se avesse scelto un altro giorno? Okay. Yeah. And then what should we replace? Avessimo. Avessimo. Avess... How do you say? Avessimo. Si. Avessimo. Avessimo. What does that mean? If we had. Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1651.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1696, 'published': '2021-07-06 13:00:03 UTC', 'start': 1651, 'text': ""Avessimo. Avessimo. Avess... How do you say? Avessimo. Si. Avessimo. Avessimo. What does that mean? If we had. Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular. So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day?"", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1696,2021-07-06 13:00:03 UTC,1651,"Avessimo. Avessimo. Avess... How do you say? Avessimo. Si. Avessimo. Avessimo. What does that mean? If we had. Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular. So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day?",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1654.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1708, 'published': '2021-07-06 13:00:03 UTC', 'start': 1654, 'text': ""Si. Avessimo. Avessimo. What does that mean? If we had. Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular. So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day? Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1708,2021-07-06 13:00:03 UTC,1654,"Si. Avessimo. Avessimo. What does that mean? If we had. Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular. So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day? Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1663.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1716, 'published': '2021-07-06 13:00:03 UTC', 'start': 1663, 'text': ""Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular. So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day? Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day. Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave..."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1716,2021-07-06 13:00:03 UTC,1663,"Avessi, avessi, avessero. Not, not the same one. No, but it's good because avessimo, it's for third person plural. It's like we had. Avessi is third person singular. So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day? Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day. Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave...",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1678.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1725, 'published': '2021-07-06 13:00:03 UTC', 'start': 1678, 'text': ""So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day? Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day. Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave... This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1725,2021-07-06 13:00:03 UTC,1678,"So if he had or she had. Yeah. Chosen something different. What does this actually mean, cosa? So what would have happened if we had chosen another day? Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day. Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave... This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1689.3600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1743, 'published': '2021-07-06 13:00:03 UTC', 'start': 1689, 'text': ""Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day. Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave... This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh. No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se..."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1743,2021-07-06 13:00:03 UTC,1689,"Ah. So the first one, se avesse scelto, it will be the third person. Uh-huh. Se avesse scelto, it will be the first person. So if I had chosen another day. Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave... This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh. No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se...",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1703.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1751, 'published': '2021-07-06 13:00:03 UTC', 'start': 1703, 'text': ""Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave... This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh. No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se... So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1751,2021-07-06 13:00:03 UTC,1703,"Uh-huh. Se avessero, it's second person plural. So it will be if they had chosen another day. Uh-huh. Se ave... This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh. No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se... So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1714.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1759, 'published': '2021-07-06 13:00:03 UTC', 'start': 1714, 'text': ""This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh. No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se... So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero. Avesero. So let's run it. Avesero. You see? That's cool."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1759,2021-07-06 13:00:03 UTC,1714,"This one no. Se ave... Yeah. This is good, se venisse scelto, as well. Uh-huh. No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se... So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero. Avesero. So let's run it. Avesero. You see? That's cool.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1721.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1772, 'published': '2021-07-06 13:00:03 UTC', 'start': 1721, 'text': ""No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se... So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero. Avesero. So let's run it. Avesero. You see? That's cool. That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but..."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1772,2021-07-06 13:00:03 UTC,1721,"No, maybe no. But the first three are very good. Yeah. I have an idea. So now if we change to se... So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero. Avesero. So let's run it. Avesero. You see? That's cool. That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but...",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1733.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1778, 'published': '2021-07-06 13:00:03 UTC', 'start': 1733, 'text': ""So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero. Avesero. So let's run it. Avesero. You see? That's cool. That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but... Yeah, it's in the wrong place, but it's saying the right... Like the meaning is correct, but the grammar, it's not correct. Okay. Right, okay. Yeah."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1778,2021-07-06 13:00:03 UTC,1733,"So if we put se loro, so if we specify the person, maybe we'll take the correct one. So if we put se loro. And then we expect it to say avessemo? Avesero. Avesero. Avesero. So let's run it. Avesero. You see? That's cool. That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but... Yeah, it's in the wrong place, but it's saying the right... Like the meaning is correct, but the grammar, it's not correct. Okay. Right, okay. Yeah.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1747.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1795, 'published': '2021-07-06 13:00:03 UTC', 'start': 1747, 'text': ""Avesero. So let's run it. Avesero. You see? That's cool. That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but... Yeah, it's in the wrong place, but it's saying the right... Like the meaning is correct, but the grammar, it's not correct. Okay. Right, okay. Yeah. It's cool. You're welcome. Not happy it actually worked, because I wasn't sure if I could just... Well, it worked with ciao coming back, but that was all I tested it with, so I was a little bit worried that it might not do anything else. But thank you."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1795,2021-07-06 13:00:03 UTC,1747,"Avesero. So let's run it. Avesero. You see? That's cool. That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but... Yeah, it's in the wrong place, but it's saying the right... Like the meaning is correct, but the grammar, it's not correct. Okay. Right, okay. Yeah. It's cool. You're welcome. Not happy it actually worked, because I wasn't sure if I could just... Well, it worked with ciao coming back, but that was all I tested it with, so I was a little bit worried that it might not do anything else. But thank you.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
35Pdoyi6ZoQ-t1754.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1831, 'published': '2021-07-06 13:00:03 UTC', 'start': 1754, 'text': ""That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but... Yeah, it's in the wrong place, but it's saying the right... Like the meaning is correct, but the grammar, it's not correct. Okay. Right, okay. Yeah. It's cool. You're welcome. Not happy it actually worked, because I wasn't sure if I could just... Well, it worked with ciao coming back, but that was all I tested it with, so I was a little bit worried that it might not do anything else. But thank you. You're welcome. Bye. Okay, so I think that's a pretty good result. So, I mean, that's pretty much everything we needed for building our model, our transform model. Although I do want to... so we're going to do one more video after this, where we're going to upload our model to the Hugging Face model hub."", 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4', 'url': 'https://youtu.be/35Pdoyi6ZoQ'}",UCv83tO5cePwHMt1952IVVHw,1831,2021-07-06 13:00:03 UTC,1754,"That's very good. And then the other one, se loro anno, is right? Ave anno? I mean, I'm saying... Well, the verb is incorrect, but... Yeah, it's in the wrong place, but it's saying the right... Like the meaning is correct, but the grammar, it's not correct. Okay. Right, okay. Yeah. It's cool. You're welcome. Not happy it actually worked, because I wasn't sure if I could just... Well, it worked with ciao coming back, but that was all I tested it with, so I was a little bit worried that it might not do anything else. But thank you. You're welcome. Bye. Okay, so I think that's a pretty good result. So, I mean, that's pretty much everything we needed for building our model, our transform model. Although I do want to... so we're going to do one more video after this, where we're going to upload our model to the Hugging Face model hub.",Training and Testing an Italian BERT - Transformers From Scratch #4,https://youtu.be/35Pdoyi6ZoQ
B7wmo_NImgM-t0.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 86, 'published': '2021-08-09 15:04:10 UTC', 'start': 0, 'text': ""Hi, welcome to the video. I'm going to take you through a few different indexes in FIAS today. So FIAS for similarity search. And we're going to learn how we can decide which index to use based on our data. Now, these indexes are reasonably complex, but we're going to just have a high level look at each one of them. At some point in the future, we'll go into more depth for sure. But for now, this is what we're going to do. So we're going to cover the indexes that you see on the screen at the moment. So we have the flat indexes, which are just the plain and simple, nothing special going on there. And then we're going to have a look at LSH or locality sensitive hashing, HNSW, which is hierarchical navigable small worlds. And then finally, we're going to have a look at an IVF index as well. So first thing I'm going to show you is how to get some data for following through this. So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,86,2021-08-09 15:04:10 UTC,0,"Hi, welcome to the video. I'm going to take you through a few different indexes in FIAS today. So FIAS for similarity search. And we're going to learn how we can decide which index to use based on our data. Now, these indexes are reasonably complex, but we're going to just have a high level look at each one of them. At some point in the future, we'll go into more depth for sure. But for now, this is what we're going to do. So we're going to cover the indexes that you see on the screen at the moment. So we have the flat indexes, which are just the plain and simple, nothing special going on there. And then we're going to have a look at LSH or locality sensitive hashing, HNSW, which is hierarchical navigable small worlds. And then finally, we're going to have a look at an IVF index as well. So first thing I'm going to show you is how to get some data for following through this. So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t18.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 109, 'published': '2021-08-09 15:04:10 UTC', 'start': 18, 'text': ""but we're going to just have a high level look at each one of them. At some point in the future, we'll go into more depth for sure. But for now, this is what we're going to do. So we're going to cover the indexes that you see on the screen at the moment. So we have the flat indexes, which are just the plain and simple, nothing special going on there. And then we're going to have a look at LSH or locality sensitive hashing, HNSW, which is hierarchical navigable small worlds. And then finally, we're going to have a look at an IVF index as well. So first thing I'm going to show you is how to get some data for following through this. So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well. So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,109,2021-08-09 15:04:10 UTC,18,"but we're going to just have a high level look at each one of them. At some point in the future, we'll go into more depth for sure. But for now, this is what we're going to do. So we're going to cover the indexes that you see on the screen at the moment. So we have the flat indexes, which are just the plain and simple, nothing special going on there. And then we're going to have a look at LSH or locality sensitive hashing, HNSW, which is hierarchical navigable small worlds. And then finally, we're going to have a look at an IVF index as well. So first thing I'm going to show you is how to get some data for following through this. So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well. So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t35.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 131, 'published': '2021-08-09 15:04:10 UTC', 'start': 35, 'text': ""nothing special going on there. And then we're going to have a look at LSH or locality sensitive hashing, HNSW, which is hierarchical navigable small worlds. And then finally, we're going to have a look at an IVF index as well. So first thing I'm going to show you is how to get some data for following through this. So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well. So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook. So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,131,2021-08-09 15:04:10 UTC,35,"nothing special going on there. And then we're going to have a look at LSH or locality sensitive hashing, HNSW, which is hierarchical navigable small worlds. And then finally, we're going to have a look at an IVF index as well. So first thing I'm going to show you is how to get some data for following through this. So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well. So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook. So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t57.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 143, 'published': '2021-08-09 15:04:10 UTC', 'start': 57, 'text': ""So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well. So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook. So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here. So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them,"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,143,2021-08-09 15:04:10 UTC,57,"So we're going to be using the SIFT 1M dataset, which is one million vectors that we can use for testing similarity. Now, there's a little bit of code, so I'm just going to show it to you. So we have here, we're just downloading the code. There'll be a notebook for this in the description as well. So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook. So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here. So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them,",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t77.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 160, 'published': '2021-08-09 15:04:10 UTC', 'start': 77, 'text': ""So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook. So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here. So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them, because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,160,2021-08-09 15:04:10 UTC,77,"So you can just use that and copy things across. But we're downloading it from here and this will give us a tar file. So we download that. And then here, all we're doing is extracting all the files from inside that tar file. And then here, I'm reading everything into the notebook. So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here. So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them, because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t100.80000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 181, 'published': '2021-08-09 15:04:10 UTC', 'start': 100, 'text': ""So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here. So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them, because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well. So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,181,2021-08-09 15:04:10 UTC,100,"So inside that tar file, we'll get these FVEX files and we have to open them in a certain way, which is what we're doing here. So we're setting up the function to read them, sorry, here. And then here, I'm reading in two files. So we get a few different files here. So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them, because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well. So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t118.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 201, 'published': '2021-08-09 15:04:10 UTC', 'start': 118, 'text': ""So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them, because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well. So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points. So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,201,2021-08-09 15:04:10 UTC,118,"So I'm sorry, this should be SIFT. So we get the base data, which is going to be the data that we're going to search through. And then we also have query data here. And then what I'm doing here is just selecting a single query, a single vector to query with rather than all of them, because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well. So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points. So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t139.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 230, 'published': '2021-08-09 15:04:10 UTC', 'start': 139, 'text': ""because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well. So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points. So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three. Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,230,2021-08-09 15:04:10 UTC,139,"because we get quite a few in there. And then here, we can just see. So this is our query vector that gets Q. And then we also have WB here, which is going to be the data that we'll index and search through. And we can see some of it there as well. So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points. So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three. Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t153.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 253, 'published': '2021-08-09 15:04:10 UTC', 'start': 153, 'text': ""So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points. So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three. Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here. But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,253,2021-08-09 15:04:10 UTC,153,"So that's how we get data. Let's move on to some flat indexes. So what you can see at the moment is a sort of a visual representation of a flat L2 index. Now up here, this is what we're doing. So we're calculating, we have all these points. So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three. Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here. But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t176.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 274, 'published': '2021-08-09 15:04:10 UTC', 'start': 176, 'text': ""So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three. Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here. But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search. They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,274,2021-08-09 15:04:10 UTC,176,"So these are all of the WB points that we saw before. And this is our query vector. And we just calculate the distance between all of those. And then what we do is just take the top three. So the top K in reality, but in this case, it's top three. Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here. But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search. They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t192.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 298, 'published': '2021-08-09 15:04:10 UTC', 'start': 192, 'text': ""Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here. But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search. They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,298,2021-08-09 15:04:10 UTC,192,"Now, we also have IP. So we have both L2 distance and IP distance as well. IP works in a different way. So we're using a different format to actually calculate the distance or similarity there. So it's not exactly as you see it here. But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search. They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t215.32000000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 310, 'published': '2021-08-09 15:04:10 UTC', 'start': 215, 'text': ""But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search. They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,310,2021-08-09 15:04:10 UTC,215,"But before we write any code, just want to say with flat indexes, they are 100% quality. And typically what we want to do with FI's and similarity search indexes is balance the search quality versus the search speed. Higher search quality, usually slower search speed. And flat indexes are just pure search quality because they are an exhaustive search. They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t242.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 329, 'published': '2021-08-09 15:04:10 UTC', 'start': 242, 'text': ""They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything. And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,329,2021-08-09 15:04:10 UTC,242,"They check the distance between your query vector and every other vector in the index, which is fine if you don't have a particularly big data set or you don't care about time. But if you do, then you probably don't want to use that because it can take an incredibly long time. If you have a billion vectors in your data set and you do 100 queries a minute, then as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything. And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t267.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 363, 'published': '2021-08-09 15:04:10 UTC', 'start': 267, 'text': ""as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything. And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,363,2021-08-09 15:04:10 UTC,267,"as far as I know, it's impossible to run that. And if you were going to run that, you'd need some pretty insane hardware. So we can't use flat indexes and exhaustive search in most cases. But I will show you how to do it. So first, I'm just going to define dimensionality of our data, which is 128, which we can see up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything. And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t291.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 384, 'published': '2021-08-09 15:04:10 UTC', 'start': 291, 'text': ""up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything. And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay. And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,384,2021-08-09 15:04:10 UTC,291,"up here, 128. I'm also going to say how many, so how many results do we want to return? I'm going to say 10. Okay. We also need to import FI's before we do anything. And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay. And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t306.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 410, 'published': '2021-08-09 15:04:10 UTC', 'start': 306, 'text': ""And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay. And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay. So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,410,2021-08-09 15:04:10 UTC,306,"And then we can initialize our index. So I said we have two. So we have FI's index flat 02 or IP. I'm going to use IP because it's very slightly faster. It seems from me testing it, it's very slightly faster, but there's hardly any difference in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay. And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay. So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t325.74,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 429, 'published': '2021-08-09 15:04:10 UTC', 'start': 325, 'text': ""in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay. And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay. So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors. Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,429,2021-08-09 15:04:10 UTC,325,"in reality. So initializes our index and then we want to add our data to it. So we add WB and then we perform a search. So let me create a new cell and let me just run this quickly. Okay. And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay. So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors. Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t351.28000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 448, 'published': '2021-08-09 15:04:10 UTC', 'start': 351, 'text': ""And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay. So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors. Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that. And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,448,2021-08-09 15:04:10 UTC,351,"And what I'm going to do is just time it so you can see how long this takes as well. So I'm going to do time and we're going to do index or sorry, DI equals index search. And in here we have our query vector and how many samples we'd like to return. So I'm going to go with K. Okay. So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors. Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that. And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t377.35999999999996,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 465, 'published': '2021-08-09 15:04:10 UTC', 'start': 377, 'text': ""So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors. Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that. And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well. Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,465,2021-08-09 15:04:10 UTC,377,"So that was reasonably quick and that's because we don't have a huge data set and we're just searching for one query. So it's not really too much of a problem there. But what I do want to show you is, so if we print out I that returns all of the IDs or the indexes of the 10 most similar vectors. Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that. And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well. Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t398.15999999999997,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 485, 'published': '2021-08-09 15:04:10 UTC', 'start': 398, 'text': ""Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that. And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well. Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets. Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,485,2021-08-09 15:04:10 UTC,398,"Now I'm going to use that as a baseline for each of our other indexes. So this is, like I said, 100% quality and we can use this accuracy to test out other indexes as well. So what I'm going to do is take that and convert it into a list. And if we just have a look at what we get, we see that we get a list like that. And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well. Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets. Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t421.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 506, 'published': '2021-08-09 15:04:10 UTC', 'start': 421, 'text': ""And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well. Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets. Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket. And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,506,2021-08-09 15:04:10 UTC,421,"And we're just going to use that, like I said, to see how our other indexes are performing. So we'll move on to the other indexes. And like I said before, we want to try and go from this, which is the flat indexes, which is 100% search quality to something that's more 50-50. But it depends on our use case as well. Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets. Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket. And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t439.74,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 523, 'published': '2021-08-09 15:04:10 UTC', 'start': 439, 'text': ""Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets. Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket. And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things. So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,523,2021-08-09 15:04:10 UTC,439,"Sometimes we might want more speed, sometimes higher quality. So we will see a few of those through these indexes. So we start with LSH. So a very high level. LSH works by grouping vectors in two different buckets. Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket. And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things. So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t458.67999999999995,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 552, 'published': '2021-08-09 15:04:10 UTC', 'start': 458, 'text': ""Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket. And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things. So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,552,2021-08-09 15:04:10 UTC,458,"Now what we can see on the screen now is a typical hashing function for like a Python dictionary. And what these hashing functions do is they try to minimize collisions. So collision is where we would have the case of two items, maybe say these two, being hashed into the same bucket. And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things. So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t479.64000000000004,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 568, 'published': '2021-08-09 15:04:10 UTC', 'start': 479, 'text': ""And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things. So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets. Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,568,2021-08-09 15:04:10 UTC,479,"And with a dictionary, you don't want that because you want every bucket to be an independent value. Otherwise, it increases the complexity of extracting your values from a single bucket if they've collided. Now LSH is slightly different because we actually do want to group things. So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets. Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t498.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 593, 'published': '2021-08-09 15:04:10 UTC', 'start': 498, 'text': ""So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets. Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there. Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,593,2021-08-09 15:04:10 UTC,498,"So we can see it as a dictionary. But rather than where before we were avoiding those collisions, you can see here we're putting them into completely different buckets every time. Rather than doing that, we're trying to maximize collisions. So you can see here that we've pushed all three of these keys into this single bucket here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets. Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there. Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t518.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 623, 'published': '2021-08-09 15:04:10 UTC', 'start': 518, 'text': ""here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets. Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there. Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here. Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,623,2021-08-09 15:04:10 UTC,518,"here. And we've also pushed all of these keys into this single bucket. So we get groupings of our values. Now when it comes to performing our search, we process our query through the same hashing function and that will push it to one of our buckets. Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there. Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here. Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t536.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 659, 'published': '2021-08-09 15:04:10 UTC', 'start': 536, 'text': ""Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there. Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here. Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,659,2021-08-09 15:04:10 UTC,536,"Now in the case of maybe appearing in this bucket here, we use Hamming Distance to find the nearest bucket and then we can search or we restrict our scope to these values. So we just restricted our scope there, which means that we do not need to search through everything. We are avoiding searching through those values down there. Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here. Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t563.5600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 682, 'published': '2021-08-09 15:04:10 UTC', 'start': 563, 'text': ""Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here. Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,682,2021-08-09 15:04:10 UTC,563,"Now let's have a look at how we implement that. So it's pretty straightforward. All we do is index, we do vise index LSH. We have our dimensionality and then we also have this other variable which is called nbits. So I will put that in a variable up here. Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t582.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 696, 'published': '2021-08-09 15:04:10 UTC', 'start': 582, 'text': ""Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,696,2021-08-09 15:04:10 UTC,582,"Do nbits and what I'm going to do is I'm going to make it d multiplied by 4. So nbits we will have to scale with the dimensionality of our data which comes into another problem which I'll mention later on which is the curse of dimensionality. But I'll talk more about it in a moment. So here we have nbits and then we add our data like we did before and then we can search our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t607.7199999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 714, 'published': '2021-08-09 15:04:10 UTC', 'start': 607, 'text': ""our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here. How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,714,2021-08-09 15:04:10 UTC,607,"our data just like we did before. So time and we do d pi equals index search and we are searching using our query, our search query and we want to return 10 items. So quicker speed, see here. And what we can also do is compare the results to our 100% quality index or flat index and we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here. How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t645.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 743, 'published': '2021-08-09 15:04:10 UTC', 'start': 645, 'text': ""we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here. How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,743,2021-08-09 15:04:10 UTC,645,"we do that using numpy in 1D baseline i. Okay so I'm just going to look at it visually here so we can see we have quite a lot of matches here. So plenty of trues, couple of falses, true, false, false, false. So these are the top 10 that have been returned using our LSH algorithm and we're checking if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here. How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t672.0400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 770, 'published': '2021-08-09 15:04:10 UTC', 'start': 672, 'text': ""if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here. How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality. So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,770,2021-08-09 15:04:10 UTC,672,"if they exist in the baseline results that we got from our flat index earlier and we're returning that most of them are present in that baseline. So most of them do match so it's a reasonably good recall there. So that's good and it was faster. So we've got 17.6 milliseconds here. How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality. So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t691.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 803, 'published': '2021-08-09 15:04:10 UTC', 'start': 691, 'text': ""How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality. So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality. Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,803,2021-08-09 15:04:10 UTC,691,"How much did we get up here? We got 157 milliseconds. So slightly less accurate but what is that 10 times faster so it's pretty good. And we can mess around with n bits. We can increase it to increase the accuracy of our index or we decrease it to increase the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality. So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality. Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t710.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 828, 'published': '2021-08-09 15:04:10 UTC', 'start': 710, 'text': ""the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality. So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality. Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,828,2021-08-09 15:04:10 UTC,710,"the speed. So again it's just trying to find that balance between them both. Okay so this is a graph just showing you the recalls with different n bit values. So as we sort of saw before we increase the n bits value for good recall but at the same time we have that curse of dimensionality. So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality. Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t730.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 864, 'published': '2021-08-09 15:04:10 UTC', 'start': 730, 'text': ""So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality. Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,864,2021-08-09 15:04:10 UTC,730,"So if we are multiplying our dimensionality value d by 8 in order to get a good recall then if we have a dimensionality of 4 that's not a very high number so it's going to be reasonably fast but if we increase that to dimensionality for example 512 that becomes very, very complex very quickly. So you have to be careful with your dimensionality. Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t759.6400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 908, 'published': '2021-08-09 15:04:10 UTC', 'start': 759, 'text': ""Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,908,2021-08-09 15:04:10 UTC,759,"Lower dimensionality is very good for LSH otherwise it's not so good. You can see that here so at the bottom here I've used this is on the same data set so an n bits value of d multiplied by 2 with LSH it's super fast. It's faster than our flat index which is what you would hope but if we increase the n bits value quite a bit so maybe you want very high performance then it gets out of hand very quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t794.1800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 942, 'published': '2021-08-09 15:04:10 UTC', 'start': 794, 'text': ""quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,942,2021-08-09 15:04:10 UTC,794,"quickly and our search time just grows massively. So you kind of have to find that balance but what we got before was pretty good. We had a d multiplied by 4 I think and we got reasonable performance and it was fast so it's good. And that also applies to the index size as well so lower n bits size index size isn't too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t817.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 969, 'published': '2021-08-09 15:04:10 UTC', 'start': 817, 'text': ""too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,969,2021-08-09 15:04:10 UTC,817,"too bad. With higher n bits it's pretty huge so also something to think about. Now let's move on to HNSW. Now HNSW is what first part of it is NSW which is Navigo Small World Graphs. Now what makes a graph small world it essentially means that this graph can be very large but the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t850.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1004, 'published': '2021-08-09 15:04:10 UTC', 'start': 850, 'text': ""the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1004,2021-08-09 15:04:10 UTC,850,"the number of hops so the number of steps you need to take between any two vertexes which is the points is very low so in this example here we have this vertex over here and to get over to this one on the opposite side we need to take 1, 2, 3, 4 hops and this is obviously a very small network so it doesn't really count but you can see this sort of behaviour in very large networks so I think in 2016 there was a study from Facebook and at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately.",Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t895.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1035, 'published': '2021-08-09 15:04:10 UTC', 'start': 895, 'text': ""at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately. EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1035,2021-08-09 15:04:10 UTC,895,at that point I don't remember the exact number of people that they had on the platform but I think it's in the billions and they found that the average number of hops that you need to take between any two people on the platform is like 3.6 so that's a very good example of a Navigo Small World Graph. Now hierarchical NSW graphs which is what we are using they're built in the same way like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately. EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t927.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1071, 'published': '2021-08-09 15:04:10 UTC', 'start': 927, 'text': ""like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately. EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1071,2021-08-09 15:04:10 UTC,927,like a NSW graph but then they're split across multiple layers which is what you can see here and when we are performing our search the path it takes will hop between different layers in order to find our nearest neighbour. Now it's pretty complicated and this is really I think over simplifying it a lot but that's the general gist of it I'm not going to go any further into it we will I think in a future video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately. EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one.,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t959.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1093, 'published': '2021-08-09 15:04:10 UTC', 'start': 959, 'text': ""video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately. EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one. So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1093,2021-08-09 15:04:10 UTC,959,video and article. Now let's put that together in code. So we have a few different variables here we have M which I'm going to set to 16 and M is the number of connections that each vertex has so of course that means greater connectivity we're probably going to find our nearest neighbours more accurately. EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one. So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t990.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1132, 'published': '2021-08-09 15:04:10 UTC', 'start': 990, 'text': ""EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one. So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1132,2021-08-09 15:04:10 UTC,990,EF search which is how what is the depth of our search every time we perform a search so we can set this to a higher value if we want to search more the network or a low value if we want to search less of the network obviously low values can be quicker high value can be more accurate. And then we have EF construction now this similar to EF search is how much of the network will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one. So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data.,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1021.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1166, 'published': '2021-08-09 15:04:10 UTC', 'start': 1021, 'text': ""will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one. So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data. So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1166,2021-08-09 15:04:10 UTC,1021,will we search but not during the actual search during the construction of the network. So this is essentially how efficiently and accurately are we going to build the network in the first place. So this will increase the add time but the search time it makes no difference on so it's good to use a high number I think for this one. So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data. So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1053.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1206, 'published': '2021-08-09 15:04:10 UTC', 'start': 1053, 'text': ""So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data. So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1206,2021-08-09 15:04:10 UTC,1053,So initialise our index and we have is FICE index HNSW flat so we can use different vectors here that we can I think PQ PQ there and essentially what that's going to do is make this search faster but slightly less accurate. Now this is already really fast with flats and that's all we're going to stick with but again like I said we will return to this at some point in future and cover it in a lot more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data. So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right.,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1085.6200000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1242, 'published': '2021-08-09 15:04:10 UTC', 'start': 1085, 'text': ""more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data. So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right. Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1242,2021-08-09 15:04:10 UTC,1085,more detail for sure. So dimensionality we need to pass in our M value here as well. Now we want to apply those two parameters so we have EF search which is obviously EF search and then we also have HNSW the obviously the EF construction. So that should be everything ready to go and all we want to do now is add our data. So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right. Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32.,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1123.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1266, 'published': '2021-08-09 15:04:10 UTC', 'start': 1123, 'text': ""So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right. Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32. This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1266,2021-08-09 15:04:10 UTC,1123,So index.add WB. Okay now like I said we have that EF construction we use a reasonably high value so you can see this is already taking a lot longer than the previous indexes to actually add our vectors into it but it's still not going to take that long. And then once it is done we are going to do our search just like we did every other time so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right. Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32. This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1151.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1291, 'published': '2021-08-09 15:04:10 UTC', 'start': 1151, 'text': ""so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right. Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32. This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1291,2021-08-09 15:04:10 UTC,1151,so we have DI equals search sorry index.search and we are going to pass in our query and also K. Okay so 43.6 seconds to add the vectors there so a fair bit longer but then look at this super fast like that 3.7 milliseconds. So much faster than the last one I think the last one was 16 milliseconds right. Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32. This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1185.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1320, 'published': '2021-08-09 15:04:10 UTC', 'start': 1185, 'text': ""Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32. This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1320,2021-08-09 15:04:10 UTC,1185,Okay this is a flat index 157 LSH we have 17.6 okay so really quick which is cool. But how's the performance so let's have a look. Okay so we get quite a few false's here and only a couple of true's so okay it's not so great it was really fast but it's not very accurate but fortunately we can fix that so let's increase our EF search I'm going to increase it a fair bit let's go 32.32. This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1231.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1349, 'published': '2021-08-09 15:04:10 UTC', 'start': 1231, 'text': ""This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1349,2021-08-09 15:04:10 UTC,1231,This is probably I would imagine more than enough to get good performance so run this and run this. Okay and now we see we get pretty good results. Now the war time is higher so it's just a case of balancing it because this is now higher than LSH but what we can do is increase EF construction time the value of EF construction increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1257.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1380, 'published': '2021-08-09 15:04:10 UTC', 'start': 1257, 'text': ""increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1380,2021-08-09 15:04:10 UTC,1257,increases or decreases depending on what you want. There's a lot of flexibility with this and it can be really fast this is HNSW is essentially one of the best performing indexes that you can use if you look at the current state of the art a lot of them are HNSW or they're based on HNSW in some way or another. So these are good ones good ones to go with you just need to play around them a little bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1281.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1410, 'published': '2021-08-09 15:04:10 UTC', 'start': 1281, 'text': ""bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells."", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1410,2021-08-09 15:04:10 UTC,1281,bit. So this is a few of the performance I found using the same data set but I'm messing around so we have the EF construction values down here so start with 16 over here up to 64 EF search values over here and our M values over here. And we've got pretty good recall over 64 on the EF construction so EF construction is a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells.,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1312.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1438, 'published': '2021-08-09 15:04:10 UTC', 'start': 1312, 'text': ""a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells. This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1438,2021-08-09 15:04:10 UTC,1312,a really good one to just increase because it doesn't increase your search time which is pretty cool I think. And then here is the search time again for HNSW M and EF search obviously I didn't include EF construction there because it doesn't make a difference. And that's this is the one thing with HNSW the index size is absolutely huge so that's just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells. This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1338.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1469, 'published': '2021-08-09 15:04:10 UTC', 'start': 1338, 'text': ""just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells. This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1469,2021-08-09 15:04:10 UTC,1338,just one thing to bear in mind the index size can take a lot of memory but otherwise really really cool index. And then that leaves us on to our final index which is the IVF index and this is super popular and with good reasons it is very good. So the inverted file index is based on essentially clustering data points so you see here we have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells. This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1370.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1497, 'published': '2021-08-09 15:04:10 UTC', 'start': 1370, 'text': ""have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells. This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1497,2021-08-09 15:04:10 UTC,1370,have all of these different data points the little crosses and then we have these three other points which are going to be our cluster centroids. So around each or based in each of our centroids we expand a catchment radius around each of those and as you can see here where each of those circles collides it creates the edge of what are going to be our almost like catchment cells. This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1399.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1531, 'published': '2021-08-09 15:04:10 UTC', 'start': 1399, 'text': ""This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1531,2021-08-09 15:04:10 UTC,1399,This is called a Voronoi diagram or try it's a really hard word Dirichlet tessellation I don't know if that's correct but it sounds I think it sounds pretty cool so I thought I'd throw that in there. So we create these cells in each one of those cells any data point within those cells will be allocated to that given centroid and then when you search within a specific cell you pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1428.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1566, 'published': '2021-08-09 15:04:10 UTC', 'start': 1428, 'text': ""pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1566,2021-08-09 15:04:10 UTC,1428,pass your XQ value in there and that will be compared the XQ value will be compared to every single cluster centroid but not the other values within that cluster or the other clusters only the cluster centroids and then from that you find out which centroid is the closest to your query vector and then what we do is we restrict our search scope to only the data points within that cluster or that cell and then we calculate the nearest vector so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1462.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1599, 'published': '2021-08-09 15:04:10 UTC', 'start': 1462, 'text': ""so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1599,2021-08-09 15:04:10 UTC,1462,so at this point we have all the vectors only within that cell and we compare all of those to our query vector. Now there is one problem with this which is called the edge problem. Now we're just showing this in two-dimensional space obviously in reality for example the data set we're using we have 128 dimensions so dimensionally the edge problem is kind of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1484.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1632, 'published': '2021-08-09 15:04:10 UTC', 'start': 1484, 'text': ""of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1632,2021-08-09 15:04:10 UTC,1484,of complicated when you think about it in the hundreds of dimensions but what this is is so with say with our query we find our query vectors right on the edge of one of the cells and if we sell n probe value so I mentioned n probe here that's how many cells we search if that is set to one it means that we're going to restrict our search to only that cell even though if you if you look at this we have two or we have I'm trying to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1516.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1665, 'published': '2021-08-09 15:04:10 UTC', 'start': 1516, 'text': ""to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1665,2021-08-09 15:04:10 UTC,1516,to think so this one for sure is closer to our query vector than any of the magenta data points and possibly also this one and this one but and maybe even this one but we're not going to consider any of those because we're restricting our search only to this cell so we're only going to look at you know these data points and also these over here so that's that's the edge problem but we can get around that by not just searching one cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1553.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1700, 'published': '2021-08-09 15:04:10 UTC', 'start': 1553, 'text': ""cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1700,2021-08-09 15:04:10 UTC,1553,cell but by searching quite a few so in this case our n probe value is eight and that means we're going to search eight of the nearest centroids or centroid cells and that's how IVF works let's go ahead and implement that in code so first thing we need to do is sell n list value which is the number of centroids that we will have within our within our data and then this time so this is a little bit different we need to set the the final vector search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1588.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1753, 'published': '2021-08-09 15:04:10 UTC', 'start': 1588, 'text': ""search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1753,2021-08-09 15:04:10 UTC,1588,search that we're going to do so we're this is kind of split into two different operations right so we're searching based on clusters and then we're actually comparing the full vectors within the selected clusters so we need to define how we're going to do that final that final search between our full vectors and our query vector so what we do is write vice so do index flat we're going to index five p you can use l2 as well we set our dimension it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1618.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1786, 'published': '2021-08-09 15:04:10 UTC', 'start': 1618, 'text': ""it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1786,2021-08-09 15:04:10 UTC,1618,it so we're just initializing a flat index there and then what we're going to do is feed that into our IVF index so our IVF index is vice index IVF and flat because we're using the flat indexes the flat vectors there we need to pass our quantizer so the this step here the other step to the search process the dimensionality and also our n list values of how many cells or clusters we're going to have in there and with this because we're clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1652.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1815, 'published': '2021-08-09 15:04:10 UTC', 'start': 1652, 'text': ""clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two so now it's faster that could have been a one-off sometimes occasionally you get a really slow search and just happens sometimes so this is so we set n probe to super fast and super accurate so that that's a very good index as well so these are the stats I got in terms of recall and search time in milliseconds for different n probe values and different endless values so again it's all it's just about balancing it again index size the only"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1815,2021-08-09 15:04:10 UTC,1652,clustering data we need to do something else so in fact let me show you so if we write index dot is trained we get this false if we wrote off any of our other indexes this would have been true because they don't need to be trained because we're not doing clustering or any other form of training or optimization there so what we need to do is we need to train our index before we use it so we write index train and we just pass all of our vectors into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two so now it's faster that could have been a one-off sometimes occasionally you get a really slow search and just happens sometimes so this is so we set n probe to super fast and super accurate so that that's a very good index as well so these are the stats I got in terms of recall and search time in milliseconds for different n probe values and different endless values so again it's all it's just about balancing it again index size the only,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1681.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1851, 'published': '2021-08-09 15:04:10 UTC', 'start': 1681, 'text': ""into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two so now it's faster that could have been a one-off sometimes occasionally you get a really slow search and just happens sometimes so this is so we set n probe to super fast and super accurate so that that's a very good index as well so these are the stats I got in terms of recall and search time in milliseconds for different n probe values and different endless values so again it's all it's just about balancing it again index size the only thing that affects your index size here is obviously the size of your data and the endless value but you can increase the endless value loads and the index size hardly increases so this is like increasing by 100 kilobytes per like double of the endless value so this is very it's like nothing so that's it for this video and we covered quite a lot so I'm gonna leave it there but I think these all these indexes are super useful and quite interesting"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1851,2021-08-09 15:04:10 UTC,1681,into that but it's very quick so it's not really an issue and then we do index add pass our data and then what we do one thing so I want to show you we have our n probe value we'll search with one for now so we search one cell and to search we write di as we have every other time search execute okay okay so I mean super fast 3.32 milliseconds I think that's maybe the fastest other than how bad performing or low quality hsw index so let's see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two so now it's faster that could have been a one-off sometimes occasionally you get a really slow search and just happens sometimes so this is so we set n probe to super fast and super accurate so that that's a very good index as well so these are the stats I got in terms of recall and search time in milliseconds for different n probe values and different endless values so again it's all it's just about balancing it again index size the only thing that affects your index size here is obviously the size of your data and the endless value but you can increase the endless value loads and the index size hardly increases so this is like increasing by 100 kilobytes per like double of the endless value so this is very it's like nothing so that's it for this video and we covered quite a lot so I'm gonna leave it there but I think these all these indexes are super useful and quite interesting,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
B7wmo_NImgM-t1732.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1884, 'published': '2021-08-09 15:04:10 UTC', 'start': 1732, 'text': ""see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two so now it's faster that could have been a one-off sometimes occasionally you get a really slow search and just happens sometimes so this is so we set n probe to super fast and super accurate so that that's a very good index as well so these are the stats I got in terms of recall and search time in milliseconds for different n probe values and different endless values so again it's all it's just about balancing it again index size the only thing that affects your index size here is obviously the size of your data and the endless value but you can increase the endless value loads and the index size hardly increases so this is like increasing by 100 kilobytes per like double of the endless value so this is very it's like nothing so that's it for this video and we covered quite a lot so I'm gonna leave it there but I think these all these indexes are super useful and quite interesting and figuring out just playing around with them like you see I've done loads with these these graphs just seeing what is faster what is slower what where the good quality is I'm just playing around the parameters and seeing what you can get out of it is super useful for actually understanding these now what I do want to do going forward is actually explore each one of these indexes in more depth because you've only covered them like"", 'title': 'Choosing Indexes for Similarity Search (Faiss in Python)', 'url': 'https://youtu.be/B7wmo_NImgM'}",UCv83tO5cePwHMt1952IVVHw,1884,2021-08-09 15:04:10 UTC,1732,see how how that's performed so you write mp dot in on d baseline hi you see it's not too bad to be fair like 50 50 almost so that's actually pretty good but what we can do if we want it to be even better is we increase the n probe value so let's go up to four so that's increased the wartime quite a bit so from like three to 125 which is now super slow actually but now we're getting perfect results we can maybe decrease that to two so now it's faster that could have been a one-off sometimes occasionally you get a really slow search and just happens sometimes so this is so we set n probe to super fast and super accurate so that that's a very good index as well so these are the stats I got in terms of recall and search time in milliseconds for different n probe values and different endless values so again it's all it's just about balancing it again index size the only thing that affects your index size here is obviously the size of your data and the endless value but you can increase the endless value loads and the index size hardly increases so this is like increasing by 100 kilobytes per like double of the endless value so this is very it's like nothing so that's it for this video and we covered quite a lot so I'm gonna leave it there but I think these all these indexes are super useful and quite interesting and figuring out just playing around with them like you see I've done loads with these these graphs just seeing what is faster what is slower what where the good quality is I'm just playing around the parameters and seeing what you can get out of it is super useful for actually understanding these now what I do want to do going forward is actually explore each one of these indexes in more depth because you've only covered them like,Choosing Indexes for Similarity Search (Faiss in Python),https://youtu.be/B7wmo_NImgM
x1lAcT3xl5M-t2.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 46, 'published': '2021-05-27 16:15:39 UTC', 'start': 2, 'text': ""Here we're going to have a look at how we can use NSP or Net Sentence Prediction to train a BERT model. Now in a previous video I covered how NSP works but I didn't really cover how you actually train a model using it. So that's what we're going to do here. So we're going to jump straight into it and we have this notebook. Here is the data that we're going to be using. I will load that in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,46,2021-05-27 16:15:39 UTC,2,Here we're going to have a look at how we can use NSP or Net Sentence Prediction to train a BERT model. Now in a previous video I covered how NSP works but I didn't really cover how you actually train a model using it. So that's what we're going to do here. So we're going to jump straight into it and we have this notebook. Here is the data that we're going to be using. I will load that in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t12.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 56, 'published': '2021-05-27 16:15:39 UTC', 'start': 12, 'text': ""a previous video I covered how NSP works but I didn't really cover how you actually train a model using it. So that's what we're going to do here. So we're going to jump straight into it and we have this notebook. Here is the data that we're going to be using. I will load that in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,56,2021-05-27 16:15:39 UTC,12,a previous video I covered how NSP works but I didn't really cover how you actually train a model using it. So that's what we're going to do here. So we're going to jump straight into it and we have this notebook. Here is the data that we're going to be using. I will load that in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t22.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 68, 'published': '2021-05-27 16:15:39 UTC', 'start': 22, 'text': ""So we're going to jump straight into it and we have this notebook. Here is the data that we're going to be using. I will load that in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,68,2021-05-27 16:15:39 UTC,22,So we're going to jump straight into it and we have this notebook. Here is the data that we're going to be using. I will load that in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t32.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 80, 'published': '2021-05-27 16:15:39 UTC', 'start': 32, 'text': ""in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,80,2021-05-27 16:15:39 UTC,32,in in a moment but first thing I want to do before doing that is import and initialise everything we need. So obviously when we are downloading that data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t42.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 90, 'published': '2021-05-27 16:15:39 UTC', 'start': 42, 'text': ""data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,90,2021-05-27 16:15:39 UTC,42,data I'm going to be using requests for that so I'm going to import requests. And for our actual training of the model we're going to be using both HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t52.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 104, 'published': '2021-05-27 16:15:39 UTC', 'start': 52, 'text': ""HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,104,2021-05-27 16:15:39 UTC,52,HuginFaces Transformers and PyTorch. So I need to import Transformers and I'm going to import a BERT tokeniser class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t62.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 116, 'published': '2021-05-27 16:15:39 UTC', 'start': 62, 'text': ""class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example. Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,116,2021-05-27 16:15:39 UTC,62,class and also a BERT for Net Sentence Prediction class. So BERT for Net Sentence Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example. Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t76.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 126, 'published': '2021-05-27 16:15:39 UTC', 'start': 76, 'text': ""Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example. Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well. Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,126,2021-05-27 16:15:39 UTC,76,"Prediction. And as well as that we need to import Torch. So once we've imported all those we can initialise our tokeniser and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example. Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well. Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t86.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 136, 'published': '2021-05-27 16:15:39 UTC', 'start': 86, 'text': ""and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example. Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well. Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,136,2021-05-27 16:15:39 UTC,86,"and model. So tokeniser equals BERT tokeniser from Pre-trained. I'm going to be using BERT based on case for this example. Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well. Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t100.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 146, 'published': '2021-05-27 16:15:39 UTC', 'start': 100, 'text': ""Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well. Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine. Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,146,2021-05-27 16:15:39 UTC,100,"Obviously you can use another BERT model if you'd like. So I'm going to copy that and initialise our model as well. Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine. Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t110.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 156, 'published': '2021-05-27 16:15:39 UTC', 'start': 110, 'text': ""Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine. Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,156,2021-05-27 16:15:39 UTC,110,"Okay. And we can run that. And now let's extract this data. So this warning here, don't need to worry about that. That's just saying if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine. Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t122.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 166, 'published': '2021-05-27 16:15:39 UTC', 'start': 122, 'text': ""if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine. Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along. And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,166,2021-05-27 16:15:39 UTC,122,"if you are using this model for inference you shouldn't because you need to train it a little bit. So we don't need to worry about that, that's fine. Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along. And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t132.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 176, 'published': '2021-05-27 16:15:39 UTC', 'start': 132, 'text': ""Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along. And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,176,2021-05-27 16:15:39 UTC,132,"Because we are going to be training. Now what we do need to do is get this data. So new data equals requests.get and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along. And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t142.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 186, 'published': '2021-05-27 16:15:39 UTC', 'start': 142, 'text': ""and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along. And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs. This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,186,2021-05-27 16:15:39 UTC,142,"and we just take this link. I will keep this link in the description for you so you can just copy it across if you want to follow along. And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs. This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t152.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 196, 'published': '2021-05-27 16:15:39 UTC', 'start': 152, 'text': ""And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs. This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,196,2021-05-27 16:15:39 UTC,152,"And we should see that we get a 200 response there, that's good. So all we need to do is extract the text from that and we're going to store it in another variable here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs. This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t162.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 206, 'published': '2021-05-27 16:15:39 UTC', 'start': 162, 'text': ""here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs. This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,206,2021-05-27 16:15:39 UTC,162,"here, text variable. And if we go back to our text variable and if we just have a quick look at what we have in there we see that we have all of these paragraphs. This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t172.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 218, 'published': '2021-05-27 16:15:39 UTC', 'start': 172, 'text': ""This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data,"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,218,2021-05-27 16:15:39 UTC,172,"This is from the Meditations by Marcus Aurelius. You can get that book online, that's why I'm using it. And the language is a bit unique as well so that's why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data,",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t182.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 228, 'published': '2021-05-27 16:15:39 UTC', 'start': 182, 'text': ""why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data, that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,228,2021-05-27 16:15:39 UTC,182,"why I want to use it because when we're using net sentence prediction we're training the BERT model to better comprehend the style of language that we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data, that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t192.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 238, 'published': '2021-05-27 16:15:39 UTC', 'start': 192, 'text': ""we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data, that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,238,2021-05-27 16:15:39 UTC,192,"we train it on. So in here we have our paragraphs and they're all separated by a new line character. So I'm just going to add another little bit of code here which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data, that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t202.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 248, 'published': '2021-05-27 16:15:39 UTC', 'start': 202, 'text': ""which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data, that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,248,2021-05-27 16:15:39 UTC,202,"which is a split by new line character. And we have a look here, we now have a list containing paragraphs and that's our training data, that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t212.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 258, 'published': '2021-05-27 16:15:39 UTC', 'start': 212, 'text': ""that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be just choosing a random sentence and pulling that in and using that. So to do that we first"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,258,2021-05-27 16:15:39 UTC,212,that's what we want to be using. So when we're using NSP we want to create a 50-50 split of sentences that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be just choosing a random sentence and pulling that in and using that. So to do that we first,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t224.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 268, 'published': '2021-05-27 16:15:39 UTC', 'start': 224, 'text': ""that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be just choosing a random sentence and pulling that in and using that. So to do that we first want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,268,2021-05-27 16:15:39 UTC,224,that are random and sentences that are not random. So we're going to be taking sentence A's and 50% of the time we're going to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be just choosing a random sentence and pulling that in and using that. So to do that we first want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t234.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 278, 'published': '2021-05-27 16:15:39 UTC', 'start': 234, 'text': ""to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be just choosing a random sentence and pulling that in and using that. So to do that we first want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,278,2021-05-27 16:15:39 UTC,234,to be adding the genuine sentence B for that sentence A's e.g. the sentence that follows it in the text. And then the other 50% of the time we're going to be just choosing a random sentence and pulling that in and using that. So to do that we first want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t244.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 288, 'published': '2021-05-27 16:15:39 UTC', 'start': 244, 'text': ""just choosing a random sentence and pulling that in and using that. So to do that we first want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,288,2021-05-27 16:15:39 UTC,244,just choosing a random sentence and pulling that in and using that. So to do that we first want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t254.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 298, 'published': '2021-05-27 16:15:39 UTC', 'start': 254, 'text': ""want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So what I'm going to do is loop through our text here so the text variable split every"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,298,2021-05-27 16:15:39 UTC,254,want a bag of sentences to actually pull that text from. So the reason we can't just use text directly is because if we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So what I'm going to do is loop through our text here so the text variable split every,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t264.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 308, 'published': '2021-05-27 16:15:39 UTC', 'start': 264, 'text': ""we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So what I'm going to do is loop through our text here so the text variable split every sentence by a by the period characters and append all of those to a new list so a flat list containing"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,308,2021-05-27 16:15:39 UTC,264,we for example look at this we see that we have multiple sentences in this single paragraph. So I just split by period we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So what I'm going to do is loop through our text here so the text variable split every sentence by a by the period characters and append all of those to a new list so a flat list containing,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t274.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 318, 'published': '2021-05-27 16:15:39 UTC', 'start': 274, 'text': ""we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So what I'm going to do is loop through our text here so the text variable split every sentence by a by the period characters and append all of those to a new list so a flat list containing just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,318,2021-05-27 16:15:39 UTC,274,we get this one two three four so we get four sentences and this empty one at the end as well which we need to remove. So what I'm going to do is loop through our text here so the text variable split every sentence by a by the period characters and append all of those to a new list so a flat list containing just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t284.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 328, 'published': '2021-05-27 16:15:39 UTC', 'start': 284, 'text': ""what I'm going to do is loop through our text here so the text variable split every sentence by a by the period characters and append all of those to a new list so a flat list containing just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,328,2021-05-27 16:15:39 UTC,284,what I'm going to do is loop through our text here so the text variable split every sentence by a by the period characters and append all of those to a new list so a flat list containing just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t294.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 338, 'published': '2021-05-27 16:15:39 UTC', 'start': 294, 'text': ""sentence by a by the period characters and append all of those to a new list so a flat list containing just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to go through each sentence. So we want each sentence from each paragraph so sentence"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,338,2021-05-27 16:15:39 UTC,294,sentence by a by the period characters and append all of those to a new list so a flat list containing just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to go through each sentence. So we want each sentence from each paragraph so sentence,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t304.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 348, 'published': '2021-05-27 16:15:39 UTC', 'start': 304, 'text': ""just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to go through each sentence. So we want each sentence from each paragraph so sentence sorry sentence for each paragraph in the text for the sentences so for sentence"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,348,2021-05-27 16:15:39 UTC,304,just sentences so no paragraphs just sentences. And at the same time we'll need to make sure we don't include these empty ones because we get those with almost I think actually every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to go through each sentence. So we want each sentence from each paragraph so sentence sorry sentence for each paragraph in the text for the sentences so for sentence,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t314.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 358, 'published': '2021-05-27 16:15:39 UTC', 'start': 314, 'text': ""every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to go through each sentence. So we want each sentence from each paragraph so sentence sorry sentence for each paragraph in the text for the sentences so for sentence in so this is where we're getting sentences from so paragraph dot split we split by the period."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,358,2021-05-27 16:15:39 UTC,314,every paragraph in there. So we need to make sure we don't include those. Now to create this bag we write something like this so we want to go through each sentence. So we want each sentence from each paragraph so sentence sorry sentence for each paragraph in the text for the sentences so for sentence in so this is where we're getting sentences from so paragraph dot split we split by the period.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t324.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 368, 'published': '2021-05-27 16:15:39 UTC', 'start': 324, 'text': ""go through each sentence. So we want each sentence from each paragraph so sentence sorry sentence for each paragraph in the text for the sentences so for sentence in so this is where we're getting sentences from so paragraph dot split we split by the period. And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,368,2021-05-27 16:15:39 UTC,324,go through each sentence. So we want each sentence from each paragraph so sentence sorry sentence for each paragraph in the text for the sentences so for sentence in so this is where we're getting sentences from so paragraph dot split we split by the period. And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t334.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 378, 'published': '2021-05-27 16:15:39 UTC', 'start': 334, 'text': ""sorry sentence for each paragraph in the text for the sentences so for sentence in so this is where we're getting sentences from so paragraph dot split we split by the period. And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence is not equal to that and that should be OK. So let's check length. OK so"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,378,2021-05-27 16:15:39 UTC,334,sorry sentence for each paragraph in the text for the sentences so for sentence in so this is where we're getting sentences from so paragraph dot split we split by the period. And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence is not equal to that and that should be OK. So let's check length. OK so,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t344.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 388, 'published': '2021-05-27 16:15:39 UTC', 'start': 344, 'text': ""in so this is where we're getting sentences from so paragraph dot split we split by the period. And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence is not equal to that and that should be OK. So let's check length. OK so we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,388,2021-05-27 16:15:39 UTC,344,in so this is where we're getting sentences from so paragraph dot split we split by the period. And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence is not equal to that and that should be OK. So let's check length. OK so we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t354.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 398, 'published': '2021-05-27 16:15:39 UTC', 'start': 354, 'text': ""And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence is not equal to that and that should be OK. So let's check length. OK so we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later. So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,398,2021-05-27 16:15:39 UTC,354,And as well as that we also need to add that condition that we don't want any sentences that look like this. So we just add that in so if sentence is not equal to that and that should be OK. So let's check length. OK so we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later. So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t364.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 408, 'published': '2021-05-27 16:15:39 UTC', 'start': 364, 'text': ""is not equal to that and that should be OK. So let's check length. OK so we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later. So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,408,2021-05-27 16:15:39 UTC,364,is not equal to that and that should be OK. So let's check length. OK so we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later. So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t374.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 418, 'published': '2021-05-27 16:15:39 UTC', 'start': 374, 'text': ""we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later. So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,418,2021-05-27 16:15:39 UTC,374,we get 1372 sentences from that. And we'll actually want to save this to a parameter because we're using it later. So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t384.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 428, 'published': '2021-05-27 16:15:39 UTC', 'start': 384, 'text': ""So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,428,2021-05-27 16:15:39 UTC,384,So we now have the 1300 sentences to sample from and now we want to do is loop through each sentence within text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t394.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 438, 'published': '2021-05-27 16:15:39 UTC', 'start': 394, 'text': ""text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to create that random 50 percent we import the random library and we also want to initialize our sentence"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,438,2021-05-27 16:15:39 UTC,394,text or each paragraph within text choose a sentence from each paragraph if there's multiple paragraphs only multiple sentences only and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to create that random 50 percent we import the random library and we also want to initialize our sentence,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t404.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 450, 'published': '2021-05-27 16:15:39 UTC', 'start': 404, 'text': 'and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to create that random 50 percent we import the random library and we also want to initialize our sentence a list sentence b list. And we also need to initialize a label list. OK so', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,450,2021-05-27 16:15:39 UTC,404,and then 50 percent of the time select a random sentence from a bag and append that to the end 50 percent of the time append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to create that random 50 percent we import the random library and we also want to initialize our sentence a list sentence b list. And we also need to initialize a label list. OK so,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t414.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 460, 'published': '2021-05-27 16:15:39 UTC', 'start': 414, 'text': ""append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to create that random 50 percent we import the random library and we also want to initialize our sentence a list sentence b list. And we also need to initialize a label list. OK so that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,460,2021-05-27 16:15:39 UTC,414,append the actual genuine net sentence onto it and then we create labels as to whether we have randomized it or not randomized it. So to create that random 50 percent we import the random library and we also want to initialize our sentence a list sentence b list. And we also need to initialize a label list. OK so that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t424.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 472, 'published': '2021-05-27 16:15:39 UTC', 'start': 424, 'text': ""create that random 50 percent we import the random library and we also want to initialize our sentence a list sentence b list. And we also need to initialize a label list. OK so that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text and then here we extract our sentences like we did with the bag before so we go sentences equals and here we"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,472,2021-05-27 16:15:39 UTC,424,create that random 50 percent we import the random library and we also want to initialize our sentence a list sentence b list. And we also need to initialize a label list. OK so that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text and then here we extract our sentences like we did with the bag before so we go sentences equals and here we,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t434.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 484, 'published': '2021-05-27 16:15:39 UTC', 'start': 434, 'text': ""a list sentence b list. And we also need to initialize a label list. OK so that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text and then here we extract our sentences like we did with the bag before so we go sentences equals and here we want to write sentence for sentence in paragraph dot split and remember we have those"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,484,2021-05-27 16:15:39 UTC,434,a list sentence b list. And we also need to initialize a label list. OK so that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text and then here we extract our sentences like we did with the bag before so we go sentences equals and here we want to write sentence for sentence in paragraph dot split and remember we have those,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t446.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 494, 'published': '2021-05-27 16:15:39 UTC', 'start': 446, 'text': ""that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text and then here we extract our sentences like we did with the bag before so we go sentences equals and here we want to write sentence for sentence in paragraph dot split and remember we have those random empty sentences we don't want to include those so we write if sentence is not equal to that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,494,2021-05-27 16:15:39 UTC,446,that'll be 0 or 1. Now what we want to do is loop through each paragraph in our text so for paragraph in text and then here we extract our sentences like we did with the bag before so we go sentences equals and here we want to write sentence for sentence in paragraph dot split and remember we have those random empty sentences we don't want to include those so we write if sentence is not equal to that,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t456.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 504, 'published': '2021-05-27 16:15:39 UTC', 'start': 456, 'text': ""and then here we extract our sentences like we did with the bag before so we go sentences equals and here we want to write sentence for sentence in paragraph dot split and remember we have those random empty sentences we don't want to include those so we write if sentence is not equal to that empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,504,2021-05-27 16:15:39 UTC,456,and then here we extract our sentences like we did with the bag before so we go sentences equals and here we want to write sentence for sentence in paragraph dot split and remember we have those random empty sentences we don't want to include those so we write if sentence is not equal to that empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t468.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 514, 'published': '2021-05-27 16:15:39 UTC', 'start': 468, 'text': ""want to write sentence for sentence in paragraph dot split and remember we have those random empty sentences we don't want to include those so we write if sentence is not equal to that empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences. Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,514,2021-05-27 16:15:39 UTC,468,want to write sentence for sentence in paragraph dot split and remember we have those random empty sentences we don't want to include those so we write if sentence is not equal to that empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences. Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t480.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 524, 'published': '2021-05-27 16:15:39 UTC', 'start': 480, 'text': ""random empty sentences we don't want to include those so we write if sentence is not equal to that empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences. Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So we'll do number of sentences equals the length of sentences and then we say if"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,524,2021-05-27 16:15:39 UTC,480,random empty sentences we don't want to include those so we write if sentence is not equal to that empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences. Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So we'll do number of sentences equals the length of sentences and then we say if,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t490.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 536, 'published': '2021-05-27 16:15:39 UTC', 'start': 490, 'text': ""empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences. Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So we'll do number of sentences equals the length of sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,536,2021-05-27 16:15:39 UTC,490,empty sentence. So we have now so we're now looping through each paragraph and we've split each paragraph into sentences. Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So we'll do number of sentences equals the length of sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t500.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 546, 'published': '2021-05-27 16:15:39 UTC', 'start': 500, 'text': ""Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So we'll do number of sentences equals the length of sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,546,2021-05-27 16:15:39 UTC,500,Now what we want to do is check if that paragraph e.g. our new sentences variable has more than one sentence. So we'll do number of sentences equals the length of sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t510.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 556, 'published': '2021-05-27 16:15:39 UTC', 'start': 510, 'text': ""we'll do number of sentences equals the length of sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,556,2021-05-27 16:15:39 UTC,510,we'll do number of sentences equals the length of sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t520.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 566, 'published': '2021-05-27 16:15:39 UTC', 'start': 520, 'text': ""number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,566,2021-05-27 16:15:39 UTC,520,number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t532.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 576, 'published': '2021-05-27 16:15:39 UTC', 'start': 532, 'text': ""and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,576,2021-05-27 16:15:39 UTC,532,and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t542.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 586, 'published': '2021-05-27 16:15:39 UTC', 'start': 542, 'text': ""we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,586,2021-05-27 16:15:39 UTC,542,we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t552.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 596, 'published': '2021-05-27 16:15:39 UTC', 'start': 552, 'text': ""sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will start from. So we write start equals random randint so this is only if we have more than one"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,596,2021-05-27 16:15:39 UTC,552,sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will start from. So we write start equals random randint so this is only if we have more than one,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t562.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 606, 'published': '2021-05-27 16:15:39 UTC', 'start': 562, 'text': ""for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will start from. So we write start equals random randint so this is only if we have more than one sentence remember. So our random randint so this is going to be the start sentence in the case that we use"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,606,2021-05-27 16:15:39 UTC,562,for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will start from. So we write start equals random randint so this is only if we have more than one sentence remember. So our random randint so this is going to be the start sentence in the case that we use,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t572.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 618, 'published': '2021-05-27 16:15:39 UTC', 'start': 572, 'text': ""can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will start from. So we write start equals random randint so this is only if we have more than one sentence remember. So our random randint so this is going to be the start sentence in the case that we use sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,618,2021-05-27 16:15:39 UTC,572,can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will start from. So we write start equals random randint so this is only if we have more than one sentence remember. So our random randint so this is going to be the start sentence in the case that we use sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t582.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 628, 'published': '2021-05-27 16:15:39 UTC', 'start': 582, 'text': ""start from. So we write start equals random randint so this is only if we have more than one sentence remember. So our random randint so this is going to be the start sentence in the case that we use sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences. So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,628,2021-05-27 16:15:39 UTC,582,start from. So we write start equals random randint so this is only if we have more than one sentence remember. So our random randint so this is going to be the start sentence in the case that we use sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences. So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t592.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 638, 'published': '2021-05-27 16:15:39 UTC', 'start': 592, 'text': ""sentence remember. So our random randint so this is going to be the start sentence in the case that we use sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences. So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have 0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,638,2021-05-27 16:15:39 UTC,592,"sentence remember. So our random randint so this is going to be the start sentence in the case that we use sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences. So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have 0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t602.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 648, 'published': '2021-05-27 16:15:39 UTC', 'start': 602, 'text': ""sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences. So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have 0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,648,2021-05-27 16:15:39 UTC,602,"sentence A and B consecutively so we don't randomise sentence B we want to make sure that we have enough space at the end of our sentences. So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have 0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t612.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 658, 'published': '2021-05-27 16:15:39 UTC', 'start': 612, 'text': ""So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have 0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,658,2021-05-27 16:15:39 UTC,612,"So here to take both sentence A and sentence B. So let's say for example we have I'm going to use an example here so we have 0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t624.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 668, 'published': '2021-05-27 16:15:39 UTC', 'start': 624, 'text': ""0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,668,2021-05-27 16:15:39 UTC,624,"0, 1, 2, 3, 4 let's say this is our paragraph we have 5 sentences in here we want the start sentence to sentence A if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t634.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 678, 'published': '2021-05-27 16:15:39 UTC', 'start': 634, 'text': ""if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,678,2021-05-27 16:15:39 UTC,634,if we select 4 then we don't have a sentence B to select from. So what we are going to do here is say you choose a random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t644.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 688, 'published': '2021-05-27 16:15:39 UTC', 'start': 644, 'text': ""random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2. Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,688,2021-05-27 16:15:39 UTC,644,random integer between 0 and we want 3 to be the maximum there. So how do we do that? We've got the number of sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2. Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t654.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 698, 'published': '2021-05-27 16:15:39 UTC', 'start': 654, 'text': ""sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2. Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B. So if random. random so this will just select random float between values 0 and 1. If that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,698,2021-05-27 16:15:39 UTC,654,sentences here so this value will be 5 in this case so we would say number of sentences is 5 minus 2 because we don't the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2. Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B. So if random. random so this will just select random float between values 0 and 1. If that,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t664.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 708, 'published': '2021-05-27 16:15:39 UTC', 'start': 664, 'text': ""the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2. Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B. So if random. random so this will just select random float between values 0 and 1. If that is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,708,2021-05-27 16:15:39 UTC,664,the maximum value we want to select is 3 in this case. So it's going to be the number of sentences minus 2. Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B. So if random. random so this will just select random float between values 0 and 1. If that is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t674.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 718, 'published': '2021-05-27 16:15:39 UTC', 'start': 674, 'text': ""Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B. So if random. random so this will just select random float between values 0 and 1. If that is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection what we do is sentence B .append and then here we would append a random sentence from our bag up here."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,718,2021-05-27 16:15:39 UTC,674,Ok. Now what we do is we do our 50-50 randomize or not randomize sentence B. So if random. random so this will just select random float between values 0 and 1. If that is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection what we do is sentence B .append and then here we would append a random sentence from our bag up here.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t684.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 728, 'published': '2021-05-27 16:15:39 UTC', 'start': 684, 'text': ""So if random. random so this will just select random float between values 0 and 1. If that is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection what we do is sentence B .append and then here we would append a random sentence from our bag up here. So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,728,2021-05-27 16:15:39 UTC,684,So if random. random so this will just select random float between values 0 and 1. If that is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection what we do is sentence B .append and then here we would append a random sentence from our bag up here. So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t694.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 738, 'published': '2021-05-27 16:15:39 UTC', 'start': 694, 'text': ""is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection what we do is sentence B .append and then here we would append a random sentence from our bag up here. So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So we're going to use that same function. So random. randint and that needs to be between 0 and the length of"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,738,2021-05-27 16:15:39 UTC,694,is greater than 0.5 then let's say we'll make this our random selection. Ok. So for the random selection what we do is sentence B .append and then here we would append a random sentence from our bag up here. So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So we're going to use that same function. So random. randint and that needs to be between 0 and the length of,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t704.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 748, 'published': '2021-05-27 16:15:39 UTC', 'start': 704, 'text': ""what we do is sentence B .append and then here we would append a random sentence from our bag up here. So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So we're going to use that same function. So random. randint and that needs to be between 0 and the length of our bag minus 1. So we use bag size that's why we have it. So bag size minus 1."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,748,2021-05-27 16:15:39 UTC,704,what we do is sentence B .append and then here we would append a random sentence from our bag up here. So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So we're going to use that same function. So random. randint and that needs to be between 0 and the length of our bag minus 1. So we use bag size that's why we have it. So bag size minus 1.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t714.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 758, 'published': '2021-05-27 16:15:39 UTC', 'start': 714, 'text': ""So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So we're going to use that same function. So random. randint and that needs to be between 0 and the length of our bag minus 1. So we use bag size that's why we have it. So bag size minus 1. Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,758,2021-05-27 16:15:39 UTC,714,So to do that we would just write bag and then in here we need to select a random integer like we did up here. Ok. So we're going to use that same function. So random. randint and that needs to be between 0 and the length of our bag minus 1. So we use bag size that's why we have it. So bag size minus 1. Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t724.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 768, 'published': '2021-05-27 16:15:39 UTC', 'start': 724, 'text': ""we're going to use that same function. So random. randint and that needs to be between 0 and the length of our bag minus 1. So we use bag size that's why we have it. So bag size minus 1. Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label. So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,768,2021-05-27 16:15:39 UTC,724,we're going to use that same function. So random. randint and that needs to be between 0 and the length of our bag minus 1. So we use bag size that's why we have it. So bag size minus 1. Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label. So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t734.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 778, 'published': '2021-05-27 16:15:39 UTC', 'start': 734, 'text': ""our bag minus 1. So we use bag size that's why we have it. So bag size minus 1. Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label. So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,778,2021-05-27 16:15:39 UTC,734,our bag minus 1. So we use bag size that's why we have it. So bag size minus 1. Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label. So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t744.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 788, 'published': '2021-05-27 16:15:39 UTC', 'start': 744, 'text': ""Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label. So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,788,2021-05-27 16:15:39 UTC,744,Ok. Now we'll select a random sentence B from that bag for us. And as well as that we also want to set a label. So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t754.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 798, 'published': '2021-05-27 16:15:39 UTC', 'start': 754, 'text': ""So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append up here. And this is just going to be sentences. And in the index we have start which is"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,798,2021-05-27 16:15:39 UTC,754,So our label in this case would be a 1. So we have the 0 which means it is the next sentence. We have a 1 which means it is not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append up here. And this is just going to be sentences. And in the index we have start which is,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t764.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 808, 'published': '2021-05-27 16:15:39 UTC', 'start': 764, 'text': ""not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append up here. And this is just going to be sentences. And in the index we have start which is our value from here. Ok. So we have the random option. Now let's do our not random option."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,808,2021-05-27 16:15:39 UTC,764,not the next sentence. So we set 1. Now our sentence A it our sentence A gets selected. It's the same thing no matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append up here. And this is just going to be sentences. And in the index we have start which is our value from here. Ok. So we have the random option. Now let's do our not random option.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t774.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 820, 'published': '2021-05-27 16:15:39 UTC', 'start': 774, 'text': ""matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append up here. And this is just going to be sentences. And in the index we have start which is our value from here. Ok. So we have the random option. Now let's do our not random option. So in here we write sentence B append. And this needs to append sentences start plus"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,820,2021-05-27 16:15:39 UTC,774,matter whether we have the random sentence B or the not random sentence B. So we can actually write our sentence A append up here. And this is just going to be sentences. And in the index we have start which is our value from here. Ok. So we have the random option. Now let's do our not random option. So in here we write sentence B append. And this needs to append sentences start plus,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t784.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 832, 'published': '2021-05-27 16:15:39 UTC', 'start': 784, 'text': ""up here. And this is just going to be sentences. And in the index we have start which is our value from here. Ok. So we have the random option. Now let's do our not random option. So in here we write sentence B append. And this needs to append sentences start plus 1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,832,2021-05-27 16:15:39 UTC,784,up here. And this is just going to be sentences. And in the index we have start which is our value from here. Ok. So we have the random option. Now let's do our not random option. So in here we write sentence B append. And this needs to append sentences start plus 1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t794.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 842, 'published': '2021-05-27 16:15:39 UTC', 'start': 794, 'text': ""our value from here. Ok. So we have the random option. Now let's do our not random option. So in here we write sentence B append. And this needs to append sentences start plus 1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence. So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,842,2021-05-27 16:15:39 UTC,794,our value from here. Ok. So we have the random option. Now let's do our not random option. So in here we write sentence B append. And this needs to append sentences start plus 1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence. So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t804.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 852, 'published': '2021-05-27 16:15:39 UTC', 'start': 804, 'text': ""So in here we write sentence B append. And this needs to append sentences start plus 1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence. So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,852,2021-05-27 16:15:39 UTC,804,So in here we write sentence B append. And this needs to append sentences start plus 1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence. So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t816.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 862, 'published': '2021-05-27 16:15:39 UTC', 'start': 816, 'text': ""1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence. So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there. Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,862,2021-05-27 16:15:39 UTC,816,"1. So the following sentence after our sentence A. And our label here would be 0 which means it is the next sentence. So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there. Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t828.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 872, 'published': '2021-05-27 16:15:39 UTC', 'start': 828, 'text': ""So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there. Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me print out what we have. So for i in range 3. So I'm just"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,872,2021-05-27 16:15:39 UTC,828,"So let's there's quite a lot of code. Let's run that and see what we get. Ok. Now what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there. Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me print out what we have. So for i in range 3. So I'm just",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t838.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 882, 'published': '2021-05-27 16:15:39 UTC', 'start': 838, 'text': ""what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there. Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me print out what we have. So for i in range 3. So I'm just doing this so we can print and see what we actually have in our training data. So I want to print the label at that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,882,2021-05-27 16:15:39 UTC,838,"what I want to do is let's have a look at the first few labels. See if we have a mix of different ones in there. Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me print out what we have. So for i in range 3. So I'm just doing this so we can print and see what we actually have in our training data. So I want to print the label at that",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t848.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 892, 'published': '2021-05-27 16:15:39 UTC', 'start': 848, 'text': ""Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me print out what we have. So for i in range 3. So I'm just doing this so we can print and see what we actually have in our training data. So I want to print the label at that index. And then I want to print the sentence A. At that index. And we'll follow that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,892,2021-05-27 16:15:39 UTC,848,"Ok. We just have 1, 1, 1. So I'm going to rerun this because I want to show you the difference between 0's and 1's here. Ok. So we have these. So let me print out what we have. So for i in range 3. So I'm just doing this so we can print and see what we actually have in our training data. So I want to print the label at that index. And then I want to print the sentence A. At that index. And we'll follow that",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t858.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 902, 'published': '2021-05-27 16:15:39 UTC', 'start': 858, 'text': ""print out what we have. So for i in range 3. So I'm just doing this so we can print and see what we actually have in our training data. So I want to print the label at that index. And then I want to print the sentence A. At that index. And we'll follow that with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,902,2021-05-27 16:15:39 UTC,858,print out what we have. So for i in range 3. So I'm just doing this so we can print and see what we actually have in our training data. So I want to print the label at that index. And then I want to print the sentence A. At that index. And we'll follow that with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t868.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 912, 'published': '2021-05-27 16:15:39 UTC', 'start': 868, 'text': ""doing this so we can print and see what we actually have in our training data. So I want to print the label at that index. And then I want to print the sentence A. At that index. And we'll follow that with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,912,2021-05-27 16:15:39 UTC,868,doing this so we can print and see what we actually have in our training data. So I want to print the label at that index. And then I want to print the sentence A. At that index. And we'll follow that with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t878.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 922, 'published': '2021-05-27 16:15:39 UTC', 'start': 878, 'text': ""index. And then I want to print the sentence A. At that index. And we'll follow that with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers. So see here that we have 0. We have our sentence A. And our sentence"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,922,2021-05-27 16:15:39 UTC,878,index. And then I want to print the sentence A. At that index. And we'll follow that with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers. So see here that we have 0. We have our sentence A. And our sentence,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t888.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 932, 'published': '2021-05-27 16:15:39 UTC', 'start': 888, 'text': ""with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers. So see here that we have 0. We have our sentence A. And our sentence B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,932,2021-05-27 16:15:39 UTC,888,with a new line character and a few dashes so we can distinguish between the start and end of sentence A and B. And then we will do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers. So see here that we have 0. We have our sentence A. And our sentence B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t898.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 942, 'published': '2021-05-27 16:15:39 UTC', 'start': 898, 'text': ""do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers. So see here that we have 0. We have our sentence A. And our sentence B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,942,2021-05-27 16:15:39 UTC,898,do print sentence B. And then I'm just going to add in a new line there to distinguish it from the next set of answers. So see here that we have 0. We have our sentence A. And our sentence B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t908.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 956, 'published': '2021-05-27 16:15:39 UTC', 'start': 908, 'text': ""So see here that we have 0. We have our sentence A. And our sentence B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've selected a random sentence B. And if we read this. I know it's not the easiest thing to read."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,956,2021-05-27 16:15:39 UTC,908,So see here that we have 0. We have our sentence A. And our sentence B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've selected a random sentence B. And if we read this. I know it's not the easiest thing to read.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t918.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 966, 'published': '2021-05-27 16:15:39 UTC', 'start': 918, 'text': ""B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've selected a random sentence B. And if we read this. I know it's not the easiest thing to read. Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,966,2021-05-27 16:15:39 UTC,918,B is a continuation of that first sentence. Because we have that label 0. We know that. So we have sentence A here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've selected a random sentence B. And if we read this. I know it's not the easiest thing to read. Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t928.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 976, 'published': '2021-05-27 16:15:39 UTC', 'start': 928, 'text': ""here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've selected a random sentence B. And if we read this. I know it's not the easiest thing to read. Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,976,2021-05-27 16:15:39 UTC,928,here. And again this one here is a continuation of this sentence A. And then down here we have a 1. So this is where we've selected a random sentence B. And if we read this. I know it's not the easiest thing to read. Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t938.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 986, 'published': '2021-05-27 16:15:39 UTC', 'start': 938, 'text': ""selected a random sentence B. And if we read this. I know it's not the easiest thing to read. Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,986,2021-05-27 16:15:39 UTC,938,selected a random sentence B. And if we read this. I know it's not the easiest thing to read. Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t952.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 996, 'published': '2021-05-27 16:15:39 UTC', 'start': 952, 'text': ""Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,996,2021-05-27 16:15:39 UTC,952,Yeah the difference. There's reasonably clear difference in the context there. Okay. Now this won't always work. In some cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t962.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1006, 'published': '2021-05-27 16:15:39 UTC', 'start': 962, 'text': ""cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine. Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1006,2021-05-27 16:15:39 UTC,962,cases we might select even the same sentence for sentence A and B. But for what we're doing here I think this is a completely reasonable way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine. Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t972.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1016, 'published': '2021-05-27 16:15:39 UTC', 'start': 972, 'text': ""way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine. Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1016,2021-05-27 16:15:39 UTC,972,way of going about it. Because we don't want to over complicate things. If we wanted to really be very strict on it we could add in some extra logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine. Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t982.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1026, 'published': '2021-05-27 16:15:39 UTC', 'start': 982, 'text': ""logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine. Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1026,2021-05-27 16:15:39 UTC,982,logic which confirms that we are not getting a sentence B from around the same area as sentence A for example. But for now this is I think fine. Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t992.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1036, 'published': '2021-05-27 16:15:39 UTC', 'start': 992, 'text': ""Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return tensors Pt. And as well as that we need to truncate or pad each one of those sequences to"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1036,2021-05-27 16:15:39 UTC,992,Okay. So we've now prepared our data. What we need to do now is tokenize it. So to tokenize our data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return tensors Pt. And as well as that we need to truncate or pad each one of those sequences to,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1002.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1046, 'published': '2021-05-27 16:15:39 UTC', 'start': 1002, 'text': ""data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return tensors Pt. And as well as that we need to truncate or pad each one of those sequences to a maximum length of 512. And we truncate using this."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1046,2021-05-27 16:15:39 UTC,1002,data we're just going to use a tokenizer which we've already initialized. And in here we can actually just pass our sentence A and sentence B like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return tensors Pt. And as well as that we need to truncate or pad each one of those sequences to a maximum length of 512. And we truncate using this.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1012.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1056, 'published': '2021-05-27 16:15:39 UTC', 'start': 1012, 'text': ""like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return tensors Pt. And as well as that we need to truncate or pad each one of those sequences to a maximum length of 512. And we truncate using this. And we also set padding equal to max length. Okay. So that should be"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1056,2021-05-27 16:15:39 UTC,1012,like this. And our tokenizer will deal with how to fit both of those together for us. So that's pretty useful. We're going to be using PyTorch. So we want to return tensors Pt. And as well as that we need to truncate or pad each one of those sequences to a maximum length of 512. And we truncate using this. And we also set padding equal to max length. Okay. So that should be,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1022.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1066, 'published': '2021-05-27 16:15:39 UTC', 'start': 1022, 'text': ""tensors Pt. And as well as that we need to truncate or pad each one of those sequences to a maximum length of 512. And we truncate using this. And we also set padding equal to max length. Okay. So that should be okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1066,2021-05-27 16:15:39 UTC,1022,"tensors Pt. And as well as that we need to truncate or pad each one of those sequences to a maximum length of 512. And we truncate using this. And we also set padding equal to max length. Okay. So that should be okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1032.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1076, 'published': '2021-05-27 16:15:39 UTC', 'start': 1032, 'text': ""a maximum length of 512. And we truncate using this. And we also set padding equal to max length. Okay. So that should be okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1076,2021-05-27 16:15:39 UTC,1032,"a maximum length of 512. And we truncate using this. And we also set padding equal to max length. Okay. So that should be okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1042.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1086, 'published': '2021-05-27 16:15:39 UTC', 'start': 1042, 'text': ""And we also set padding equal to max length. Okay. So that should be okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few of those. Now our token type IDs what we would expect is sentence A would have a token type"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1086,2021-05-27 16:15:39 UTC,1042,"And we also set padding equal to max length. Okay. So that should be okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few of those. Now our token type IDs what we would expect is sentence A would have a token type",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1052.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1096, 'published': '2021-05-27 16:15:39 UTC', 'start': 1052, 'text': ""okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few of those. Now our token type IDs what we would expect is sentence A would have a token type ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1096,2021-05-27 16:15:39 UTC,1052,"okay. Let's have a look at what we have. We see that we have input IDs, token type IDs and attention mass. Let's have a look at what they look like. So you see here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few of those. Now our token type IDs what we would expect is sentence A would have a token type ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1062.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1106, 'published': '2021-05-27 16:15:39 UTC', 'start': 1062, 'text': ""here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few of those. Now our token type IDs what we would expect is sentence A would have a token type ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand that out a little bit. So we'll go with token type IDs. Let's go with number 0."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1106,2021-05-27 16:15:39 UTC,1062,here we have all these different vectors and that is a single pair of sentence A and sentence B. And we have quite a few of those. Now our token type IDs what we would expect is sentence A would have a token type ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand that out a little bit. So we'll go with token type IDs. Let's go with number 0.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1072.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1116, 'published': '2021-05-27 16:15:39 UTC', 'start': 1072, 'text': ""of those. Now our token type IDs what we would expect is sentence A would have a token type ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand that out a little bit. So we'll go with token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1116,2021-05-27 16:15:39 UTC,1072,of those. Now our token type IDs what we would expect is sentence A would have a token type ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand that out a little bit. So we'll go with token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1082.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1126, 'published': '2021-05-27 16:15:39 UTC', 'start': 1082, 'text': ""ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand that out a little bit. So we'll go with token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1126,2021-05-27 16:15:39 UTC,1082,ID of 0 and sentence B would have token type ID of 1. We don't see those ones in there. So let's expand that out a little bit. So we'll go with token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1092.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1136, 'published': '2021-05-27 16:15:39 UTC', 'start': 1092, 'text': ""that out a little bit. So we'll go with token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1136,2021-05-27 16:15:39 UTC,1092,that out a little bit. So we'll go with token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1102.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1146, 'published': '2021-05-27 16:15:39 UTC', 'start': 1102, 'text': ""Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1146,2021-05-27 16:15:39 UTC,1102,"Okay. So now we see okay the reason is because they're in the middle here. So what we're seeing here is sentence A followed by sentence B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1112.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1156, 'published': '2021-05-27 16:15:39 UTC', 'start': 1112, 'text': 'B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay. So we have our input tensors. We also need to build our labels tensor. And to do that', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1156,2021-05-27 16:15:39 UTC,1112,"B. And then these remaining 0 tokens are our padding tokens. So we can also see that if we switch across to input IDs we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay. So we have our input tensors. We also need to build our labels tensor. And to do that",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1122.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1166, 'published': '2021-05-27 16:15:39 UTC', 'start': 1122, 'text': 'we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay. So we have our input tensors. We also need to build our labels tensor. And to do that we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1166,2021-05-27 16:15:39 UTC,1122,"we see that we have all these padding tokens. And as well another item that the tokenizer does for us automatically is adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay. So we have our input tensors. We also need to build our labels tensor. And to do that we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1132.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1176, 'published': '2021-05-27 16:15:39 UTC', 'start': 1132, 'text': 'adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay. So we have our input tensors. We also need to build our labels tensor. And to do that we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch long tensor. And this is a little bit different. So let me just expand that out.', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1176,2021-05-27 16:15:39 UTC,1132,"adds a separator token in the middle of our sentence A and B. So sentence A is this, sentence B is this. Okay. So we have our input tensors. We also need to build our labels tensor. And to do that we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch long tensor. And this is a little bit different. So let me just expand that out.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1142.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1186, 'published': '2021-05-27 16:15:39 UTC', 'start': 1142, 'text': ""So we have our input tensors. We also need to build our labels tensor. And to do that we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch long tensor. And this is a little bit different. So let me just expand that out. So let's say we just add labels in here. So sorry label. And we just get this one big tensor"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1186,2021-05-27 16:15:39 UTC,1142,So we have our input tensors. We also need to build our labels tensor. And to do that we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch long tensor. And this is a little bit different. So let me just expand that out. So let's say we just add labels in here. So sorry label. And we just get this one big tensor,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1152.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1196, 'published': '2021-05-27 16:15:39 UTC', 'start': 1152, 'text': ""we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch long tensor. And this is a little bit different. So let me just expand that out. So let's say we just add labels in here. So sorry label. And we just get this one big tensor which is not really in the correct format that we need. We need each one of these to match to our"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1196,2021-05-27 16:15:39 UTC,1152,we just we add it to this inputs variable. So we have inputs labels. And we set that equal to torch long tensor. And this is a little bit different. So let me just expand that out. So let's say we just add labels in here. So sorry label. And we just get this one big tensor which is not really in the correct format that we need. We need each one of these to match to our,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1162.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1206, 'published': '2021-05-27 16:15:39 UTC', 'start': 1162, 'text': ""long tensor. And this is a little bit different. So let me just expand that out. So let's say we just add labels in here. So sorry label. And we just get this one big tensor which is not really in the correct format that we need. We need each one of these to match to our input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1206,2021-05-27 16:15:39 UTC,1162,"long tensor. And this is a little bit different. So let me just expand that out. So let's say we just add labels in here. So sorry label. And we just get this one big tensor which is not really in the correct format that we need. We need each one of these to match to our input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1172.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1216, 'published': '2021-05-27 16:15:39 UTC', 'start': 1172, 'text': ""So let's say we just add labels in here. So sorry label. And we just get this one big tensor which is not really in the correct format that we need. We need each one of these to match to our input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1216,2021-05-27 16:15:39 UTC,1172,"So let's say we just add labels in here. So sorry label. And we just get this one big tensor which is not really in the correct format that we need. We need each one of these to match to our input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1182.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1226, 'published': '2021-05-27 16:15:39 UTC', 'start': 1182, 'text': ""which is not really in the correct format that we need. We need each one of these to match to our input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see. So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1226,2021-05-27 16:15:39 UTC,1182,"which is not really in the correct format that we need. We need each one of these to match to our input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see. So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1192.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1236, 'published': '2021-05-27 16:15:39 UTC', 'start': 1192, 'text': ""input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see. So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1236,2021-05-27 16:15:39 UTC,1192,"input IDs, token type IDs and attention mask. So what I mean by that is if we just have a look at this input IDs you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see. So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1202.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1246, 'published': '2021-05-27 16:15:39 UTC', 'start': 1202, 'text': ""you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see. So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1246,2021-05-27 16:15:39 UTC,1202,you see that we get it's like a list within a list. We need that but for our labels as well. They're in a different format at the moment as you can see. So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1212.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1256, 'published': '2021-05-27 16:15:39 UTC', 'start': 1212, 'text': ""So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this almost vector of each of these and each one of these here. So this vector matches up to this value here."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1256,2021-05-27 16:15:39 UTC,1212,So we could try transposing that. But you see that doesn't actually do anything because it's just a single invention. So it's just switching everything around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this almost vector of each of these and each one of these here. So this vector matches up to this value here.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1222.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1268, 'published': '2021-05-27 16:15:39 UTC', 'start': 1222, 'text': ""around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this almost vector of each of these and each one of these here. So this vector matches up to this value here. And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1268,2021-05-27 16:15:39 UTC,1222,around. So let's remove that transpose and let's add a list inside here. You see now we're getting somewhere not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this almost vector of each of these and each one of these here. So this vector matches up to this value here. And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1232.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1278, 'published': '2021-05-27 16:15:39 UTC', 'start': 1232, 'text': ""not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this almost vector of each of these and each one of these here. So this vector matches up to this value here. And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1278,2021-05-27 16:15:39 UTC,1232,not quite there yet. So now we have a list within a list. And now what we do is we transpose it and now we get what we need. So we have this almost vector of each of these and each one of these here. So this vector matches up to this value here. And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1242.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1288, 'published': '2021-05-27 16:15:39 UTC', 'start': 1242, 'text': ""almost vector of each of these and each one of these here. So this vector matches up to this value here. And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1288,2021-05-27 16:15:39 UTC,1242,almost vector of each of these and each one of these here. So this vector matches up to this value here. And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1252.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1300, 'published': '2021-05-27 16:15:39 UTC', 'start': 1252, 'text': ""And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1300,2021-05-27 16:15:39 UTC,1252,And this one matches up to this one. And that's what we want. So let's copy that and put it here. So now we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1264.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1310, 'published': '2021-05-27 16:15:39 UTC', 'start': 1264, 'text': ""we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1310,2021-05-27 16:15:39 UTC,1264,we have all the sensors we need for training our model. And what we now need to do is set up the input pipeline for training. So when we're training we're going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1274.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1322, 'published': '2021-05-27 16:15:39 UTC', 'start': 1274, 'text': ""going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1322,2021-05-27 16:15:39 UTC,1274,going to need to use a PyTorch data loader object. And to create that data loader object we need to create a PyTorch data set object from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1284.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1332, 'published': '2021-05-27 16:15:39 UTC', 'start': 1284, 'text': ""from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1332,2021-05-27 16:15:39 UTC,1284,from our data. So to do that we write this. So we're going to be using a data set class here. So I'm going to call it MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1294.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1342, 'published': '2021-05-27 16:15:39 UTC', 'start': 1294, 'text': ""MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1342,2021-05-27 16:15:39 UTC,1294,MeditationsDataSet. And in here we write torch utils data data set. So that makes sure that we are using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1306.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1352, 'published': '2021-05-27 16:15:39 UTC', 'start': 1306, 'text': ""using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings equals encodings. So that allows us to create our data set class. And then our data loader needs two"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1352,2021-05-27 16:15:39 UTC,1306,using the correct format for the class for a data set. Now we need to define a few methods inside here. So our initialization method and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings equals encodings. So that allows us to create our data set class. And then our data loader needs two,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1318.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1364, 'published': '2021-05-27 16:15:39 UTC', 'start': 1318, 'text': ""and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings equals encodings. So that allows us to create our data set class. And then our data loader needs two different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1364,2021-05-27 16:15:39 UTC,1318,and for our initialization method we need to be able to pass our data. So we'll pass it through this encodings variable. And all we need to do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings equals encodings. So that allows us to create our data set class. And then our data loader needs two different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1328.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1374, 'published': '2021-05-27 16:15:39 UTC', 'start': 1328, 'text': ""do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings equals encodings. So that allows us to create our data set class. And then our data loader needs two different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier. So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1374,2021-05-27 16:15:39 UTC,1328,do in here is assign our encodings variable to be an internal attribute of that data set or that class. So write self encodings equals encodings. So that allows us to create our data set class. And then our data loader needs two different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier. So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1338.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1384, 'published': '2021-05-27 16:15:39 UTC', 'start': 1338, 'text': ""equals encodings. So that allows us to create our data set class. And then our data loader needs two different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier. So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1384,2021-05-27 16:15:39 UTC,1338,equals encodings. So that allows us to create our data set class. And then our data loader needs two different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier. So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1348.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1394, 'published': '2021-05-27 16:15:39 UTC', 'start': 1348, 'text': ""different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier. So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same method on your class. And inside here all we need to do is return the length. So what length should we return? Well"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1394,2021-05-27 16:15:39 UTC,1348,different methods from this class as well. It needs a get item method and a length method. So let's do the length method first. It's easier. So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same method on your class. And inside here all we need to do is return the length. So what length should we return? Well,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1360.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1404, 'published': '2021-05-27 16:15:39 UTC', 'start': 1360, 'text': ""So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same method on your class. And inside here all we need to do is return the length. So what length should we return? Well if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1404,2021-05-27 16:15:39 UTC,1360,So our length we don't need to pass anything to this. It's just the same as when you would write this and you put something inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same method on your class. And inside here all we need to do is return the length. So what length should we return? Well if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1370.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1414, 'published': '2021-05-27 16:15:39 UTC', 'start': 1370, 'text': ""inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same method on your class. And inside here all we need to do is return the length. So what length should we return? Well if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that. We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1414,2021-05-27 16:15:39 UTC,1370,inside it. So list 0 1. We get that length. That's exactly what we're doing here. So this creates a enables you to do this same method on your class. And inside here all we need to do is return the length. So what length should we return? Well if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that. We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1380.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1424, 'published': '2021-05-27 16:15:39 UTC', 'start': 1380, 'text': ""method on your class. And inside here all we need to do is return the length. So what length should we return? Well if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that. We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids shape 0. So we have these 317 items. So just show you here. See this is"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1424,2021-05-27 16:15:39 UTC,1380,method on your class. And inside here all we need to do is return the length. So what length should we return? Well if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that. We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids shape 0. So we have these 317 items. So just show you here. See this is,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1390.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1436, 'published': '2021-05-27 16:15:39 UTC', 'start': 1390, 'text': ""if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that. We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids shape 0. So we have these 317 items. So just show you here. See this is our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1436,2021-05-27 16:15:39 UTC,1390,if we just do length of inputs we get 4 because we only have 4 items in there. So we don't want that. We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids shape 0. So we have these 317 items. So just show you here. See this is our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1400.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1448, 'published': '2021-05-27 16:15:39 UTC', 'start': 1400, 'text': ""We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids shape 0. So we have these 317 items. So just show you here. See this is our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs that we have. So we take that and we return it. But obviously we don't have inputs. We now have this"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1448,2021-05-27 16:15:39 UTC,1400,We actually want the number of samples that we have within our inputs. So what we can do instead is we write inputs input ids shape 0. So we have these 317 items. So just show you here. See this is our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs that we have. So we take that and we return it. But obviously we don't have inputs. We now have this,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1410.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1458, 'published': '2021-05-27 16:15:39 UTC', 'start': 1410, 'text': ""shape 0. So we have these 317 items. So just show you here. See this is our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs that we have. So we take that and we return it. But obviously we don't have inputs. We now have this self encodings. So swap it like that. And then we want to pass this get item"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1458,2021-05-27 16:15:39 UTC,1410,shape 0. So we have these 317 items. So just show you here. See this is our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs that we have. So we take that and we return it. But obviously we don't have inputs. We now have this self encodings. So swap it like that. And then we want to pass this get item,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1420.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1468, 'published': '2021-05-27 16:15:39 UTC', 'start': 1420, 'text': ""our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs that we have. So we take that and we return it. But obviously we don't have inputs. We now have this self encodings. So swap it like that. And then we want to pass this get item method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1468,2021-05-27 16:15:39 UTC,1420,"our encoding size. So the max length we set here. And this is the number of sentences or sentence pairs that we have. So we take that and we return it. But obviously we don't have inputs. We now have this self encodings. So swap it like that. And then we want to pass this get item method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1432.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1478, 'published': '2021-05-27 16:15:39 UTC', 'start': 1432, 'text': ""that we have. So we take that and we return it. But obviously we don't have inputs. We now have this self encodings. So swap it like that. And then we want to pass this get item method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1478,2021-05-27 16:15:39 UTC,1432,"that we have. So we take that and we return it. But obviously we don't have inputs. We now have this self encodings. So swap it like that. And then we want to pass this get item method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1444.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1488, 'published': '2021-05-27 16:15:39 UTC', 'start': 1444, 'text': ""self encodings. So swap it like that. And then we want to pass this get item method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index argument there. What we do is return let me show you down here what that would look like."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1488,2021-05-27 16:15:39 UTC,1444,"self encodings. So swap it like that. And then we want to pass this get item method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index argument there. What we do is return let me show you down here what that would look like.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1454.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1498, 'published': '2021-05-27 16:15:39 UTC', 'start': 1454, 'text': ""method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index argument there. What we do is return let me show you down here what that would look like. So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1498,2021-05-27 16:15:39 UTC,1454,"method. And what this does is given a certain index it will return your 4 dictionaries your input ids, token type ids attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index argument there. What we do is return let me show you down here what that would look like. So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1464.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1510, 'published': '2021-05-27 16:15:39 UTC', 'start': 1464, 'text': ""attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index argument there. What we do is return let me show you down here what that would look like. So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write is key. And then we write our value index. So maybe it makes more sense for me to write so tensor."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1510,2021-05-27 16:15:39 UTC,1464,attention mask and labels dictionary which we've created down here for that specific index. So we need to allow it to take an index argument there. What we do is return let me show you down here what that would look like. So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write is key. And then we write our value index. So maybe it makes more sense for me to write so tensor.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1474.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1520, 'published': '2021-05-27 16:15:39 UTC', 'start': 1474, 'text': 'argument there. What we do is return let me show you down here what that would look like. So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write is key. And then we write our value index. So maybe it makes more sense for me to write so tensor. So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1520,2021-05-27 16:15:39 UTC,1474,"argument there. What we do is return let me show you down here what that would look like. So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write is key. And then we write our value index. So maybe it makes more sense for me to write so tensor. So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1484.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1530, 'published': '2021-05-27 16:15:39 UTC', 'start': 1484, 'text': ""So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write is key. And then we write our value index. So maybe it makes more sense for me to write so tensor. So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317 items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1530,2021-05-27 16:15:39 UTC,1484,"So we want to create a dictionary just like we have up here but we just want that for that specific index. So what we write is key. And then we write our value index. So maybe it makes more sense for me to write so tensor. So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317 items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1494.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1540, 'published': '2021-05-27 16:15:39 UTC', 'start': 1494, 'text': ""is key. And then we write our value index. So maybe it makes more sense for me to write so tensor. So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317 items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1540,2021-05-27 16:15:39 UTC,1494,"is key. And then we write our value index. So maybe it makes more sense for me to write so tensor. So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317 items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1506.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1550, 'published': '2021-05-27 16:15:39 UTC', 'start': 1506, 'text': ""So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317 items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So we do for key and tensor in and in here we would write let's say we do"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1550,2021-05-27 16:15:39 UTC,1506,"So our key is our input ids, attention mask and so on. Our tensor is obviously the tensor inside there but we have the full tensor containing all 317 items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So we do for key and tensor in and in here we would write let's say we do",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1516.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1560, 'published': '2021-05-27 16:15:39 UTC', 'start': 1516, 'text': ""items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So we do for key and tensor in and in here we would write let's say we do inputs items. So let me just take that out so you can see. So that gives us our dictionary items."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1560,2021-05-27 16:15:39 UTC,1516,"items. So then we pull out the index for that tensor. But we need to make sure we're doing that for each of our items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So we do for key and tensor in and in here we would write let's say we do inputs items. So let me just take that out so you can see. So that gives us our dictionary items.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1526.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1570, 'published': '2021-05-27 16:15:39 UTC', 'start': 1526, 'text': ""items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So we do for key and tensor in and in here we would write let's say we do inputs items. So let me just take that out so you can see. So that gives us our dictionary items. And if we do a for loop so for key tensor in we want to"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1570,2021-05-27 16:15:39 UTC,1526,"items. So because we have multiple tensors here don't we? We have the 4 input ids, labels and so on. So we do for key and tensor in and in here we would write let's say we do inputs items. So let me just take that out so you can see. So that gives us our dictionary items. And if we do a for loop so for key tensor in we want to",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1536.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1580, 'published': '2021-05-27 16:15:39 UTC', 'start': 1536, 'text': ""we do for key and tensor in and in here we would write let's say we do inputs items. So let me just take that out so you can see. So that gives us our dictionary items. And if we do a for loop so for key tensor in we want to print the key. So you see that we're looping through each one of those. And we also get the tensor out for each"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1580,2021-05-27 16:15:39 UTC,1536,we do for key and tensor in and in here we would write let's say we do inputs items. So let me just take that out so you can see. So that gives us our dictionary items. And if we do a for loop so for key tensor in we want to print the key. So you see that we're looping through each one of those. And we also get the tensor out for each,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1546.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1590, 'published': '2021-05-27 16:15:39 UTC', 'start': 1546, 'text': ""inputs items. So let me just take that out so you can see. So that gives us our dictionary items. And if we do a for loop so for key tensor in we want to print the key. So you see that we're looping through each one of those. And we also get the tensor out for each one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1590,2021-05-27 16:15:39 UTC,1546,inputs items. So let me just take that out so you can see. So that gives us our dictionary items. And if we do a for loop so for key tensor in we want to print the key. So you see that we're looping through each one of those. And we also get the tensor out for each one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1556.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1600, 'published': '2021-05-27 16:15:39 UTC', 'start': 1556, 'text': ""And if we do a for loop so for key tensor in we want to print the key. So you see that we're looping through each one of those. And we also get the tensor out for each one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here. We get the 0 tensor and nothing more. But we want to specify an index so we copy"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1600,2021-05-27 16:15:39 UTC,1556,And if we do a for loop so for key tensor in we want to print the key. So you see that we're looping through each one of those. And we also get the tensor out for each one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here. We get the 0 tensor and nothing more. But we want to specify an index so we copy,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1566.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1612, 'published': '2021-05-27 16:15:39 UTC', 'start': 1566, 'text': ""print the key. So you see that we're looping through each one of those. And we also get the tensor out for each one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here. We get the 0 tensor and nothing more. But we want to specify an index so we copy that. And it's what we're going to return here. Except here we change it to self encodings."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1612,2021-05-27 16:15:39 UTC,1566,print the key. So you see that we're looping through each one of those. And we also get the tensor out for each one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here. We get the 0 tensor and nothing more. But we want to specify an index so we copy that. And it's what we're going to return here. Except here we change it to self encodings.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1576.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1622, 'published': '2021-05-27 16:15:39 UTC', 'start': 1576, 'text': ""one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here. We get the 0 tensor and nothing more. But we want to specify an index so we copy that. And it's what we're going to return here. Except here we change it to self encodings. So that's our class. And with that we can initialize our data set object. So data set equals"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1622,2021-05-27 16:15:39 UTC,1576,one of those as well. But we're specifying a certain tensor with each one. So let's say we want 0 here. We get the 0 tensor and nothing more. But we want to specify an index so we copy that. And it's what we're going to return here. Except here we change it to self encodings. So that's our class. And with that we can initialize our data set object. So data set equals,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1586.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1632, 'published': '2021-05-27 16:15:39 UTC', 'start': 1586, 'text': ""We get the 0 tensor and nothing more. But we want to specify an index so we copy that. And it's what we're going to return here. Except here we change it to self encodings. So that's our class. And with that we can initialize our data set object. So data set equals meditations data set. And then all we need to do is pass our data which is just inputs here."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1632,2021-05-27 16:15:39 UTC,1586,We get the 0 tensor and nothing more. But we want to specify an index so we copy that. And it's what we're going to return here. Except here we change it to self encodings. So that's our class. And with that we can initialize our data set object. So data set equals meditations data set. And then all we need to do is pass our data which is just inputs here.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1596.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1642, 'published': '2021-05-27 16:15:39 UTC', 'start': 1596, 'text': ""that. And it's what we're going to return here. Except here we change it to self encodings. So that's our class. And with that we can initialize our data set object. So data set equals meditations data set. And then all we need to do is pass our data which is just inputs here. Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1642,2021-05-27 16:15:39 UTC,1596,that. And it's what we're going to return here. Except here we change it to self encodings. So that's our class. And with that we can initialize our data set object. So data set equals meditations data set. And then all we need to do is pass our data which is just inputs here. Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1606.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1654, 'published': '2021-05-27 16:15:39 UTC', 'start': 1606, 'text': ""So that's our class. And with that we can initialize our data set object. So data set equals meditations data set. And then all we need to do is pass our data which is just inputs here. Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that like this. So we do loader torch utils data dot data loader."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1654,2021-05-27 16:15:39 UTC,1606,So that's our class. And with that we can initialize our data set object. So data set equals meditations data set. And then all we need to do is pass our data which is just inputs here. Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that like this. So we do loader torch utils data dot data loader.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1618.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1664, 'published': '2021-05-27 16:15:39 UTC', 'start': 1618, 'text': ""meditations data set. And then all we need to do is pass our data which is just inputs here. Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that like this. So we do loader torch utils data dot data loader. We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1664,2021-05-27 16:15:39 UTC,1618,meditations data set. And then all we need to do is pass our data which is just inputs here. Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that like this. So we do loader torch utils data dot data loader. We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1628.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1674, 'published': '2021-05-27 16:15:39 UTC', 'start': 1628, 'text': ""Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that like this. So we do loader torch utils data dot data loader. We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16. And then we also want to shuffle our data set as well. So we write shuffle equals true."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1674,2021-05-27 16:15:39 UTC,1628,Like that. OK. So that's our data set ready. Now we can initialize our data loader. And we do that like this. So we do loader torch utils data dot data loader. We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16. And then we also want to shuffle our data set as well. So we write shuffle equals true.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1638.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1684, 'published': '2021-05-27 16:15:39 UTC', 'start': 1638, 'text': ""like this. So we do loader torch utils data dot data loader. We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16. And then we also want to shuffle our data set as well. So we write shuffle equals true. And that's our data loader. So now now we just need to set up a few model"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1684,2021-05-27 16:15:39 UTC,1638,like this. So we do loader torch utils data dot data loader. We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16. And then we also want to shuffle our data set as well. So we write shuffle equals true. And that's our data loader. So now now we just need to set up a few model,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1650.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1694, 'published': '2021-05-27 16:15:39 UTC', 'start': 1650, 'text': ""We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16. And then we also want to shuffle our data set as well. So we write shuffle equals true. And that's our data loader. So now now we just need to set up a few model training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1694,2021-05-27 16:15:39 UTC,1650,We pass our data set object. We also want to specify the batch size. So I'm going to use batches of 16. And then we also want to shuffle our data set as well. So we write shuffle equals true. And that's our data loader. So now now we just need to set up a few model training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1660.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1704, 'published': '2021-05-27 16:15:39 UTC', 'start': 1660, 'text': ""And then we also want to shuffle our data set as well. So we write shuffle equals true. And that's our data loader. So now now we just need to set up a few model training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So to figure that out what we do is write let me do this torch device CUDA."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1704,2021-05-27 16:15:39 UTC,1660,And then we also want to shuffle our data set as well. So we write shuffle equals true. And that's our data loader. So now now we just need to set up a few model training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So to figure that out what we do is write let me do this torch device CUDA.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1670.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1716, 'published': '2021-05-27 16:15:39 UTC', 'start': 1670, 'text': ""And that's our data loader. So now now we just need to set up a few model training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So to figure that out what we do is write let me do this torch device CUDA. So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1716,2021-05-27 16:15:39 UTC,1670,And that's our data loader. So now now we just need to set up a few model training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So to figure that out what we do is write let me do this torch device CUDA. So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1680.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1726, 'published': '2021-05-27 16:15:39 UTC', 'start': 1680, 'text': ""training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So to figure that out what we do is write let me do this torch device CUDA. So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available. So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1726,2021-05-27 16:15:39 UTC,1680,training parameters. So the first thing we want to do is move our model to a GPU if we have a GPU. So to figure that out what we do is write let me do this torch device CUDA. So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available. So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1690.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1736, 'published': '2021-05-27 16:15:39 UTC', 'start': 1690, 'text': ""to figure that out what we do is write let me do this torch device CUDA. So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available. So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1736,2021-05-27 16:15:39 UTC,1690,to figure that out what we do is write let me do this torch device CUDA. So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available. So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1700.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1746, 'published': '2021-05-27 16:15:39 UTC', 'start': 1700, 'text': ""So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available. So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this. So we saw that in device. And then what we can do is move our model and also move our tensors later on to that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1746,2021-05-27 16:15:39 UTC,1700,So this is we say we want to use a CUDA enabled GPU if torch dot CUDA is available. So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this. So we saw that in device. And then what we can do is move our model and also move our tensors later on to that,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1712.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1756, 'published': '2021-05-27 16:15:39 UTC', 'start': 1712, 'text': ""So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this. So we saw that in device. And then what we can do is move our model and also move our tensors later on to that device for training. So we just write model to device. And we'll get a lot of output from that."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1756,2021-05-27 16:15:39 UTC,1712,So this will check our environment and check if we have a CUDA enabled GPU. If it isn't available we want to use a torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this. So we saw that in device. And then what we can do is move our model and also move our tensors later on to that device for training. So we just write model to device. And we'll get a lot of output from that.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1722.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1766, 'published': '2021-05-27 16:15:39 UTC', 'start': 1722, 'text': ""torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this. So we saw that in device. And then what we can do is move our model and also move our tensors later on to that device for training. So we just write model to device. And we'll get a lot of output from that. Just ignore that we don't need to worry about it. And we can also activate our models training mode like that."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1766,2021-05-27 16:15:39 UTC,1722,torch device CPU. So run that and see for me I have a CUDA enabled GPU so it comes up with this. So we saw that in device. And then what we can do is move our model and also move our tensors later on to that device for training. So we just write model to device. And we'll get a lot of output from that. Just ignore that we don't need to worry about it. And we can also activate our models training mode like that.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1732.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1776, 'published': '2021-05-27 16:15:39 UTC', 'start': 1732, 'text': ""So we saw that in device. And then what we can do is move our model and also move our tensors later on to that device for training. So we just write model to device. And we'll get a lot of output from that. Just ignore that we don't need to worry about it. And we can also activate our models training mode like that. OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1776,2021-05-27 16:15:39 UTC,1732,So we saw that in device. And then what we can do is move our model and also move our tensors later on to that device for training. So we just write model to device. And we'll get a lot of output from that. Just ignore that we don't need to worry about it. And we can also activate our models training mode like that. OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1742.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1786, 'published': '2021-05-27 16:15:39 UTC', 'start': 1742, 'text': ""device for training. So we just write model to device. And we'll get a lot of output from that. Just ignore that we don't need to worry about it. And we can also activate our models training mode like that. OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer. So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1786,2021-05-27 16:15:39 UTC,1742,device for training. So we just write model to device. And we'll get a lot of output from that. Just ignore that we don't need to worry about it. And we can also activate our models training mode like that. OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer. So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1752.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1798, 'published': '2021-05-27 16:15:39 UTC', 'start': 1752, 'text': ""Just ignore that we don't need to worry about it. And we can also activate our models training mode like that. OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer. So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers. So from transformers import AdamW. And we initialize the"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1798,2021-05-27 16:15:39 UTC,1752,Just ignore that we don't need to worry about it. And we can also activate our models training mode like that. OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer. So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers. So from transformers import AdamW. And we initialize the,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1762.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1808, 'published': '2021-05-27 16:15:39 UTC', 'start': 1762, 'text': ""OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer. So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers. So from transformers import AdamW. And we initialize the optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1808,2021-05-27 16:15:39 UTC,1762,OK. So we've moved our model over to GPU activate training mode. Now what we need to do is initialize our optimizer. So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers. So from transformers import AdamW. And we initialize the optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1772.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1818, 'published': '2021-05-27 16:15:39 UTC', 'start': 1772, 'text': ""So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers. So from transformers import AdamW. And we initialize the optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1818,2021-05-27 16:15:39 UTC,1772,So we're going to be using Adam with weighted decay for our optimizer. So to use that we need to import it from transformers. So from transformers import AdamW. And we initialize the optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1782.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1828, 'published': '2021-05-27 16:15:39 UTC', 'start': 1782, 'text': ""So from transformers import AdamW. And we initialize the optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers. And that looks pretty good to me. So now we can begin our training loop. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1828,2021-05-27 16:15:39 UTC,1782,So from transformers import AdamW. And we initialize the optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers. And that looks pretty good to me. So now we can begin our training loop. So,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1794.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1838, 'published': '2021-05-27 16:15:39 UTC', 'start': 1794, 'text': ""optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers. And that looks pretty good to me. So now we can begin our training loop. So first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1838,2021-05-27 16:15:39 UTC,1794,optimizer like this. So we AdamW we pass our model parameters. And we also want to pass the learning rate which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers. And that looks pretty good to me. So now we can begin our training loop. So first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1804.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1848, 'published': '2021-05-27 16:15:39 UTC', 'start': 1804, 'text': ""which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers. And that looks pretty good to me. So now we can begin our training loop. So first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1848,2021-05-27 16:15:39 UTC,1804,which is going to be 5e to the minus 5. 5e to the minus 5. OK that's a pretty common one for training transformers. And that looks pretty good to me. So now we can begin our training loop. So first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1814.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1858, 'published': '2021-05-27 16:15:39 UTC', 'start': 1814, 'text': ""And that looks pretty good to me. So now we can begin our training loop. So first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1858,2021-05-27 16:15:39 UTC,1814,And that looks pretty good to me. So now we can begin our training loop. So first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1824.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1868, 'published': '2021-05-27 16:15:39 UTC', 'start': 1824, 'text': ""first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1868,2021-05-27 16:15:39 UTC,1824,first I want to import something called TQDM. Now this is purely for aesthetics. We don't need it for training. This is so that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1834.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1878, 'published': '2021-05-27 16:15:39 UTC', 'start': 1834, 'text': ""that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1878,2021-05-27 16:15:39 UTC,1834,that we see a little progress bar during training otherwise we don't see anything. So I just want to include that so we can actually see what is going on. So from TQDM import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1844.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1888, 'published': '2021-05-27 16:15:39 UTC', 'start': 1844, 'text': ""import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1888,2021-05-27 16:15:39 UTC,1844,import TQDM. So this is optional we don't need to include it. It's up to you. But I would recommend it. We'll train for let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1854.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1898, 'published': '2021-05-27 16:15:39 UTC', 'start': 1854, 'text': ""let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1898,2021-05-27 16:15:39 UTC,1854,let's go with 2 epochs. Again we don't want to train transformers too much because they will easily over fit. And to be honest they'll probably over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1864.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1908, 'published': '2021-05-27 16:15:39 UTC', 'start': 1864, 'text': ""over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader. And we also want to write leave equals true. So this is so that we can see the"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1908,2021-05-27 16:15:39 UTC,1864,over fit on this dataset because it's very small. But that's fine. We just want to use this as an example. So we're going to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader. And we also want to write leave equals true. So this is so that we can see the,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1874.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1920, 'published': '2021-05-27 16:15:39 UTC', 'start': 1874, 'text': ""to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader. And we also want to write leave equals true. So this is so that we can see the progress bar. And then we loop through each batch that will be generated by that loop generator."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1920,2021-05-27 16:15:39 UTC,1874,to train for 2 epochs. And because we're using TQDM we want to set up our training loop like this. So we wrap it within a TQDM instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader. And we also want to write leave equals true. So this is so that we can see the progress bar. And then we loop through each batch that will be generated by that loop generator.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1884.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1930, 'published': '2021-05-27 16:15:39 UTC', 'start': 1884, 'text': ""instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader. And we also want to write leave equals true. So this is so that we can see the progress bar. And then we loop through each batch that will be generated by that loop generator. So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1930,2021-05-27 16:15:39 UTC,1884,instance. And all we do here is pass our data loader. So we create that up here. That's our PyTorch data loader. And we also want to write leave equals true. So this is so that we can see the progress bar. And then we loop through each batch that will be generated by that loop generator. So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1894.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1940, 'published': '2021-05-27 16:15:39 UTC', 'start': 1894, 'text': ""And we also want to write leave equals true. So this is so that we can see the progress bar. And then we loop through each batch that will be generated by that loop generator. So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1940,2021-05-27 16:15:39 UTC,1894,And we also want to write leave equals true. So this is so that we can see the progress bar. And then we loop through each batch that will be generated by that loop generator. So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1904.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1950, 'published': '2021-05-27 16:15:39 UTC', 'start': 1904, 'text': ""progress bar. And then we loop through each batch that will be generated by that loop generator. So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1950,2021-05-27 16:15:39 UTC,1904,progress bar. And then we loop through each batch that will be generated by that loop generator. So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1914.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1960, 'published': '2021-05-27 16:15:39 UTC', 'start': 1914, 'text': ""So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim 0grad and after that we can load in our"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1960,2021-05-27 16:15:39 UTC,1914,So for batch in loop. So now we're in our training loop. What we want to do here. Very first thing is set our optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim 0grad and after that we can load in our,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1926.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1970, 'published': '2021-05-27 16:15:39 UTC', 'start': 1926, 'text': ""optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim 0grad and after that we can load in our batches or our tensors from our batch here. So we want input IDs equals"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1970,2021-05-27 16:15:39 UTC,1926,optimizer's gradients to 0. So obviously in the very first loop that it's fine. It doesn't matter. But every loop after that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim 0grad and after that we can load in our batches or our tensors from our batch here. So we want input IDs equals,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1936.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1980, 'published': '2021-05-27 16:15:39 UTC', 'start': 1936, 'text': 'that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim 0grad and after that we can load in our batches or our tensors from our batch here. So we want input IDs equals a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1980,2021-05-27 16:15:39 UTC,1936,that our optimizer will have a set of gradients that have been calculated from the previous loop. And we need to reset those. So we write optim 0grad and after that we can load in our batches or our tensors from our batch here. So we want input IDs equals a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1946.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1990, 'published': '2021-05-27 16:15:39 UTC', 'start': 1946, 'text': ""0grad and after that we can load in our batches or our tensors from our batch here. So we want input IDs equals a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,1990,2021-05-27 16:15:39 UTC,1946,0grad and after that we can load in our batches or our tensors from our batch here. So we want input IDs equals a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1956.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2000, 'published': '2021-05-27 16:15:39 UTC', 'start': 1956, 'text': ""batches or our tensors from our batch here. So we want input IDs equals a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well. So we just write that. OK. And copy this. So we have one more."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2000,2021-05-27 16:15:39 UTC,1956,batches or our tensors from our batch here. So we want input IDs equals a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well. So we just write that. OK. And copy this. So we have one more.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1966.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2010, 'published': '2021-05-27 16:15:39 UTC', 'start': 1966, 'text': ""a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well. So we just write that. OK. And copy this. So we have one more. So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2010,2021-05-27 16:15:39 UTC,1966,"a batch. And we access it like a dictionary. So we have input IDs. So input IDs. And one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well. So we just write that. OK. And copy this. So we have one more. So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1976.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2028, 'published': '2021-05-27 16:15:39 UTC', 'start': 1976, 'text': ""one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well. So we just write that. OK. And copy this. So we have one more. So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those. So token type IDs attention mask and labels."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2028,2021-05-27 16:15:39 UTC,1976,"one other thing that we need to do is our model is on our GPU. So we need to move the data that we're training on to our GPU as well. So we just write that. OK. And copy this. So we have one more. So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those. So token type IDs attention mask and labels.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1986.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2038, 'published': '2021-05-27 16:15:39 UTC', 'start': 1986, 'text': 'So we just write that. OK. And copy this. So we have one more. So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those. So token type IDs attention mask and labels. OK. So initialize our gradients. We have pulled in our tensors and now we can', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2038,2021-05-27 16:15:39 UTC,1986,"So we just write that. OK. And copy this. So we have one more. So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those. So token type IDs attention mask and labels. OK. So initialize our gradients. We have pulled in our tensors and now we can",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t1996.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2052, 'published': '2021-05-27 16:15:39 UTC', 'start': 1996, 'text': 'So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those. So token type IDs attention mask and labels. OK. So initialize our gradients. We have pulled in our tensors and now we can process them through our model. So we do model input IDs. We have token type IDs.', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2052,2021-05-27 16:15:39 UTC,1996,"So we have all these that we create up here. So input ID, setting type ID attention mask and labels. We want all of those. So token type IDs attention mask and labels. OK. So initialize our gradients. We have pulled in our tensors and now we can process them through our model. So we do model input IDs. We have token type IDs.",Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2006.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2062, 'published': '2021-05-27 16:15:39 UTC', 'start': 2006, 'text': 'So token type IDs attention mask and labels. OK. So initialize our gradients. We have pulled in our tensors and now we can process them through our model. So we do model input IDs. We have token type IDs. We also have the attention mask. And we also have our labels. OK.', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2062,2021-05-27 16:15:39 UTC,2006,So token type IDs attention mask and labels. OK. So initialize our gradients. We have pulled in our tensors and now we can process them through our model. So we do model input IDs. We have token type IDs. We also have the attention mask. And we also have our labels. OK.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2024.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2072, 'published': '2021-05-27 16:15:39 UTC', 'start': 2024, 'text': 'OK. So initialize our gradients. We have pulled in our tensors and now we can process them through our model. So we do model input IDs. We have token type IDs. We also have the attention mask. And we also have our labels. OK. So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction.', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2072,2021-05-27 16:15:39 UTC,2024,OK. So initialize our gradients. We have pulled in our tensors and now we can process them through our model. So we do model input IDs. We have token type IDs. We also have the attention mask. And we also have our labels. OK. So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2034.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2086, 'published': '2021-05-27 16:15:39 UTC', 'start': 2034, 'text': ""process them through our model. So we do model input IDs. We have token type IDs. We also have the attention mask. And we also have our labels. OK. So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction. And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2086,2021-05-27 16:15:39 UTC,2034,process them through our model. So we do model input IDs. We have token type IDs. We also have the attention mask. And we also have our labels. OK. So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction. And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2048.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2098, 'published': '2021-05-27 16:15:39 UTC', 'start': 2048, 'text': ""We also have the attention mask. And we also have our labels. OK. So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction. And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2098,2021-05-27 16:15:39 UTC,2048,We also have the attention mask. And we also have our labels. OK. So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction. And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2058.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2110, 'published': '2021-05-27 16:15:39 UTC', 'start': 2058, 'text': ""So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction. And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within our model so we can optimize on that. So we just write loss backward. Yeah backward. And then"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2110,2021-05-27 16:15:39 UTC,2058,So that will create two tensors for us in the outputs. It will create a logits tensor which is our prediction. And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within our model so we can optimize on that. So we just write loss backward. Yeah backward. And then,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2068.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2120, 'published': '2021-05-27 16:15:39 UTC', 'start': 2068, 'text': ""And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within our model so we can optimize on that. So we just write loss backward. Yeah backward. And then we do optim step. And this will use our optimizer and take a step to optimize based on the loss"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2120,2021-05-27 16:15:39 UTC,2068,And it will create a loss tensor which is the difference between our prediction and our labels. So let's extract that loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within our model so we can optimize on that. So we just write loss backward. Yeah backward. And then we do optim step. And this will use our optimizer and take a step to optimize based on the loss,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2080.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2130, 'published': '2021-05-27 16:15:39 UTC', 'start': 2080, 'text': 'loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within our model so we can optimize on that. So we just write loss backward. Yeah backward. And then we do optim step. And this will use our optimizer and take a step to optimize based on the loss calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to', 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2130,2021-05-27 16:15:39 UTC,2080,loss. So we do outputs.loss. And then we also after extracting that loss we need to calculate that this is the overall loss. We need to calculate the loss for every parameter within our model so we can optimize on that. So we just write loss backward. Yeah backward. And then we do optim step. And this will use our optimizer and take a step to optimize based on the loss calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2092.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2142, 'published': '2021-05-27 16:15:39 UTC', 'start': 2092, 'text': ""our model so we can optimize on that. So we just write loss backward. Yeah backward. And then we do optim step. And this will use our optimizer and take a step to optimize based on the loss calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2142,2021-05-27 16:15:39 UTC,2092,our model so we can optimize on that. So we just write loss backward. Yeah backward. And then we do optim step. And this will use our optimizer and take a step to optimize based on the loss calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2106.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2152, 'published': '2021-05-27 16:15:39 UTC', 'start': 2106, 'text': ""we do optim step. And this will use our optimizer and take a step to optimize based on the loss calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch. So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2152,2021-05-27 16:15:39 UTC,2106,we do optim step. And this will use our optimizer and take a step to optimize based on the loss calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch. So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2116.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2162, 'published': '2021-05-27 16:15:39 UTC', 'start': 2116, 'text': ""calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch. So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2162,2021-05-27 16:15:39 UTC,2116,calculated here. And that is all we actually need for our training loop. We do also have the TQDM up here as well. So I just want to use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch. So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2126.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2172, 'published': '2021-05-27 16:15:39 UTC', 'start': 2126, 'text': ""use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch. So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item like that. Now that should be okay. Let's give it a go. See what happens. Okay."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2172,2021-05-27 16:15:39 UTC,2126,use that. And what we're going to do is we're just going to set the description of our loop at this current step equal to the epoch. So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item like that. Now that should be okay. Let's give it a go. See what happens. Okay.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2138.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2182, 'published': '2021-05-27 16:15:39 UTC', 'start': 2138, 'text': ""So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item like that. Now that should be okay. Let's give it a go. See what happens. Okay. So that looks pretty good. So you can see that our model is training. Loss is reducing. Now there isn't that much training data"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2182,2021-05-27 16:15:39 UTC,2138,So this is just purely aesthetics. We don't need this for training but it's just so we can see what is going on. And we also want to loop set postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item like that. Now that should be okay. Let's give it a go. See what happens. Okay. So that looks pretty good. So you can see that our model is training. Loss is reducing. Now there isn't that much training data,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2148.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2192, 'published': '2021-05-27 16:15:39 UTC', 'start': 2148, 'text': ""postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item like that. Now that should be okay. Let's give it a go. See what happens. Okay. So that looks pretty good. So you can see that our model is training. Loss is reducing. Now there isn't that much training data so we're not going to see anything crazy here but we can see that it is moving in the right direction. So that's pretty good. So that's everything for this video."", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2192,2021-05-27 16:15:39 UTC,2148,postfix. And here I'm going to add in our loss which is just going to be loss equals loss.item like that. Now that should be okay. Let's give it a go. See what happens. Okay. So that looks pretty good. So you can see that our model is training. Loss is reducing. Now there isn't that much training data so we're not going to see anything crazy here but we can see that it is moving in the right direction. So that's pretty good. So that's everything for this video.,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
x1lAcT3xl5M-t2158.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2202, 'published': '2021-05-27 16:15:39 UTC', 'start': 2158, 'text': ""like that. Now that should be okay. Let's give it a go. See what happens. Okay. So that looks pretty good. So you can see that our model is training. Loss is reducing. Now there isn't that much training data so we're not going to see anything crazy here but we can see that it is moving in the right direction. So that's pretty good. So that's everything for this video. It's a pretty long video. Recorded for 41 minutes. It'll probably be a little bit short for you. But yeah that's long. So"", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}",UCv83tO5cePwHMt1952IVVHw,2202,2021-05-27 16:15:39 UTC,2158,like that. Now that should be okay. Let's give it a go. See what happens. Okay. So that looks pretty good. So you can see that our model is training. Loss is reducing. Now there isn't that much training data so we're not going to see anything crazy here but we can see that it is moving in the right direction. So that's pretty good. So that's everything for this video. It's a pretty long video. Recorded for 41 minutes. It'll probably be a little bit short for you. But yeah that's long. So,Training BERT #4 - Train With Next Sentence Prediction (NSP),https://youtu.be/x1lAcT3xl5M
r-zQQ16wTCA-t5.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 144, 'published': '2021-09-23 13:30:07 UTC', 'start': 5, 'text': ""We're going to have a look at some of what I think are the most useful data sets. And we're going to look at how we can use the library to build what I think are very good pipelines or data input pipelines for NLP. So let's get started. So the first thing we want to do is actually, well, install data sets. So we'll go clip install data sets and that will install the library for you. After this, we'll want to go ahead and import data sets. And then we can start having a look at which data sets are available to us. Now there's two ways that you can have a look at all of the data sets. The first one is using the data sets viewer, which you can find on Google. You just type in data sets viewer and it's just an interactive app which allows you to go through and have a look at the different data sets. Now I'm not going to, I've already spoken about that a lot before and it's super easy to use. So we're not going to go through it. Instead, we're just going to have a look at how we can have view everything in Python, which is the second option. So first we can do this. So we just list all of our data sets. Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,144,2021-09-23 13:30:07 UTC,5,"We're going to have a look at some of what I think are the most useful data sets. And we're going to look at how we can use the library to build what I think are very good pipelines or data input pipelines for NLP. So let's get started. So the first thing we want to do is actually, well, install data sets. So we'll go clip install data sets and that will install the library for you. After this, we'll want to go ahead and import data sets. And then we can start having a look at which data sets are available to us. Now there's two ways that you can have a look at all of the data sets. The first one is using the data sets viewer, which you can find on Google. You just type in data sets viewer and it's just an interactive app which allows you to go through and have a look at the different data sets. Now I'm not going to, I've already spoken about that a lot before and it's super easy to use. So we're not going to go through it. Instead, we're just going to have a look at how we can have view everything in Python, which is the second option. So first we can do this. So we just list all of our data sets. Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t40.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 180, 'published': '2021-09-23 13:30:07 UTC', 'start': 40, 'text': ""After this, we'll want to go ahead and import data sets. And then we can start having a look at which data sets are available to us. Now there's two ways that you can have a look at all of the data sets. The first one is using the data sets viewer, which you can find on Google. You just type in data sets viewer and it's just an interactive app which allows you to go through and have a look at the different data sets. Now I'm not going to, I've already spoken about that a lot before and it's super easy to use. So we're not going to go through it. Instead, we're just going to have a look at how we can have view everything in Python, which is the second option. So first we can do this. So we just list all of our data sets. Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well. So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,180,2021-09-23 13:30:07 UTC,40,"After this, we'll want to go ahead and import data sets. And then we can start having a look at which data sets are available to us. Now there's two ways that you can have a look at all of the data sets. The first one is using the data sets viewer, which you can find on Google. You just type in data sets viewer and it's just an interactive app which allows you to go through and have a look at the different data sets. Now I'm not going to, I've already spoken about that a lot before and it's super easy to use. So we're not going to go through it. Instead, we're just going to have a look at how we can have view everything in Python, which is the second option. So first we can do this. So we just list all of our data sets. Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well. So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t68.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 204, 'published': '2021-09-23 13:30:07 UTC', 'start': 68, 'text': ""app which allows you to go through and have a look at the different data sets. Now I'm not going to, I've already spoken about that a lot before and it's super easy to use. So we're not going to go through it. Instead, we're just going to have a look at how we can have view everything in Python, which is the second option. So first we can do this. So we just list all of our data sets. Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well. So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well. But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,204,2021-09-23 13:30:07 UTC,68,"app which allows you to go through and have a look at the different data sets. Now I'm not going to, I've already spoken about that a lot before and it's super easy to use. So we're not going to go through it. Instead, we're just going to have a look at how we can have view everything in Python, which is the second option. So first we can do this. So we just list all of our data sets. Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well. So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well. But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t92.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 229, 'published': '2021-09-23 13:30:07 UTC', 'start': 92, 'text': ""Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well. So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well. But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data. So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here,"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,229,2021-09-23 13:30:07 UTC,92,"Now I'm going to just write dslists here. And from this, we will just get, I think it's something like 1400 data sets now. So it's quite a lot. So if we go len of all dslists. So yeah, it's 1.4 thousand data sets, which is obviously a lot. And some of these are massive as well. So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well. But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data. So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here,",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t127.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 244, 'published': '2021-09-23 13:30:07 UTC', 'start': 127, 'text': ""So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well. But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data. So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here, and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,244,2021-09-23 13:30:07 UTC,127,"So if we, for example, if we were to look at the Oscar data set, so in dslists, we could go data set for data set in dslists. If Oscar is in the data set. So these are just data set names. Okay, and we have Oscar, I think PT is, what is PT? I imagine it's probably Portuguese. And then we have all these other ones as well. But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data. So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here, and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t169.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 263, 'published': '2021-09-23 13:30:07 UTC', 'start': 169, 'text': ""But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data. So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here, and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad. There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,263,2021-09-23 13:30:07 UTC,169,"But these are just, these are users uploaded Oscar data sets. This is the actual Oscar data set that's been sold by Hugging Face and it's huge. It contains, I think, more than 160 languages. And some of them, for example, English, obviously English is one of the biggest ones, that contains 1.2 terabytes of data. So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here, and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad. There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t193.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 284, 'published': '2021-09-23 13:30:07 UTC', 'start': 193, 'text': ""So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here, and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad. There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine. Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,284,2021-09-23 13:30:07 UTC,193,"So there's a lot of data in there, but that's just unstructured text. What I want to have a look at is the squad data sets. Squad data sets. So we're going to be using, we're just going to use the original squad in this video. But you can see that we have a few different ones here. So Italian, Spanish, Korean, you have Thai, Thai QA squad here, and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad. There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine. Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t224.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 309, 'published': '2021-09-23 13:30:07 UTC', 'start': 224, 'text': ""and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad. There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine. Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method. We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,309,2021-09-23 13:30:07 UTC,224,"and then also French as well at the bottom. So you have plenty of choice. Now, obviously, you kind of need to know what sort of data set you're looking for. I know I'm looking for a squad data set. So I've gone, I've looked for squad. There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine. Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method. We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t237.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 328, 'published': '2021-09-23 13:30:07 UTC', 'start': 237, 'text': ""There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine. Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method. We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes. So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on,"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,328,2021-09-23 13:30:07 UTC,237,"There are other ones as well, actually. If I change this to lower, you'll see those also pop up. Okay, so we have like this one here and this one. This one doesn't seem to work. It's fine. Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method. We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes. So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on,",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t252.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 355, 'published': '2021-09-23 13:30:07 UTC', 'start': 252, 'text': ""Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method. We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes. So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on, which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set,"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,355,2021-09-23 13:30:07 UTC,252,"Now, to load one of those data sets, obviously we're going to be using squad. We write data set equals data sets dot load data set. And then in here, we just write our data set name, so squad. Now, there's two ways to download your data. So if we do this, this is the default method. We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes. So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on, which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set,",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t279.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 380, 'published': '2021-09-23 13:30:07 UTC', 'start': 279, 'text': ""We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes. So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on, which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set, it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,380,2021-09-23 13:30:07 UTC,279,"We are going to download and cache the whole data set in memory. Which for squad is fine. I think squad, it's not a huge data set, so it's not really a problem. But when you think, okay, if we wanted the English OSCA data set, that's massive. That's 1.2 terabytes. So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on, which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set, it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t297.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 402, 'published': '2021-09-23 13:30:07 UTC', 'start': 297, 'text': ""So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on, which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set, it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data. If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,402,2021-09-23 13:30:07 UTC,297,"So in those cases, you probably don't want to download it all onto your machine. So what you can do instead is you set streaming equal to true. And when streaming is equal to true, you do need to make some changes to your code, which I'll show you. And there are also some things, particularly filtering, which we will cover later on, which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set, it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data. If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t322.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 418, 'published': '2021-09-23 13:30:07 UTC', 'start': 322, 'text': ""which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set, it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data. If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation. And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,418,2021-09-23 13:30:07 UTC,322,"which we can't do with streaming. But we will just go ahead and for now we're going to use streaming. We'll switch over to not streaming later on. And this creates like a iteratable data set object. And it means that whenever we are calling a specific record within that data set, it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data. If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation. And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t344.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 440, 'published': '2021-09-23 13:30:07 UTC', 'start': 344, 'text': ""it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data. If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation. And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset. And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,440,2021-09-23 13:30:07 UTC,344,"it is only going to download or store that single record or multiple records in our memory at once. So we're not downloading the data set. We're just processing it as we get, which is, I think, very important. And it is, I think, very useful. Now, you can see here we have two actual subsets within our data. If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation. And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset. And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t371.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 456, 'published': '2021-09-23 13:30:07 UTC', 'start': 371, 'text': ""If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation. And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset. And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this. So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,456,2021-09-23 13:30:07 UTC,371,"If we want to select a specific subset, all we have to do is rewrite data sets again. So let me actually copy this. So we copy that. And if we just want a subset, we write split. And in this case, it would be train or validation. And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset. And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this. So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t396.15999999999997,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 476, 'published': '2021-09-23 13:30:07 UTC', 'start': 396, 'text': ""And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset. And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this. So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there. So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,476,2021-09-23 13:30:07 UTC,396,"And if I just call execute that. So I'm not going to store that in our data set variable here, because I don't want to use just train. We have this single iterable data set object. So we're just pulling in this single part of it or single subset. And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this. So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there. So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t415.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 508, 'published': '2021-09-23 13:30:07 UTC', 'start': 415, 'text': ""And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this. So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there. So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there. Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,508,2021-09-23 13:30:07 UTC,415,"And we can also view. So here we can see we have train and validation. If you want to see it in a more clear way, you can use dictionary syntax. So sorry, data set keys. You can use dictionary syntax for most of this. So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there. So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there. Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t433.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 533, 'published': '2021-09-23 13:30:07 UTC', 'start': 433, 'text': ""So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there. So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there. Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is. So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,533,2021-09-23 13:30:07 UTC,433,"So we have train and validation. Now there's also, so the moment we have our data set, we don't really know anything about it. So we have this train subset. And let's say I want to understand what is in there. So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there. Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is. So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t448.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 552, 'published': '2021-09-23 13:30:07 UTC', 'start': 448, 'text': ""So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there. Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is. So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,552,2021-09-23 13:30:07 UTC,448,"So what I can do to start is I write a data set train. And I can write, for example, the data set size. So how big is it? Right, data set size, data set, not data size, size. Don't know what I was doing there. Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is. So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t466.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 572, 'published': '2021-09-23 13:30:07 UTC', 'start': 466, 'text': ""Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is. So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers. All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,572,2021-09-23 13:30:07 UTC,466,"Let me see that we get, so it's like, so 80, about 90, 90 megabytes. So reasonably big, but it's not anything huge, nothing crazy. We can also, so we have that. We can also get, if I copy this, you can also get a description. Let me see what the data set is. So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers. All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t502.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 592, 'published': '2021-09-23 13:30:07 UTC', 'start': 502, 'text': ""So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers. All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think. And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,592,2021-09-23 13:30:07 UTC,502,"So SQUAD, I didn't even mention it already, but SQUAD is the Stanford Question Answering Data Set. So use it generally for training Q&A models or testing Q&A models. And you can pause and read that if you want to. And then another thing that is pretty important is what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers. All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think. And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t525.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 608, 'published': '2021-09-23 13:30:07 UTC', 'start': 525, 'text': ""what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers. All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think. And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this. But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,608,2021-09-23 13:30:07 UTC,525,"what are the features that we have inside here? Now we can also just print out one of the samples, but it's useful to know, I think. And this also gives you data types, so it's kind of useful. So we have ID, title, context, question, and answers. All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think. And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this. But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t543.9200000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 625, 'published': '2021-09-23 13:30:07 UTC', 'start': 543, 'text': ""All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think. And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this. But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample. And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia,"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,625,2021-09-23 13:30:07 UTC,543,"All of them are strings. Answers is actually, so within answers we have, it says, Sequency, we can view it as a dictionary. But we have a text, a attribute, and also an answer star attribute. So that's pretty useful to know, I think. And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this. But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample. And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia,",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t563.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 642, 'published': '2021-09-23 13:30:07 UTC', 'start': 563, 'text': ""And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this. But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample. And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia, pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,642,2021-09-23 13:30:07 UTC,563,"And to view one of our samples, so we have all the features here, but let's say we just want to see what it actually looks like. We can write data set and we go train. And when we have streaming set to false, we can write this. But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample. And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia, pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t585.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 659, 'published': '2021-09-23 13:30:07 UTC', 'start': 585, 'text': ""But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample. And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia, pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here. So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,659,2021-09-23 13:30:07 UTC,585,"But because we have streaming set to true, we can't do this. So instead what we have to do is we actually just iterate through the data set. So we just go for sample in data set. And we just want to print a single sample. And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia, pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here. So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t604.5600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 679, 'published': '2021-09-23 13:30:07 UTC', 'start': 604, 'text': ""And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia, pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here. So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking. And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,679,2021-09-23 13:30:07 UTC,604,"And then I don't want to print anymore, so I'm going to write break after that. So we just print one of those samples. And then we see, okay, we have the ID, we have title. So each of these samples is being pulled from a different Wikipedia, pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here. So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking. And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t621.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 707, 'published': '2021-09-23 13:30:07 UTC', 'start': 621, 'text': ""pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here. So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking. And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library. We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,707,2021-09-23 13:30:07 UTC,621,"pulled from a different Wikipedia page. In this case, the title is a titled page. So this one is from the University of Notre Dame Wikipedia page. We have answers. So further down, we're going to ask a question and these answers here. So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking. And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library. We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t636.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 726, 'published': '2021-09-23 13:30:07 UTC', 'start': 636, 'text': ""So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking. And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library. We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well. So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,726,2021-09-23 13:30:07 UTC,636,"So we have the text, which is the text answer. And then we have the position, so the character position where the answer starts within the context, which is what you can see here. Now we have a question here, which we're asking. And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library. We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well. So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t651.8399999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 751, 'published': '2021-09-23 13:30:07 UTC', 'start': 651, 'text': ""And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library. We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well. So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that. So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,751,2021-09-23 13:30:07 UTC,651,"And then the model, the Q&A model is going to extract the answer from our context there. Okay. So we're not going to be training model in this video or anything like that. We're just experimenting with the data sets library. We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well. So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that. So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t670.3199999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 770, 'published': '2021-09-23 13:30:07 UTC', 'start': 670, 'text': ""We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well. So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that. So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us. So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,770,2021-09-23 13:30:07 UTC,670,"We don't need to worry so much about that. So the first thing I want to do is have a look at how we can modify some of the features in our data. So with SQUAD, when we are training a model, one of the first things we would do is we take our answer start and the text and we will use that to get the answer end position as well. So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that. So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us. So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t696.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 795, 'published': '2021-09-23 13:30:07 UTC', 'start': 696, 'text': ""So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that. So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us. So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end. Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,795,2021-09-23 13:30:07 UTC,696,"So let's go ahead and do that. So I first I want to just have a look, okay, for sample in the data set train, I'm just going to print out a few of the answer features. So we have sample answer or answers, sorry. And I just want to print that. So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us. So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end. Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t720.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 814, 'published': '2021-09-23 13:30:07 UTC', 'start': 720, 'text': ""So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us. So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end. Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set. So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,814,2021-09-23 13:30:07 UTC,720,"So print it. And I want to say, okay, I want to enumerate this. So I can count how many times we're going through it. So here I'm just viewing the data so we can actually see what we have in there. So I want to say, if i is greater than four, just break, just stop printing answers for us. So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end. Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set. So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t748.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 845, 'published': '2021-09-23 13:30:07 UTC', 'start': 748, 'text': ""So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end. Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set. So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x. So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,845,2021-09-23 13:30:07 UTC,748,"So and then we have a few of these. So we have text and we have answer start. We want to add answer end. And the way that we do that is pretty straightforward. We just need to take the answer start and we add the length of our text to that to get the answer end. Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set. So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x. So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t762.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 876, 'published': '2021-09-23 13:30:07 UTC', 'start': 762, 'text': ""Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set. So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x. So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially. So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,876,2021-09-23 13:30:07 UTC,762,"Nothing, nothing complicated there. So what we're going to do here is modify the answers feature. And the best way or I think the least the most common way of modifying features or adding new features as well is to use the map method. So we go data set. So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x. So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially. So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t785.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 895, 'published': '2021-09-23 13:30:07 UTC', 'start': 785, 'text': ""So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x. So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially. So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end. So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,895,2021-09-23 13:30:07 UTC,785,So it's going to output a new data set. So we write data set train equals data set train. And we're going to use the map method. And with map we use lambda. So we write lambda x. So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially. So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end. So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t809.9200000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 924, 'published': '2021-09-23 13:30:07 UTC', 'start': 809, 'text': ""So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially. So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end. So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is. So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,924,2021-09-23 13:30:07 UTC,809,So in here we're building a lambda function. And what we need to do. So this is one of the things that changes depending on whether you're using streaming or not. So with streaming equals true in here we need to specify every single feature. So what I mean by that is let me do it for streaming faults initially. So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end. So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is. So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t835.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 944, 'published': '2021-09-23 13:30:07 UTC', 'start': 835, 'text': ""So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end. So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is. So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text. OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,944,2021-09-23 13:30:07 UTC,835,So when streaming is false we would just write answers. And we would write the modification to that feature. So in this case we are taking the current answers. So it would be x answers. And we would be merging that with a new dictionary item which is going to be answers end. So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is. So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text. OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t864.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 961, 'published': '2021-09-23 13:30:07 UTC', 'start': 864, 'text': ""So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is. So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text. OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well. I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,961,2021-09-23 13:30:07 UTC,864,So answer start. So answer end is equal to. And here what we have to do is we go x answers. So this is a little bit messy now. But it's just how it is. So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text. OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well. I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t887.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 982, 'published': '2021-09-23 13:30:07 UTC', 'start': 887, 'text': ""So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text. OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well. I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there. We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,982,2021-09-23 13:30:07 UTC,887,So we're within answers and we want to take the answer start position. So answer start. And we want to add. Let me start a new line here. And we want to add the length of answers text. OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well. I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there. We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t915.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1006, 'published': '2021-09-23 13:30:07 UTC', 'start': 915, 'text': ""OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well. I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there. We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course. A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1006,2021-09-23 13:30:07 UTC,915,OK so all we're doing there is we're taking answer start and we're adding answer text. Or the length of answer text to that to get our answer end. Now this is all we would have to write if we were using streaming equals false. But we're not. With streaming equals true we need to add every other feature in there as well. I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there. We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course. A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t938.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1030, 'published': '2021-09-23 13:30:07 UTC', 'start': 938, 'text': ""I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there. We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course. A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title. Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1030,2021-09-23 13:30:07 UTC,938,I'm not sure why this is the case. But it is. So we need to just add those in as well. So all they are is a direct mapping from the old version to the new data set. So we don't need to really do anything there. We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course. A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title. Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t955.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1052, 'published': '2021-09-23 13:30:07 UTC', 'start': 955, 'text': ""We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course. A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title. Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded. So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1052,2021-09-23 13:30:07 UTC,955,We just need to add ID. We want to map that to ID and do that for the other features as well. So we have also have context. Which is exit context. We have answer already done of course. A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title. Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded. So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t972.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1071, 'published': '2021-09-23 13:30:07 UTC', 'start': 972, 'text': ""A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title. Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded. So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again. This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1071,2021-09-23 13:30:07 UTC,972,A question which is going to be exit question. So ID context question answers. Is there anything else I'm missing? ID oh title of course. Title just title. Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded. So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again. This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t994.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1093, 'published': '2021-09-23 13:30:07 UTC', 'start': 994, 'text': ""Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded. So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again. This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down. We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1093,2021-09-23 13:30:07 UTC,994,Yeah so also add title in there as well. Okay and with that we should be ready to go. So let's map that. And what we'll find is when we're using streaming keywords equals true. The actual process is or the transformation that we just built is lazily loaded. So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again. This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down. We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1021.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1114, 'published': '2021-09-23 13:30:07 UTC', 'start': 1021, 'text': ""So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again. This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down. We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list. So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1114,2021-09-23 13:30:07 UTC,1021,So we haven't actually just done anything there. All we've said is we've passed this instruction to transform the data set in this way. But it hasn't actually transformed anything yet. It only performs this transformation when we call the data set. So if we did this again. This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down. We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list. So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1041.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1134, 'published': '2021-09-23 13:30:07 UTC', 'start': 1041, 'text': ""This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down. We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list. So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1134,2021-09-23 13:30:07 UTC,1041,This would call the data set and it would force the code to run this instruction or this transformation. So let's run that. And you see we actually do get an error here. And why is that? So let me come down. We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list. So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1062.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1150, 'published': '2021-09-23 13:30:07 UTC', 'start': 1062, 'text': ""We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list. So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one. So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1150,2021-09-23 13:30:07 UTC,1062,We have so what am I doing? And start plus delete those answers. What's wrong with that? Ah okay so if we look up here. We have these items here that are within the list. So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one. So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1081.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1169, 'published': '2021-09-23 13:30:07 UTC', 'start': 1081, 'text': ""So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one. So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this. And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1169,2021-09-23 13:30:07 UTC,1081,So we actually need to access that first item. But that's good because we saw that when we first execute this code nothing happened. And it only actually came across that error when we called a data set. Because that's when this transformation is actually performed. And now what we have to do is because we've already added this instruction to our data set you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one. So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this. And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1108.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1191, 'published': '2021-09-23 13:30:07 UTC', 'start': 1108, 'text': ""you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one. So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this. And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library. So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1191,2021-09-23 13:30:07 UTC,1108,you know transformation or building process. We actually need to reinitialize our data set. So we will come back up here. So where are you here? So date no not that one this one. So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this. And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library. So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1123.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1205, 'published': '2021-09-23 13:30:07 UTC', 'start': 1123, 'text': ""So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this. And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library. So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here. I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1205,2021-09-23 13:30:07 UTC,1123,So we need to load that again to reinitialize the all of the instructions that we've added in there. And then we can go ahead rerun this. And now it should work hopefully let's see. There we go. So now if we have a look at this. And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library. So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here. I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1141.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1222, 'published': '2021-09-23 13:30:07 UTC', 'start': 1141, 'text': ""And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library. So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here. I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing. And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1222,2021-09-23 13:30:07 UTC,1141,And this is something I probably should have done but I completely forgot to. So I should have added this as maybe a list rather than just the number. But it's fine because you know if you come across and you need to do this. You may want to add that in. But we're not doing anything other than playing around with the data sets library. So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here. I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing. And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1162.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1239, 'published': '2021-09-23 13:30:07 UTC', 'start': 1162, 'text': ""So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here. I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing. And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks. Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1239,2021-09-23 13:30:07 UTC,1162,So it's not really a problem. But you can see that we have added answers and into there now which is what we wanted to do. And also importantly is if I let me copy this bring down here. We'll notice that we do still have all of our data sets. So if I go here. I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing. And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks. Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1187.6799999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1260, 'published': '2021-09-23 13:30:07 UTC', 'start': 1187, 'text': ""I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing. And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks. Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this. So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1260,2021-09-23 13:30:07 UTC,1187,I don't really need to remove that's fine. I'll just break straight away. That's fine. So sample sorry here. So you see the whole thing. And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks. Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this. So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1201.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1275, 'published': '2021-09-23 13:30:07 UTC', 'start': 1201, 'text': ""And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks. Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this. So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation. But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1275,2021-09-23 13:30:07 UTC,1201,And we see that we still have the ID. We have the text. We have the context. We have everything in there. Now I'm just going to show you you know why this breaks. Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this. So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation. But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1213.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1298, 'published': '2021-09-23 13:30:07 UTC', 'start': 1213, 'text': ""Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this. So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation. But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here. Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1298,2021-09-23 13:30:07 UTC,1213,Or what happens if I remove these. Okay so let me rerun that and this as well. So yeah so this should look the same. Do we have yet that's fine. But then if I run this. So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation. But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here. Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1231.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1312, 'published': '2021-09-23 13:30:07 UTC', 'start': 1231, 'text': ""So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation. But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here. Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens. As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1312,2021-09-23 13:30:07 UTC,1231,So before this had all the all the features but now we only have the single feature that we specified in this formula. So the answers. So that's why you need to when shuffle is set to true. That's why you need to add every single feature in there. Otherwise it's just going to remove them when you perform the map operation. But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here. Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens. As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1251.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1332, 'published': '2021-09-23 13:30:07 UTC', 'start': 1251, 'text': ""But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here. Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens. As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time. So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1332,2021-09-23 13:30:07 UTC,1251,But that's only the case when shuffle is actually set to true. Shuffle? Why am I saying shuffle? Streaming is set to true. So let me bring this down here. And let me also copy our initial loading code. So here. Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens. As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time. So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1269.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1350, 'published': '2021-09-23 13:30:07 UTC', 'start': 1269, 'text': ""Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens. As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time. So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated. So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1350,2021-09-23 13:30:07 UTC,1269,Because we're going to need to reload our data set now anyway. Because we just removed all the features from it. Okay and what I'm going to do now is just set streaming to false. And I'm going to run this same code where we still don't have our ids or anything like that in there. And we'll see what happens. As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time. So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated. So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1293.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1366, 'published': '2021-09-23 13:30:07 UTC', 'start': 1293, 'text': ""As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time. So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated. So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false. We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1366,2021-09-23 13:30:07 UTC,1293,As well we'll also notice we'll get a loading bar here. And it's going to take a little bit of time to process this. Although actually with this it's probably going to be super fast. So probably ignore that. But it will you see okay it's taking a little bit of time. So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated. So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false. We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1308.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1388, 'published': '2021-09-23 13:30:07 UTC', 'start': 1308, 'text': ""So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated. So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false. We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there. So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1388,2021-09-23 13:30:07 UTC,1308,So now it's going through a whole data set. We haven't called the date set. But we have used this map function. When streaming is set to false the data set isn't lazily loaded. And so the operation is going to be a bit more complicated. So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false. We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there. So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1324.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1409, 'published': '2021-09-23 13:30:07 UTC', 'start': 1324, 'text': ""So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false. We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there. So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers. The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1409,2021-09-23 13:30:07 UTC,1324,"So the operation the map operation is performed as soon as you call it. So it's a slightly different behavior. And the other behavior which is different is the fact that we only need to specify the answers feature here. So we only when we have streaming set to false. We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there. So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers. The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1342.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1438, 'published': '2021-09-23 13:30:07 UTC', 'start': 1342, 'text': ""We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there. So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers. The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it. Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1438,2021-09-23 13:30:07 UTC,1342,"We don't need to include every feature within the map operation. We only need to include the feature that we are modifying or creating. Which you know it's weird I don't know why there's a behavior difference when streaming is true or false. But it is there. So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers. The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it. Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1358.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1464, 'published': '2021-09-23 13:30:07 UTC', 'start': 1358, 'text': ""So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers. The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it. Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A. So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1464,2021-09-23 13:30:07 UTC,1358,"So if I now take this again. Come down here and run that. We see now that we have all of our features again. Right so before when streaming was true. If I run this code it would have only included our answers. The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it. Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A. So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1378.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1488, 'published': '2021-09-23 13:30:07 UTC', 'start': 1378, 'text': ""The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it. Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A. So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased. Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1488,2021-09-23 13:30:07 UTC,1378,"The id, title, context, question they all would have been removed. But now with streaming equal to false they're still there. So weird a weird. So it's a weird feature or a weird behavior. But it's how it is and we obviously just need to deal with it. Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A. So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased. Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1402.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1516, 'published': '2021-09-23 13:30:07 UTC', 'start': 1402, 'text': ""Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A. So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased. Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way. So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1516,2021-09-23 13:30:07 UTC,1402,Now the next thing I want to show you is how we can also add batching to our mapping process. So typically with well pretty much every or any as far as I can think of any NLP tasks we're going to want to tokenize our text. So we're going to go ahead and do that for Q&A. So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased. Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way. So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1427.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1540, 'published': '2021-09-23 13:30:07 UTC', 'start': 1427, 'text': ""So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased. Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way. So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer. So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1540,2021-09-23 13:30:07 UTC,1427,So we would import transformers or from transformers import a BERT tokenizer. Let's say and I would initialize that. So this is you know what we typically do. We do tokenizer equals BERT tokenizer from pre-trained. And let's say BERT base un-gased. Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way. So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer. So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1456.3999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1563, 'published': '2021-09-23 13:30:07 UTC', 'start': 1456, 'text': ""Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way. So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer. So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs. You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1563,2021-09-23 13:30:07 UTC,1456,Okay I'll initialize that. And then what I want to do is I'm going to tokenize my context or question and context. In the format that SQUAD would usually expect when you're doing Q&A or building a Q&A model. And I want to do that using the map function. So you can do this in both streaming and non-streaming by the way. So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer. So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs. You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1481.6000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1584, 'published': '2021-09-23 13:30:07 UTC', 'start': 1481, 'text': ""So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer. So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs. You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length. And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1584,2021-09-23 13:30:07 UTC,1481,So we just write date set. It was train so same as before. Date set it was train or date set train dot map. We are using a lambda function so lambda x. And in here we just want to say tokenizer. So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs. You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length. And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1504.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1608, 'published': '2021-09-23 13:30:07 UTC', 'start': 1504, 'text': ""So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs. You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length. And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true. So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1608,2021-09-23 13:30:07 UTC,1504,So I'm not doing the usually when you write this you would include a dictionary here. But the tokenizer the output from the tokenizer is already in dictionary format. So we don't need to I don't need to do it in this case. But basically what we have here is is still a dictionary. And what I want to do is so with Q&A in your tokenizer you pass two text inputs. You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length. And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true. So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1530.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1636, 'published': '2021-09-23 13:30:07 UTC', 'start': 1530, 'text': ""You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length. And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true. So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32. So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1636,2021-09-23 13:30:07 UTC,1530,You pass your question and you would also pass your question. And you would also then pass your context. And as usual we would we set our max length. So usually 512. I would set padding equal to the max length. And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true. So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32. So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1556.3200000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1661, 'published': '2021-09-23 13:30:07 UTC', 'start': 1556, 'text': ""And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true. So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32. So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well. Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1661,2021-09-23 13:30:07 UTC,1556,And also do truncation as well. Okay so very typical tokenization process. Nothing there's nothing different going on here. This is what we normally do when we tokenize our text going into a transform model. And then we want to say okay batched equals true. So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32. So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well. Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1577.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1688, 'published': '2021-09-23 13:30:07 UTC', 'start': 1577, 'text': ""So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32. So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well. Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask. We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1688,2021-09-23 13:30:07 UTC,1577,So this allows us to do everything or perform this operation in batches. And then we can also specify our batch size. So batch size equals let's say 32. So now when we run this where is it where is it going? It's here. Now when we run this the map function here is going to tokenize our question and context in batches of 32. So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well. Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask. We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1601.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1727, 'published': '2021-09-23 13:30:07 UTC', 'start': 1601, 'text': ""So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well. Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask. We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false. We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1727,2021-09-23 13:30:07 UTC,1601,So let's go ahead and do that. Okay and then you can you can see that processing there. So I mean that's that's all we really need to do with that. So I think that's probably it for the map method. And we'll well I'll fast forward and we'll continue with I think a few of the methods I think are quite useful as well. Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask. We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false. We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1628.3999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1759, 'published': '2021-09-23 13:30:07 UTC', 'start': 1628, 'text': ""Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask. We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false. We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question. If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1759,2021-09-23 13:30:07 UTC,1628,Okay so that's just finishing up now. So we can go ahead and have a look at what we've actually produced. So come to here and say dataset train. So what do we have now? We have answers like we did before but now we also have attention mask. We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false. We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question. If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1652.3999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1789, 'published': '2021-09-23 13:30:07 UTC', 'start': 1652, 'text': ""We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false. We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question. If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example. So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1789,2021-09-23 13:30:07 UTC,1652,We have input ids and we also have token type ids. Which are the three tensors that we usually output from the tokenizer when we do that. So we now have those in there as well. We can also have a look another thing as well we can now rather than looping through our dataset because we're not using a we're not using streaming which is true we're using streaming equals false. We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question. If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example. So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1675.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1822, 'published': '2021-09-23 13:30:07 UTC', 'start': 1675, 'text': ""We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question. If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example. So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset. So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1822,2021-09-23 13:30:07 UTC,1675,We can now do this and we can see okay we have attention mask and it's not going to show me everything because it's quite large. So I'll just delete that but you can see that we have the attention mask in there. So what I want to do is say I want to be quite pedantic and I don't like the fact that there is the fact that we have one feature called title. Maybe I want to say okay it should be topic because it's a topic of the context and the question. If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example. So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset. So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1713.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1852, 'published': '2021-09-23 13:30:07 UTC', 'start': 1713, 'text': ""If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example. So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset. So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want. And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1852,2021-09-23 13:30:07 UTC,1713,If I want to be really pedantic and modify that I could say dataset train rename column. And to be honest you can use it for this of course but you're probably not going to you're probably going to use it more for when you need to rename a column to make sure it aligns to whatever the expected inputs are for a transformer model for example. So that's where you would use it but I'm just using this example. So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset. So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want. And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame.,Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1743.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1873, 'published': '2021-09-23 13:30:07 UTC', 'start': 1743, 'text': ""So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset. So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want. And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame. Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1873,2021-09-23 13:30:07 UTC,1743,"So I'm going to rename the column title to topic and let's print out and dataset train again. So down here we have title and the moment we're going to have topic. Okay so now we have topic. So just rename column like I said come useful not in this case but generally this is usually useful. Now what I may want to do as well is remove certain records from this dataset. So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want. And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame. Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1780.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1902, 'published': '2021-09-23 13:30:07 UTC', 'start': 1780, 'text': ""So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want. And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame. Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle. We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1902,2021-09-23 13:30:07 UTC,1780,"So so far we've been printing out the here we have this which is now topic. We have University of Notre Dame. Maybe for whatever reason we don't want to include those topics so we can say very similar to before we write dataset train equals dataset train again. This time I'm going to filter so we're going to filter out records that we don't want. And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame. Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle. We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1809.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1934, 'published': '2021-09-23 13:30:07 UTC', 'start': 1809, 'text': ""And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame. Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle. We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this. It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1934,2021-09-23 13:30:07 UTC,1809,"And again it's very similar to the syntax we use for the map function which is the lambda. And in here we just need to specify the condition for the samples that we do want to include or we do want to keep. And in this case we want to say okay wherever the topic is not equal to University of Notre Dame. Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle. We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this. It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1839.2800000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1972, 'published': '2021-09-23 13:30:07 UTC', 'start': 1839, 'text': ""Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle. We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this. It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference with Q&A with a transformer model. We don't really need all of the features that we have here. So we would only need the attention mask, the input ids and also the token type ids. So what we can do now is we can remove some of those columns. So we'll do dataset train as always dataset train again."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,1972,2021-09-23 13:30:07 UTC,1839,"Okay so we'll run this and we'll have a look at what we produce. So dataset train so somehow like we have number of rows here which is just over 88,000. And we should get a lower number now. Now this will also go through so this remember we have shuffle. Set to shuffle what I keep calling it shuffle. We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this. It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference with Q&A with a transformer model. We don't really need all of the features that we have here. So we would only need the attention mask, the input ids and also the token type ids. So what we can do now is we can remove some of those columns. So we'll do dataset train as always dataset train again.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1863.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2006, 'published': '2021-09-23 13:30:07 UTC', 'start': 1863, 'text': ""We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this. It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference with Q&A with a transformer model. We don't really need all of the features that we have here. So we would only need the attention mask, the input ids and also the token type ids. So what we can do now is we can remove some of those columns. So we'll do dataset train as always dataset train again. And we want to remove those columns so remove columns. And we'll just remove so all of them other than the ones that we want. So do answers context id question and topic. Okay and then let's have a look at what we have left. Okay and then that's it so we have those final features and these are the ones that we would"", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,2006,2021-09-23 13:30:07 UTC,1863,"We have streaming set to false this time. So it's going to run through the whole dataset and perform this filtering operation. Now whilst we're waiting for that. Now I'll just fast forward again to when this finishes in a moment. Okay so now we have this finished and we can now run this. It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference with Q&A with a transformer model. We don't really need all of the features that we have here. So we would only need the attention mask, the input ids and also the token type ids. So what we can do now is we can remove some of those columns. So we'll do dataset train as always dataset train again. And we want to remove those columns so remove columns. And we'll just remove so all of them other than the ones that we want. So do answers context id question and topic. Okay and then let's have a look at what we have left. Okay and then that's it so we have those final features and these are the ones that we would",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
r-zQQ16wTCA-t1888.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2029, 'published': '2021-09-23 13:30:07 UTC', 'start': 1888, 'text': ""It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference with Q&A with a transformer model. We don't really need all of the features that we have here. So we would only need the attention mask, the input ids and also the token type ids. So what we can do now is we can remove some of those columns. So we'll do dataset train as always dataset train again. And we want to remove those columns so remove columns. And we'll just remove so all of them other than the ones that we want. So do answers context id question and topic. Okay and then let's have a look at what we have left. Okay and then that's it so we have those final features and these are the ones that we would input into a transform model for training. Now I mean there's nothing else I really want to cover. I think that is pretty much all you need to know on Iconface datasets to get started and start building pretty I think good input pipelines and using some of the datasets that are available."", 'title': 'Build NLP Pipelines with HuggingFace Datasets', 'url': 'https://youtu.be/r-zQQ16wTCA'}",UCv83tO5cePwHMt1952IVVHw,2029,2021-09-23 13:30:07 UTC,1888,"It's finished and we have before we had 88,000 rows now we have 87.3. And we should see so let me take the dataset train topic and I want to see let's say the first five of those. Okay now they're all Beyonce rather than before where it was the University of Notre Dame. So we have those and what we may want to do now is say for example we're performing inference with Q&A with a transformer model. We don't really need all of the features that we have here. So we would only need the attention mask, the input ids and also the token type ids. So what we can do now is we can remove some of those columns. So we'll do dataset train as always dataset train again. And we want to remove those columns so remove columns. And we'll just remove so all of them other than the ones that we want. So do answers context id question and topic. Okay and then let's have a look at what we have left. Okay and then that's it so we have those final features and these are the ones that we would input into a transform model for training. Now I mean there's nothing else I really want to cover. I think that is pretty much all you need to know on Iconface datasets to get started and start building pretty I think good input pipelines and using some of the datasets that are available.",Build NLP Pipelines with HuggingFace Datasets,https://youtu.be/r-zQQ16wTCA
DFtP1THE8fE-t14.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 132, 'published': '2020-12-04 14:00:03 UTC', 'start': 14, 'text': ""that we can simply import and use to make predictions. So it actually allows us to use some of the most powerful models out there as well. So in this tutorial, we're going to be using the Distilbert model, which is based on a BERT, but it's a lot smaller, but almost as powerful as BERT itself. So we're going to go ahead and begin. First, if you haven't already, you need to pip install Flare. And alongside Flare, you are also going to need PyTorch. If you haven't got PyTorch installed already, you'll need to head over to the PyTorch website. And they give you instructions on exactly what you need to install. So we come down to here and we can see, OK, for me, I have Windows. I want to install using Conda, using Python and then CUDA. So this is if you have a CUDA enabled GPU on your machine. If you don't know what that means, you probably don't. So in that case, just click none. But for me, I have 10.2. So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,132,2020-12-04 14:00:03 UTC,14,"that we can simply import and use to make predictions. So it actually allows us to use some of the most powerful models out there as well. So in this tutorial, we're going to be using the Distilbert model, which is based on a BERT, but it's a lot smaller, but almost as powerful as BERT itself. So we're going to go ahead and begin. First, if you haven't already, you need to pip install Flare. And alongside Flare, you are also going to need PyTorch. If you haven't got PyTorch installed already, you'll need to head over to the PyTorch website. And they give you instructions on exactly what you need to install. So we come down to here and we can see, OK, for me, I have Windows. I want to install using Conda, using Python and then CUDA. So this is if you have a CUDA enabled GPU on your machine. If you don't know what that means, you probably don't. So in that case, just click none. But for me, I have 10.2. So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t40.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 165, 'published': '2020-12-04 14:00:03 UTC', 'start': 40, 'text': ""First, if you haven't already, you need to pip install Flare. And alongside Flare, you are also going to need PyTorch. If you haven't got PyTorch installed already, you'll need to head over to the PyTorch website. And they give you instructions on exactly what you need to install. So we come down to here and we can see, OK, for me, I have Windows. I want to install using Conda, using Python and then CUDA. So this is if you have a CUDA enabled GPU on your machine. If you don't know what that means, you probably don't. So in that case, just click none. But for me, I have 10.2. So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away. So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,165,2020-12-04 14:00:03 UTC,40,"First, if you haven't already, you need to pip install Flare. And alongside Flare, you are also going to need PyTorch. If you haven't got PyTorch installed already, you'll need to head over to the PyTorch website. And they give you instructions on exactly what you need to install. So we come down to here and we can see, OK, for me, I have Windows. I want to install using Conda, using Python and then CUDA. So this is if you have a CUDA enabled GPU on your machine. If you don't know what that means, you probably don't. So in that case, just click none. But for me, I have 10.2. So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away. So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t67.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 190, 'published': '2020-12-04 14:00:03 UTC', 'start': 67, 'text': ""I want to install using Conda, using Python and then CUDA. So this is if you have a CUDA enabled GPU on your machine. If you don't know what that means, you probably don't. So in that case, just click none. But for me, I have 10.2. So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away. So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment. En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,190,2020-12-04 14:00:03 UTC,67,"I want to install using Conda, using Python and then CUDA. So this is if you have a CUDA enabled GPU on your machine. If you don't know what that means, you probably don't. So in that case, just click none. But for me, I have 10.2. So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away. So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment. En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t89.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 211, 'published': '2020-12-04 14:00:03 UTC', 'start': 89, 'text': ""So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away. So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment. En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data. I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,211,2020-12-04 14:00:03 UTC,89,"So all we need to do is copy the command underneath here. And then we would run this in our Anaconda prompt. I already have these installed, so I'm going to go ahead and actually begin coding. So we're going to need to use Pandas and also Flare. So now we have imported Flare, we can actually import a sentiment model straight away. So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment. En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data. I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t122.88000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 235, 'published': '2020-12-04 14:00:03 UTC', 'start': 122, 'text': ""So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment. En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data. I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes. You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,235,2020-12-04 14:00:03 UTC,122,"So all we need to do is pass our sentiment model to a variable. Which we will call sentiment model. And we just need to write Flare.models.textClassifier and load. And then in here, we pass the model name that we would like to load. And in our case, it will be the English sentiment model, which is en-sentiment. En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data. I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes. You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t159.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 274, 'published': '2020-12-04 14:00:03 UTC', 'start': 159, 'text': ""En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data. I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes. You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data. So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,274,2020-12-04 14:00:03 UTC,159,"En-sentiment. Like so. OK, so now we are downloading the model. And in a moment that will have downloaded and we can begin using it. Now, obviously, we need data. I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes. You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data. So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t181.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 312, 'published': '2020-12-04 14:00:03 UTC', 'start': 181, 'text': ""I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes. You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data. So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase. That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,312,2020-12-04 14:00:03 UTC,181,"I have downloaded some data here. Which is a sentiment data set based on the IMDB Movery reviews. So you can find the same data set over here. OK, so sentiment analysis on Movery reviews data set. So it's from Rotten Tomatoes. You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data. So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase. That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t203.35999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 341, 'published': '2020-12-04 14:00:03 UTC', 'start': 203, 'text': ""You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data. So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase. That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first. OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,341,2020-12-04 14:00:03 UTC,203,"You can just scroll down and we have the training data and test data here. I'm just going to use the test data and build a test data set. I'm just going to use the test data, but we can use either. We're just going to be making predictions based on the phrase here. So we need to read in our data. So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase. That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first. OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t227.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 381, 'published': '2020-12-04 14:00:03 UTC', 'start': 227, 'text': ""So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase. That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first. OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID. It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,381,2020-12-04 14:00:03 UTC,227,"So it's going to read it in as if it were a CSV file. And we will just pass a tab as our separator, because we are actually working with a tab separated file. OK, so here it's actually a CSV, not CSV. OK, so the first thing you'll notice is that we actually have duplicates of the same phrase. That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first. OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID. It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t265.91999999999996,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 411, 'published': '2020-12-04 14:00:03 UTC', 'start': 265, 'text': ""That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first. OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID. It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry. OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,411,2020-12-04 14:00:03 UTC,265,"That is actually just how this data set is. It just contains the full phrase initially. So this first entry here is the full phrase. And then all of these following are actually parts of that phrase. So what we can do, so let's change it so we can actually see the full phrase first. OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID. It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry. OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t304.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 436, 'published': '2020-12-04 14:00:03 UTC', 'start': 304, 'text': ""OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID. It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry. OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens. So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,436,2020-12-04 14:00:03 UTC,304,"OK, so we can't really see that much more anyway, but we can see that the full phrase has fine. So to remove this, we just want to drop all of the duplicates whilst keeping the first instance of the sentence ID. So you see each one of these, they all have the same sentence ID. It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry. OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens. So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t326.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 461, 'published': '2020-12-04 14:00:03 UTC', 'start': 326, 'text': ""It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry. OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens. So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens. And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,461,2020-12-04 14:00:03 UTC,326,"It's actually only the first one that we need. So we just drop duplicates on this column, keeping the first entry. OK, so we're keeping the first entry, dropping duplicates from sentence ID, and we're just doing this operation in place. OK, so now we can see each sample is now a unique entry. OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens. So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens. And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t369.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 484, 'published': '2020-12-04 14:00:03 UTC', 'start': 369, 'text': ""OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens. So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens. And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction. Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,484,2020-12-04 14:00:03 UTC,369,"OK, so now our data is ready. So we need to actually first convert our text into a token as list using Flare. So Flare does this one sentence at a time. So if we, for example, pass Hello World into the Flare tokenizer, we will be able to see what it's actually doing. OK, so here we can see that it split each one of these into tokens. So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens. And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction. Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t403.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 520, 'published': '2020-12-04 14:00:03 UTC', 'start': 403, 'text': ""So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens. And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction. Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample. And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,520,2020-12-04 14:00:03 UTC,403,"So we've got Hello is a token, World is a token, and then we have also split the exclamation mark at the end there. And you can see that Flare is telling us that there are a total of three tokens. So we can see that Flare is telling us that there are three tokens. So we can see that Flare is telling us that there are three tokens. And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction. Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample. And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t424.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 551, 'published': '2020-12-04 14:00:03 UTC', 'start': 424, 'text': ""And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction. Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample. And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels. So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,551,2020-12-04 14:00:03 UTC,424,"And you can see that Flare is telling us that there are a total of three tokens there. So each one of our samples here will need to be processed by this Flare.data.sentence method before we pass it into the actual model. Once we do have this, so let's call this sample as well. We will pass it to our model for prediction. Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample. And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels. So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t454.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 602, 'published': '2020-12-04 14:00:03 UTC', 'start': 454, 'text': ""Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample. And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels. So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive. And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,602,2020-12-04 14:00:03 UTC,454,"Which is really easy. All we need to do is call the predict method. On the sample. And now this doesn't output anything. Instead, it actually just modifies the sentence object that we have produced. So it modifies sample. And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels. So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive. And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t474.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 695, 'published': '2020-12-04 14:00:03 UTC', 'start': 474, 'text': ""And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels. So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive. And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list. And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,695,2020-12-04 14:00:03 UTC,474,"And we can see now that our sample, we solved the sentence and we solved the number of tokens. But we also have these additional labels which are the predictions. We have the label which is positive, which means it's a happy or it's a positive sentiment. And then what we have here is actually the probability or the confidence in that prediction. That's great, but realistically we want to be extracting these labels. So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive. And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list. And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t506.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 736, 'published': '2020-12-04 14:00:03 UTC', 'start': 506, 'text': ""So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive. And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list. And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here. Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,736,2020-12-04 14:00:03 UTC,506,"So we're actually able to extract these by accessing the labels method. So you have labels here and this produces the positive and the confidence. To access each one of these we access the positive and the confidence. We access index zero followed by dot value. Okay, so this will give us the positive. And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list. And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here. Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t543.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 770, 'published': '2020-12-04 14:00:03 UTC', 'start': 543, 'text': ""And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list. And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here. Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame. So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,770,2020-12-04 14:00:03 UTC,543,"And then we can also do the same to get the confidence called the score. Like that. So what we can do now is just create a simple for loop that will go through each sample in our test data and assign a probability for each one. So we will initially create a sentiment and confidence list. And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here. Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame. So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t571.7,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 792, 'published': '2020-12-04 14:00:03 UTC', 'start': 571, 'text': ""And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here. Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame. So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good. So intermittently pleasing but mostly routine effort. Incredibly negative but basically saying it's occasionally okay but generally nothing special. So obviously it's a negative sentiment which is matched up to negative sentiment here. Here we're saying okay Kidman's the only thing that's worth watching in birthday girl. And it serves as another example of the sad decline of British comedies in the post full monty world."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,792,2020-12-04 14:00:03 UTC,571,"And then we will just as we are looping through the data we will append our sentiment values. So the positive or negative and the confidence to each one of these lists. So here we are first tokenizing our sentence. Then we are making a prediction using that tokenized sentence which we are calling sample. And as we did before we have now got this labeled sentence and we just need to extract the two labels that we have here. Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame. So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good. So intermittently pleasing but mostly routine effort. Incredibly negative but basically saying it's occasionally okay but generally nothing special. So obviously it's a negative sentiment which is matched up to negative sentiment here. Here we're saying okay Kidman's the only thing that's worth watching in birthday girl. And it serves as another example of the sad decline of British comedies in the post full monty world.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t653.14,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 814, 'published': '2020-12-04 14:00:03 UTC', 'start': 653, 'text': ""Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame. So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good. So intermittently pleasing but mostly routine effort. Incredibly negative but basically saying it's occasionally okay but generally nothing special. So obviously it's a negative sentiment which is matched up to negative sentiment here. Here we're saying okay Kidman's the only thing that's worth watching in birthday girl. And it serves as another example of the sad decline of British comedies in the post full monty world. Fair enough, also negative. So this one is our first positive. Once you get into it it's relevant. The movie becomes a heady experience. Yeah, I mean sounds pretty positive to me."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,814,2020-12-04 14:00:03 UTC,653,"Okay, so we can see here that one of our sentences was just blank so we will add in some logic to avoid any errors there. Okay, so looking at this it's also whenever there's a space as well. So we just need to trim this which we can do easily using the strip method. Okay, so it took a little bit of time but we now have our predictions. So what we want to do is actually add what we have here in the sentiment and confidence list to our data frame. So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good. So intermittently pleasing but mostly routine effort. Incredibly negative but basically saying it's occasionally okay but generally nothing special. So obviously it's a negative sentiment which is matched up to negative sentiment here. Here we're saying okay Kidman's the only thing that's worth watching in birthday girl. And it serves as another example of the sad decline of British comedies in the post full monty world. Fair enough, also negative. So this one is our first positive. Once you get into it it's relevant. The movie becomes a heady experience. Yeah, I mean sounds pretty positive to me.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
DFtP1THE8fE-t719.86,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 837, 'published': '2020-12-04 14:00:03 UTC', 'start': 719, 'text': ""So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good. So intermittently pleasing but mostly routine effort. Incredibly negative but basically saying it's occasionally okay but generally nothing special. So obviously it's a negative sentiment which is matched up to negative sentiment here. Here we're saying okay Kidman's the only thing that's worth watching in birthday girl. And it serves as another example of the sad decline of British comedies in the post full monty world. Fair enough, also negative. So this one is our first positive. Once you get into it it's relevant. The movie becomes a heady experience. Yeah, I mean sounds pretty positive to me. So it's quite good even here where we're not saying anything particularly like a negative or positive word. We're just saying that the movie is or the movie delivers on the performance of striking skill and depth. Which must be pretty hard for a machine to understand and actually get right. But looking at all these it's doing really well. And I think it's really cool that we can actually do this with so little effort."", 'title': 'How-to do Sentiment Analysis with Flair in Python', 'url': 'https://youtu.be/DFtP1THE8fE'}",UCv83tO5cePwHMt1952IVVHw,837,2020-12-04 14:00:03 UTC,719,"So to do that we just add df sentiment to create a new sentiment column. And we made that equal to the sentiment list that we have created. And then we also do the same for confidence as well. Then we can see our data frame. Okay, so initially looking at this it looks pretty good. So intermittently pleasing but mostly routine effort. Incredibly negative but basically saying it's occasionally okay but generally nothing special. So obviously it's a negative sentiment which is matched up to negative sentiment here. Here we're saying okay Kidman's the only thing that's worth watching in birthday girl. And it serves as another example of the sad decline of British comedies in the post full monty world. Fair enough, also negative. So this one is our first positive. Once you get into it it's relevant. The movie becomes a heady experience. Yeah, I mean sounds pretty positive to me. So it's quite good even here where we're not saying anything particularly like a negative or positive word. We're just saying that the movie is or the movie delivers on the performance of striking skill and depth. Which must be pretty hard for a machine to understand and actually get right. But looking at all these it's doing really well. And I think it's really cool that we can actually do this with so little effort.",How-to do Sentiment Analysis with Flair in Python,https://youtu.be/DFtP1THE8fE
ZIRmXkHp0-c-t7.6000000000000005,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 188, 'published': '2021-02-12 13:30:03 UTC', 'start': 7, 'text': ""So for those of you that don't know, Q&A just means question answering and it's one of the biggest topics in NLP at the moment. There's a lot of models out there where you ask a question and it will give you an answer. And one of the biggest things that you need to know how to do when you are working with transformers, whether that's Q&A or any of the other transformer based solutions, is how to actually fine-tune those. So that's what we're going to be doing in this video. We're going to go through how we can fine-tune a Q&A Transformer model in Python. So I think it's really interesting and I think you will enjoy it a lot. So let's just go ahead and we can get started. Okay, so first thing we need to do is actually download our data. So we're going to be using the SQuAD dataset, which is the Stanford question answering dataset, which is essentially one of the better known Q&A datasets out there that we can use to fine-tune our model. So let's first create a folder. It's going to use OS and OSMaker. We'll just call it SQuAD. Obviously, call this and organize it as you want. This is what I will be doing. Now, the URL that we are going to be downloading this from is this. Okay, and there are actually two files here that we're going to be downloading, but both will be coming from the same URL. So because we're making a request through URL, we're going to import requests. We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,188,2021-02-12 13:30:03 UTC,7,"So for those of you that don't know, Q&A just means question answering and it's one of the biggest topics in NLP at the moment. There's a lot of models out there where you ask a question and it will give you an answer. And one of the biggest things that you need to know how to do when you are working with transformers, whether that's Q&A or any of the other transformer based solutions, is how to actually fine-tune those. So that's what we're going to be doing in this video. We're going to go through how we can fine-tune a Q&A Transformer model in Python. So I think it's really interesting and I think you will enjoy it a lot. So let's just go ahead and we can get started. Okay, so first thing we need to do is actually download our data. So we're going to be using the SQuAD dataset, which is the Stanford question answering dataset, which is essentially one of the better known Q&A datasets out there that we can use to fine-tune our model. So let's first create a folder. It's going to use OS and OSMaker. We'll just call it SQuAD. Obviously, call this and organize it as you want. This is what I will be doing. Now, the URL that we are going to be downloading this from is this. Okay, and there are actually two files here that we're going to be downloading, but both will be coming from the same URL. So because we're making a request through URL, we're going to import requests. We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t51.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 220, 'published': '2021-02-12 13:30:03 UTC', 'start': 51, 'text': ""So I think it's really interesting and I think you will enjoy it a lot. So let's just go ahead and we can get started. Okay, so first thing we need to do is actually download our data. So we're going to be using the SQuAD dataset, which is the Stanford question answering dataset, which is essentially one of the better known Q&A datasets out there that we can use to fine-tune our model. So let's first create a folder. It's going to use OS and OSMaker. We'll just call it SQuAD. Obviously, call this and organize it as you want. This is what I will be doing. Now, the URL that we are going to be downloading this from is this. Okay, and there are actually two files here that we're going to be downloading, but both will be coming from the same URL. So because we're making a request through URL, we're going to import requests. We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here. Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,220,2021-02-12 13:30:03 UTC,51,"So I think it's really interesting and I think you will enjoy it a lot. So let's just go ahead and we can get started. Okay, so first thing we need to do is actually download our data. So we're going to be using the SQuAD dataset, which is the Stanford question answering dataset, which is essentially one of the better known Q&A datasets out there that we can use to fine-tune our model. So let's first create a folder. It's going to use OS and OSMaker. We'll just call it SQuAD. Obviously, call this and organize it as you want. This is what I will be doing. Now, the URL that we are going to be downloading this from is this. Okay, and there are actually two files here that we're going to be downloading, but both will be coming from the same URL. So because we're making a request through URL, we're going to import requests. We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here. Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t88.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 248, 'published': '2021-02-12 13:30:03 UTC', 'start': 88, 'text': ""It's going to use OS and OSMaker. We'll just call it SQuAD. Obviously, call this and organize it as you want. This is what I will be doing. Now, the URL that we are going to be downloading this from is this. Okay, and there are actually two files here that we're going to be downloading, but both will be coming from the same URL. So because we're making a request through URL, we're going to import requests. We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here. Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file. And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,248,2021-02-12 13:30:03 UTC,88,"It's going to use OS and OSMaker. We'll just call it SQuAD. Obviously, call this and organize it as you want. This is what I will be doing. Now, the URL that we are going to be downloading this from is this. Okay, and there are actually two files here that we're going to be downloading, but both will be coming from the same URL. So because we're making a request through URL, we're going to import requests. We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here. Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file. And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t126.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 273, 'published': '2021-02-12 13:30:03 UTC', 'start': 126, 'text': ""We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here. Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file. And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open. And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,273,2021-02-12 13:30:03 UTC,126,"We can also use the Wget library as well, or if you're on Linux, you can just use Wget directly in the terminal. It's up to you what we're going to be using requests. Okay, and to request our data, we're going to be doing this. So it's just a get request, use a FString, and we have the URL that we've already defined. And then the training data that we'll be using is this file here. Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file. And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open. And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t172.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 296, 'published': '2021-02-12 13:30:03 UTC', 'start': 172, 'text': ""Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file. And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open. And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here. And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,296,2021-02-12 13:30:03 UTC,172,"Okay, requests. Okay, we can see that we've successfully put that data in there. Okay, so like I said before, there's actually two of these files that we want to extract. So what I'm going to do is just put this into a for loop, which will go through both of them. Just copy and paste this across. Rename this file. And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open. And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here. And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t210.48000000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 308, 'published': '2021-02-12 13:30:03 UTC', 'start': 210, 'text': ""And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open. And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here. And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four. And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,308,2021-02-12 13:30:03 UTC,210,"And the other file is the same, but instead of train, we have dev. Okay, so here we're making our request. And then next thing we want to do after making our request is actually saving this file to our drive. Okay, and we want to put that inside this squad folder here. So to do that, we use open. And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here. And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four. And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t238.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 334, 'published': '2021-02-12 13:30:03 UTC', 'start': 238, 'text': ""And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here. And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four. And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now. So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,334,2021-02-12 13:30:03 UTC,238,"And again, we're going to use a string here. I want to put inside the squad folder here. And then here we are just going to put our file name, which is file. Now we're writing this in binary because it's JSON. So we put wb for our flags here. And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four. And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now. So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t262.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 355, 'published': '2021-02-12 13:30:03 UTC', 'start': 262, 'text': ""And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four. And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now. So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers. And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,355,2021-02-12 13:30:03 UTC,262,"And then within this namespace, we are going to run through the file and download it in chunks. So we do for chunk. And then we iterate through the response. Like this. We'll just use a chunk size of four. And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now. So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers. And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t288.40000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 378, 'published': '2021-02-12 13:30:03 UTC', 'start': 288, 'text': 'And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now. So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers. And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context. So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract.', 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,378,2021-02-12 13:30:03 UTC,288,"And then we just want to write to the file like that. So that will download both files. Just add the colon there. So that will download both files. We should be able to see them here now. So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers. And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context. So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t303.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 405, 'published': '2021-02-12 13:30:03 UTC', 'start': 303, 'text': ""So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers. And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context. So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract. And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,405,2021-02-12 13:30:03 UTC,303,"So in here we have data. We have essentially a lot of different topics. So the first one is Beyonce. And then in here, we will see, if we just come to here, we get a context. But alongside this context, we also have QAS, which is question and answers. And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context. So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract. And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t323.84000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 423, 'published': '2021-02-12 13:30:03 UTC', 'start': 323, 'text': ""And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context. So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract. And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same. OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,423,2021-02-12 13:30:03 UTC,323,"And each one of these contains a question and answer pair. So we have this question, when did Beyonce start becoming popular? So this answer is actually within this context. And what we want our model to do is extract the answer from that context by telling us the start and end token of the answer within that context. So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract. And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same. OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t348.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 452, 'published': '2021-02-12 13:30:03 UTC', 'start': 348, 'text': ""So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract. And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same. OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs. There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,452,2021-02-12 13:30:03 UTC,348,"So we go zero and it is in the late 1990s. And we have answer start 269. So that means that a character 269, we get I. So if we go through here, we can find it here. OK, so this is the extract. And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same. OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs. There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t368.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 474, 'published': '2021-02-12 13:30:03 UTC', 'start': 368, 'text': ""And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same. OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs. There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position. So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,474,2021-02-12 13:30:03 UTC,368,"And that's what we will be aiming for our model to actually extract. But there will be a start point and also the end point as well, which is not included in here. But we will add that manually quite soon. So that's our data, and then we'll also be testing on the dev data as well, which is exactly the same. OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs. There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position. So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t399.28000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 496, 'published': '2021-02-12 13:30:03 UTC', 'start': 399, 'text': ""OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs. There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position. So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path. And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,496,2021-02-12 13:30:03 UTC,399,"OK, so we move on to the data prep. So now we have our files here. We're going to want to read them in. So we're going to use the JSON library for that. And like we saw before, there's quite a complex structure in these JSONs. There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position. So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path. And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t417.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 516, 'published': '2021-02-12 13:30:03 UTC', 'start': 417, 'text': ""There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position. So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path. And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB. I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,516,2021-02-12 13:30:03 UTC,417,"There's a lot of different layers. So we need to use a few for loops to fill through each of these and extract what we want, which is the context, questions and answers all corresponding to each other. So in the end, we're going to have a list of strings, which is going to be all of these. And in the case of the answers, we also have the starting position. So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path. And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB. I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t438.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 554, 'published': '2021-02-12 13:30:03 UTC', 'start': 438, 'text': ""So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path. And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB. I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across. And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,554,2021-02-12 13:30:03 UTC,438,"So it will be a list of dictionaries where one value is a text and one value is the starting position. So to do that, we're going to define a function called Rebsquad. We'll define our path here as well. And the first thing we need to do is actually open the JSON file. So we do with open path. And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB. I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across. And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t468.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 571, 'published': '2021-02-12 13:30:03 UTC', 'start': 468, 'text': ""And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB. I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across. And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here. Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,571,2021-02-12 13:30:03 UTC,468,"And again, we are using a binary file. So we're going to have B as a flag. But instead of writing, we are reading. So use R here. So RB. I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across. And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here. Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t488.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 600, 'published': '2021-02-12 13:30:03 UTC', 'start': 488, 'text': ""I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across. And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here. Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data. And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,600,2021-02-12 13:30:03 UTC,488,"I'm just going to do JSON load F here. So now we have our dictionary within this squad dict here. So maybe whilst we're just building this function up, it's probably more useful to do it here. So we can see what we're actually doing. So let's copy that across. And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here. Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data. And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t512.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 628, 'published': '2021-02-12 13:30:03 UTC', 'start': 512, 'text': ""And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here. Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data. And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce. And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,628,2021-02-12 13:30:03 UTC,512,"And then we'll fill this out afterwards. And then we'll fill this out afterwards. Of course, we do actually need to include the path. So let's take this. And now we can see what's inside here. Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data. And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce. And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t544.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 658, 'published': '2021-02-12 13:30:03 UTC', 'start': 544, 'text': ""Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data. And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce. And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict. And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,658,2021-02-12 13:30:03 UTC,544,"Maybe we can load just a few rather than all of them. Or we can investigate it like this. Okay, so we have the version and data, which we can actually see over here. Version and data. So we want to access the data. And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce. And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict. And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t566.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 680, 'published': '2021-02-12 13:30:03 UTC', 'start': 566, 'text': ""And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce. And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict. And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs. And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,680,2021-02-12 13:30:03 UTC,566,"And within data, we have a list of all these different items, which is what I was trying to do before. So we go into data and just take a few of those. Okay, and then we get our different sections. First one, let's just take zero, which is Beyonce. And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict. And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs. And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t593.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 705, 'published': '2021-02-12 13:30:03 UTC', 'start': 593, 'text': ""And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict. And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs. And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context. However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,705,2021-02-12 13:30:03 UTC,593,"And then we have all of these. So we're going to want to loop through each one of these because we have this one, the next, and we're going to keep needing to just run through all of these. So let's do that. We want to do for group in squad dict. And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs. And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context. However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t618.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 728, 'published': '2021-02-12 13:30:03 UTC', 'start': 618, 'text': ""And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs. And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context. However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context. So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,728,2021-02-12 13:30:03 UTC,618,"And remember, we need to include the data here. Let's just see how, say group title. So group title, so we can see a few of those. Okay, we're going to go through each one of those. So the second part of that are these paragraphs. And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context. However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context. So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t642.9599999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 755, 'published': '2021-02-12 13:30:03 UTC', 'start': 642, 'text': ""And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context. However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context. So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index. And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,755,2021-02-12 13:30:03 UTC,642,"And within the paragraphs, we have each one of our questions. So let's first go with paragraphs and we'll do a chop in here. Sorry, it's a list. There we go. And the first thing we need to extract is the easiest one, which is our context. However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context. So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index. And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t671.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 774, 'published': '2021-02-12 13:30:03 UTC', 'start': 671, 'text': ""However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context. So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index. And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go. So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,774,2021-02-12 13:30:03 UTC,671,"However, that is also within a list. So now if we access the context, we get this. So we're essentially going to need to jump through or loop through each one of these here. Now we're going to access the paragraphs and loop through each one of those. And then here, we're going to access the context. So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index. And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go. So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t696.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 795, 'published': '2021-02-12 13:30:03 UTC', 'start': 696, 'text': ""So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index. And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go. So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here. And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,795,2021-02-12 13:30:03 UTC,696,"So let's write that. So we already have one group here. So let's just stick with that. And we're going to run through the passage in the paragraphs. So already here, we're going through the for loop on this index. And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go. So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here. And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t720.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 823, 'published': '2021-02-12 13:30:03 UTC', 'start': 720, 'text': ""And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go. So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here. And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop. So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,823,2021-02-12 13:30:03 UTC,720,"And now we're going to go through the for loop. And now we're going to go through a loop on this index. Let's keep that. So that means that we will be able to print the passage context. And there we go. So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here. And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop. So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t750.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 857, 'published': '2021-02-12 13:30:03 UTC', 'start': 750, 'text': ""So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here. And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop. So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there. Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,857,2021-02-12 13:30:03 UTC,750,"So here we have all of our context. So that's one of our three items that we need to extract. Okay, so that's great. Let's put that all together. So we're going to take this, put it here. And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop. So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there. Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t765.3599999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 872, 'published': '2021-02-12 13:30:03 UTC', 'start': 765, 'text': ""And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop. So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there. Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect. So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there,"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,872,2021-02-12 13:30:03 UTC,765,"And then we have our context. Okay, that's great. But obviously for each context, we have a few different questions and answers. So we need to get those as well. Now, that requires us to go through another for loop. So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there. Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect. So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there,",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t786.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 902, 'published': '2021-02-12 13:30:03 UTC', 'start': 786, 'text': ""So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there. Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect. So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there, which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,902,2021-02-12 13:30:03 UTC,786,"So let's go this passage. We need to go into the QAS key and loop through this list of question and answers. So we have this, and then we have our list. So another layer in our for loop will be for question answer in passage QAS. And then let's take a look at what we have there. Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect. So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there, which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t817.2800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 932, 'published': '2021-02-12 13:30:03 UTC', 'start': 817, 'text': ""Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect. So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there, which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,932,2021-02-12 13:30:03 UTC,817,"Okay, great. So we have plausible answers, question and answers. So what we want in here is the question and answers. So question is our first one. Perfect. So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there, which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t834.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 952, 'published': '2021-02-12 13:30:03 UTC', 'start': 834, 'text': ""So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there, which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different. So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,952,2021-02-12 13:30:03 UTC,834,"So we have the questions now. And then after we have extracted the question, we can move on to our answers. As we see here, the answers comes as another list. Now each one of these lists all just have one actual answer in there, which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different. So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t869.0400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 978, 'published': '2021-02-12 13:30:03 UTC', 'start': 869, 'text': ""which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different. So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those. So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,978,2021-02-12 13:30:03 UTC,869,"which is completely fine. So we can access that in two ways. We can either loop through or we can access the zero value of that array. Either way, it doesn't matter. So all we need to do here is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different. So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those. So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t884.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1002, 'published': '2021-02-12 13:30:03 UTC', 'start': 884, 'text': ""is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different. So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those. So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1002,2021-02-12 13:30:03 UTC,884,"is loop through those answers or if we want just go with QA answers 0. So in most cases, this should be completely fine. As we can see here, most of these question and then they have the answers dictionary. The question and then they have the answers dictionary, which is fine. However, some of these are slightly different. So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those. So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t923.2800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1044, 'published': '2021-02-12 13:30:03 UTC', 'start': 923, 'text': ""So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those. So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay. Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1044,2021-02-12 13:30:03 UTC,923,"So if we scroll right down to the end here, see, okay, we have this which is talking about physics. And then rather than having our answers array, we have these plausible answers, which is obviously slightly different. And this is the case for a couple of those. So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay. Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t945.4399999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1067, 'published': '2021-02-12 13:30:03 UTC', 'start': 945, 'text': ""So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay. Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers. And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1067,2021-02-12 13:30:03 UTC,945,"So from what I've seen to state, it's like the best way to deal with this is simply to have a check. If there is a plausible answers key within the dictionary, we will include that as a check. Within the dictionary, we will include that as the answer rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay. Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers. And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t963.3599999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1101, 'published': '2021-02-12 13:30:03 UTC', 'start': 963, 'text': ""rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay. Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers. And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next. So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1101,2021-02-12 13:30:03 UTC,963,"rather than the actual answers dictionary. So to do that, all we need to do is check if QA keys contains plausible answers. If it does, we use that. Otherwise, we use answers. Okay. Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers. And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next. So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t998.4000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1131, 'published': '2021-02-12 13:30:03 UTC', 'start': 998, 'text': ""Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers. And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next. So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list. And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1131,2021-02-12 13:30:03 UTC,998,"Then we use this one. Otherwise, we will use answers. So let's just add all of that into our for loop here. So we have our context, and then we want to loop through the question answers. And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next. So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list. And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1022.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1160, 'published': '2021-02-12 13:30:03 UTC', 'start': 1022, 'text': ""And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next. So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list. And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context. And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1160,2021-02-12 13:30:03 UTC,1022,"And this is where we get our question. Then once we're here, we need to do something slightly different, which is this plausible answers. Okay. And then we use this access variable in order to define what we're going to loop through next. So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list. And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context. And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1057.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1184, 'published': '2021-02-12 13:30:03 UTC', 'start': 1057, 'text': ""So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list. And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context. And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again. But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1184,2021-02-12 13:30:03 UTC,1057,"So here we go for answers. Answer, sorry, in QA access, because this will switch to implausible answers or answers. And then within this for loop, this is where we can begin adding this context, question, and answer to a list of questions, contexts, and answers that we still need to define up here. So each one of these is just going to be an empty list. And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context. And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again. But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1089.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1211, 'published': '2021-02-12 13:30:03 UTC', 'start': 1089, 'text': ""And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context. And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again. But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set. So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1211,2021-02-12 13:30:03 UTC,1089,"And then all we do is copy this across, and we just append everything that we've extracted in this loop. And the context. And then we just add the context. And then we just add the context. And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again. But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set. So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1114.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1270, 'published': '2021-02-12 13:30:03 UTC', 'start': 1114, 'text': ""And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again. But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set. So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers. So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1270,2021-02-12 13:30:03 UTC,1114,"And the context, question, and answer. And that should work. So now let's take a look at a few of our contexts. Okay, and we can see we have this and because we have multiple question and answers for each context, the context just repeat over and over again. But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set. So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers. So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1152.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1305, 'published': '2021-02-12 13:30:03 UTC', 'start': 1152, 'text': ""But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set. So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers. So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them. And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1305,2021-02-12 13:30:03 UTC,1152,"But then we should see something slightly different when we go with answers. And questions. Okay, so that's great. We have our data in a reasonable format now. But we want to do this for both the training set and the validation set. So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers. So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them. And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1172.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1327, 'published': '2021-02-12 13:30:03 UTC', 'start': 1172, 'text': ""So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers. So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them. And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here. So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1327,2021-02-12 13:30:03 UTC,1172,"So what we're going to do is just going to put this into a function like we were going to do before. Just read squad. So here we're going to read in our data, and then we run through it and transform it into our three lists. All we need to do now is actually return those three lists. And answers. So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them. And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here. So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1203.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1353, 'published': '2021-02-12 13:30:03 UTC', 'start': 1203, 'text': ""So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them. And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here. So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position. So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1353,2021-02-12 13:30:03 UTC,1203,"So now what we're going to do is just go back to our training set. And we're going to do this for both the training set and the validation set. And answers. So now what we can do is execute this function for both our training and validation sets. So we're going to train context, questions and answers. Okay, so that is one of them. And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here. So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position. So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1252.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1384, 'published': '2021-02-12 13:30:03 UTC', 'start': 1252, 'text': ""And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here. So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position. So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is. And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1384,2021-02-12 13:30:03 UTC,1252,"And we can just copy that. And we just want this to be our validation set. Like so. Okay, so that's great. We now have the training context and the validation context, which we can see right here. So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position. So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is. And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1291.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1423, 'published': '2021-02-12 13:30:03 UTC', 'start': 1291, 'text': ""So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position. So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is. And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices. So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1423,2021-02-12 13:30:03 UTC,1291,"So here let's hope that there is a slight difference in what we see between both. Okay, great. That's what we would expect. Okay, so now we have our data almost in the right format. We just need to add the ending position. So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is. And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices. So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1316.8799999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1451, 'published': '2021-02-12 13:30:03 UTC', 'start': 1316, 'text': ""So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is. And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices. So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair. And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1451,2021-02-12 13:30:03 UTC,1316,"So we already have the start position if we take a look in our train answers. Okay, we have the answer start, but we also need the answer end. And that's not included within the data. So what we need to do here is actually define a function that will go through each one of our answers and context and figure out where that ending character actually is. And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices. So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair. And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1345.9199999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1491, 'published': '2021-02-12 13:30:03 UTC', 'start': 1345, 'text': ""And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices. So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair. And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here. So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1491,2021-02-12 13:30:03 UTC,1345,"And of course, we could just say, okay, it's the length of the text. We add that onto the answer start and we have our answer end. However, that unfortunately won't work because some of the answer starts are actually incorrect and they're usually off by one or two characters. So we actually need to go through and one, fix that and two, add our end indices. So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair. And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here. So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1375.1999999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1528, 'published': '2021-02-12 13:30:03 UTC', 'start': 1375, 'text': ""So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair. And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here. So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that. So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1528,2021-02-12 13:30:03 UTC,1375,"So to do that, we're just going to define a new function. It's going to be add end index. And here we will have our answers and the context. And then we're going to just feed these in. So first thing we do is loop through each answer and context pair. And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here. So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that. So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1415.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1554, 'published': '2021-02-12 13:30:03 UTC', 'start': 1415, 'text': ""And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here. So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that. So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there. And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1554,2021-02-12 13:30:03 UTC,1415,"And then we extract something which is called the gold text, which is essentially the answer that we are looking for. It's called the golden text or gold text. So simply our answer and within that the text. So we are pulling this out here. So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that. So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there. And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1439.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1571, 'published': '2021-02-12 13:30:03 UTC', 'start': 1439, 'text': ""So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that. So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there. And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1571,2021-02-12 13:30:03 UTC,1439,"So we should already know the starting index. So what we do here is simply pull that out as well. And then the end index ideally will be the start plus the length of the gold text. However, that's not always the case because like I said before, they can be off by one or two characters. So we need to add in some logic just to deal with that. So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there. And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1477.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1601, 'published': '2021-02-12 13:30:03 UTC', 'start': 1477, 'text': ""So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there. And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier. But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1601,2021-02-12 13:30:03 UTC,1477,"So in our first case, let's assume that the characters are not off. So if context start to end. Start to end equals the gold text. This means everything is good and we don't need to worry about it. So we can modify the original dictionary and we can add answer end into there. And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier. But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1517.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1642, 'published': '2021-02-12 13:30:03 UTC', 'start': 1517, 'text': ""And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier. But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine. So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1642,2021-02-12 13:30:03 UTC,1517,"And we made that equal to our end index. However, if that's not the case, that means we have a problem. It's one of those dodgy question answer pairs. And so this time what we can do is we'll add a else statement. So we're just going to go through when the position is off by one or two characters because it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier. But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine. So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1545.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1676, 'published': '2021-02-12 13:30:03 UTC', 'start': 1545, 'text': ""it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier. But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine. So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets. So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1676,2021-02-12 13:30:03 UTC,1545,"it is not off by any more than that in the squad data set. Loop through each of those and we'll say, OK, if the context. And then in here, we need to add the start index and this again. So let's just copy and paste that across. Be easier. But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine. So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets. So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1565.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1712, 'published': '2021-02-12 13:30:03 UTC', 'start': 1565, 'text': ""But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine. So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets. So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1712,2021-02-12 13:30:03 UTC,1565,"But this time we're checking to see if it is off by one or two characters. So just minus N. And it's always minus and it isn't shifted. It's always shifted to the left rather than shifted to the right. So that's this is fine. So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets. So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1582.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1760, 'published': '2021-02-12 13:30:03 UTC', 'start': 1582, 'text': ""So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets. So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this. And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1760,2021-02-12 13:30:03 UTC,1582,"So in this case, the answer is off by end tokens. And so we need to update our answer start value and also add our answer end value. So start index minus N and we also have the end. So that's great. We can take that and we can apply it to our train and validation sets. So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this. And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1628.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1788, 'published': '2021-02-12 13:30:03 UTC', 'start': 1628, 'text': ""So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this. And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on. So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1788,2021-02-12 13:30:03 UTC,1628,"So all we do here is call the function. And we just see train answers and train context. And of course, we can just copy this and do the same for our validation set. OK, perfect. So now if we have a quick look, we should be able to see that we have a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this. And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on. So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1665.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1807, 'published': '2021-02-12 13:30:03 UTC', 'start': 1665, 'text': ""a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this. And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on. So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer. And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1807,2021-02-12 13:30:03 UTC,1665,"a few of these ending points as well. OK, so I think that looks pretty good. And that means we can move on to actually encoding our text. To tokenize or encode our text, this is where we bring in a BERT tokenizer. So we need to import the Transformers library for this. And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on. So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer. And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1702.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1852, 'published': '2021-02-12 13:30:03 UTC', 'start': 1702, 'text': ""And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on. So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer. And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1852,2021-02-12 13:30:03 UTC,1702,"And from Transformers, we are going to import the Distilbert. So Distilbert is a smaller version of BERT, which is just going to run a bit quicker, but it will take a very long time. And we're going to import the FAST version of this tokenizer because this allows us to more easily adjust our character and then start locations to token and start locations later on. So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer. And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1731.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1892, 'published': '2021-02-12 13:30:03 UTC', 'start': 1731, 'text': ""So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer. And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects. So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1892,2021-02-12 13:30:03 UTC,1731,"So first, we need to actually initialize our tokenizer, which is super easy. All we're doing is loading it from a pre-trained model. And then all we do to create our encodings is to load the tokenizer. So we'll do the training set first. Let's call it tokenizer. And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects. So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1780.1599999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1930, 'published': '2021-02-12 13:30:03 UTC', 'start': 1780, 'text': ""And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects. So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1930,2021-02-12 13:30:03 UTC,1780,"And in here, we include our training context. And the training questions. So what this will do is actually merge these two strings together. So what we will have is our context and then there will be a separator token followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects. So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1802.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1957, 'published': '2021-02-12 13:30:03 UTC', 'start': 1802, 'text': ""followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects. So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,1957,2021-02-12 13:30:03 UTC,1802,"followed by the question. And this will be fed into Distilbert during training. I just want to add padding there as well. And then we'll copy this and do the same for our relation set. Okay, and this will convert our data into encoding objects. So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1827.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2004, 'published': '2021-02-12 13:30:03 UTC', 'start': 1827, 'text': ""So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2004,2021-02-12 13:30:03 UTC,1827,So what we can do here is So what we can do here is print out different parts that we have within our encodings. So in here you have the input IDs so let's access that and you'll find in here we have a big list of all of our samples so check that we have 130k and let's open one of those okay and we have these token IDs and this is what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1879.6799999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2036, 'published': '2021-02-12 13:30:03 UTC', 'start': 1879, 'text': ""what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2036,2021-02-12 13:30:03 UTC,1879,what Bert will be reading. Now if we want to have a look at what this actually is in sort of human readable language we can use the tokenizer to just decode it for us. Okay this is what we're feeding in so we have a couple of these special tokens this just means it's the sort of sequence and in here we have a process form of our original context. Now you find that the context actually ends here and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1918.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2072, 'published': '2021-02-12 13:30:03 UTC', 'start': 1918, 'text': ""and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2072,2021-02-12 13:30:03 UTC,1918,and like I said before we have this separated token and then after that we have our actual question and this is what is being fed into Bert but obviously the token ID version. So it's just good to be aware of what is actually being fed in and what we're actually using here but this is a format that Bert is expecting and then after that we have another separated token followed by all of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1945.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2110, 'published': '2021-02-12 13:30:03 UTC', 'start': 1945, 'text': ""of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2110,2021-02-12 13:30:03 UTC,1945,of our padding tokens because Bert is going to be expecting 512 tokens to be fed in for every one sample so we just need to fill that space essentially so that's all that is doing. So let's remove those and we can continue. So the next thing we need to add to our encodings is the start and end positions because at the moment we just don't have them in there. So to do that we need to add a additional bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t1983.1599999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2145, 'published': '2021-02-12 13:30:03 UTC', 'start': 1983, 'text': ""bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2145,2021-02-12 13:30:03 UTC,1983,bit of logic. We use this character to token method so if we just take out one of these. Let's take the first one. Okay we have this and what we can do is actually modify this to use the character token method. Remove the input IDs because we just need to pass it the index of whichever encoding we are wanting to modify or get the start and end position of and in here all we're doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2024.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2199, 'published': '2021-02-12 13:30:03 UTC', 'start': 2024, 'text': ""doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2199,2021-02-12 13:30:03 UTC,2024,doing is converting from the character that we have found a position for to the token that we want to find a position for and what we need to add is train answers. We have our position again because the answers and encodings the context and question that needs to match up to the answer of course that we're asking about and we do answers start. So here we're just feeding in the position of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2056.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2241, 'published': '2021-02-12 13:30:03 UTC', 'start': 2056, 'text': ""of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2241,2021-02-12 13:30:03 UTC,2056,of the character and this is answer. Okay so feeding in the position of the character and we're expecting to return the position of the token which is position 64. So all we need to do now is do this for both of those so for the start position and end position. See here we should get a different value. Okay but this is one limitations of this. Sometimes this is going to return nothing as you can see it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2098.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2270, 'published': '2021-02-12 13:30:03 UTC', 'start': 2098, 'text': ""it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2270,2021-02-12 13:30:03 UTC,2098,it's not returning anything here and that is because sometimes it is actually returning the space and when it looks at the space and the tokenizer see that and they say okay that's nothing we're not concerned about spaces and it returns this non value that you can see here. So this is something that we need to consider and build in some added logic for. So to do that again we're going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2127.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2307, 'published': '2021-02-12 13:30:03 UTC', 'start': 2127, 'text': ""going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2307,2021-02-12 13:30:03 UTC,2127,going to use a function to contain all this and call it add token positions. Here we'll have our encodings and our answers and then we just modify this code so we have the encodings we have the answers and because we're collecting all of the token positions we also need to initialize a list to containers. So we do start positions empty list and end positions. And now we just want to loop through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions.,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2174.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2358, 'published': '2021-02-12 13:30:03 UTC', 'start': 2174, 'text': ""through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions. Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2358,2021-02-12 13:30:03 UTC,2174,through every single answer and encoding that we have. Like so. And here we have our start position so we need to append that to our start positions list. And we just do the same for our end positions which is here. Now here we can deal with this problem that we had. So if we find that the end positions the most recent one so the negative one index is non that means it wasn't found and it means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions. Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2227.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2407, 'published': '2021-02-12 13:30:03 UTC', 'start': 2227, 'text': ""means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions. Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2407,2021-02-12 13:30:03 UTC,2227,"means there is a space. So what we do is we change it to instead use the minus one version. And all this needs to do is update the end positions here. Okay that's great but in some cases this also happens with the start position but that is for a different reason. The reason that will occasionally happen with start position is when the passage of data that we're adding in here so you saw before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions. Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2258.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2454, 'published': '2021-02-12 13:30:03 UTC', 'start': 2258, 'text': ""before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions. Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers. We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2454,2021-02-12 13:30:03 UTC,2258,"before we had the context that separated token and then the question. Sometimes the context passage is truncated in order to fit in the question. So some of it will be cut off and in that case we do have a bit of a problem but we still need to just allow our code to run without any problems. So what we do is we just modify the start positions again just like we did with the end positions. Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers. We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2292.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2519, 'published': '2021-02-12 13:30:03 UTC', 'start': 2292, 'text': ""Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers. We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2519,2021-02-12 13:30:03 UTC,2292,"Obviously only if it's a non and we just set it to be equal to the maximum length that has been defined by the tokenizer. It's as simple as that. Now the only final thing we need to do which is because we're using the encodings is actually update those encodings to include this data because as of yet we haven't added that back in. So to do that we can use this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers. We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2331.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2609, 'published': '2021-02-12 13:30:03 UTC', 'start': 2331, 'text': ""this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers. We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2609,2021-02-12 13:30:03 UTC,2331,"this quite handy update method and just add in our data as a dictionary. So you have start positions, start positions and we also have our end positions. And then again we just need to apply this to our training and validation sets and let's just modify that. Let's add the training encodings here and train answers. We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2379.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2644, 'published': '2021-02-12 13:30:03 UTC', 'start': 2379, 'text': ""We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2644,2021-02-12 13:30:03 UTC,2379,We do that again the validation set. So now let's take a look at our encodings and here we can see great now have those start positions and end positions. We can even so a quick look what they look like. What we've done is actually not included the index here so we're just taking it for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2433.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2692, 'published': '2021-02-12 13:30:03 UTC', 'start': 2433, 'text': ""for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2692,2021-02-12 13:30:03 UTC,2433,for the very first item every single time. So let's just update that. So obviously that won't get us very far. And just update that as well. And now this should look a little bit better. So it's lucky we checked. Okay so our data at the moment is in the right format. We just need to use it to create a PyTorch dataset object. So to do that obviously we need to import PyTorch. And we define that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2484.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2735, 'published': '2021-02-12 13:30:03 UTC', 'start': 2484, 'text': ""that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2735,2021-02-12 13:30:03 UTC,2484,that dataset using a class. I'm just passing the torch. utils data dataset. We need to initialize that. Like so. And this is coming from the Houden Face Transformers documentation. Don't take credit for this. And we essentially need to do this so that we can load in our data using the PyTorch data loader later on. Which makes things incredibly easy. And then we just have one more function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2566.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2764, 'published': '2021-02-12 13:30:03 UTC', 'start': 2566, 'text': ""function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2764,2021-02-12 13:30:03 UTC,2566,function here. Or method. Okay. And return. And also this as well. That should be okay. So we apply this to our datasets to create dataset objects. Now our encodings. And then the same again for the validation set. Okay so that is our data. Almost fully prepared. All we do now is load it into a data loader object. But this is everything on the data side done. Which is great because I know this bit does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2634.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2815, 'published': '2021-02-12 13:30:03 UTC', 'start': 2634, 'text': ""does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2815,2021-02-12 13:30:03 UTC,2634,"does take some time. And I know it's not the most interesting part of it. But it's just something that we need to do. And need to understand what we're doing as well. So now we get to the more interesting bit. So we'll just add the imports in here. So we need our data loader. We're going to import the Adam optimizer with weighted decay. Which is pretty commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2680.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2851, 'published': '2021-02-12 13:30:03 UTC', 'start': 2680, 'text': ""commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2851,2021-02-12 13:30:03 UTC,2680,"commonly used for transformer models when you are fine-tuning. Because transformer models are generally very large models. And they can over fit very easily. So this Adam optimizer with weighted decay essentially just reduces the chances of that happening. Which is supposed to be very useful and quite important. So obviously we're going to use that. And then final bit is TQDM. So TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2725.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2876, 'published': '2021-02-12 13:30:03 UTC', 'start': 2725, 'text': ""TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2876,2021-02-12 13:30:03 UTC,2725,"TQDM is a progress bar that we are going to be using. So that we can actually see the progress of our training. Otherwise we're just going to sit there for probably quite a long time not knowing what is actually happening. And trust me it won't take long before you start questioning whether anything is happening. Because it takes a long time to train these models. So they are our imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2750.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2919, 'published': '2021-02-12 13:30:03 UTC', 'start': 2750, 'text': ""imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2919,2021-02-12 13:30:03 UTC,2750,"imports. And I'm being stupid again here. That's from, did that twice. Okay so that's all good. So now we just need to do a few little bits for the setup. So we need to tell Pytorch whether we're using CPU or GPU. In my case it will be a GPU. If you're using CPU this is going to take you a very long time to train. And it's still going to take you a long time on GPU. So just be aware of that. But what we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2786.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2953, 'published': '2021-02-12 13:30:03 UTC', 'start': 2786, 'text': ""we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2953,2021-02-12 13:30:03 UTC,2786,we're going to do here is say device. It's CUDA. If CUDA is available. Otherwise we are going to use the CPU. And good luck if that is what you're doing. So once we've defined the device we want to move our model over to it. So we just.model.to device. So this.to method is essentially a way of transferring data between different hardware components. So your CPU or GPU. It's quite useful. And then we want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2837.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2996, 'published': '2021-02-12 13:30:03 UTC', 'start': 2837, 'text': ""want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,2996,2021-02-12 13:30:03 UTC,2837,want to activate our model for training. So there's two things we have here. So we have.train and eval. So when we're in train mode there's a lot of different layers and different parts of your model that will behave differently depending on whether you are using the model for training or you're using it for inference which is predictions. So we just need to make sure our model is in the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that.,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2865.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3039, 'published': '2021-02-12 13:30:03 UTC', 'start': 2865, 'text': ""the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that. Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3039,2021-02-12 13:30:03 UTC,2865,the right mode for whatever we're doing. And later on we'll switch it to eval to make some predictions. So that's almost everything. So we just need to initialize the optimizer. And here we're using the weighted decay Adam optimizer. We need to pass in our model parameters and also give it a learning rate. And we're going to use this value here. All these are the recommended parameters for what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that. Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2902.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3081, 'published': '2021-02-12 13:30:03 UTC', 'start': 2902, 'text': ""what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that. Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3081,2021-02-12 13:30:03 UTC,2902,what we are doing here. So the one thing that I have somehow missed is defining the actual initializing the model. So let's just add that in. And all we're doing here is loading again a pre-trained one. So like we did before when we were loading the transformers tokenizer. This time it's for question answering. So this the Silbert of question answering is a the Silbert model with a question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that. Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2944.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3120, 'published': '2021-02-12 13:30:03 UTC', 'start': 2944, 'text': ""question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that. Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3120,2021-02-12 13:30:03 UTC,2944,question and answering head added on to the end of it. So essentially with transformers you have all these different heads that you add on and they will do different things depending on what head it has on there. So let's initialize that from pre-trained. And we're using the same one we use up here which is the Silbert base uncased. And sometimes you will need to download that. Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t2989.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3148, 'published': '2021-02-12 13:30:03 UTC', 'start': 2989, 'text': ""Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3148,2021-02-12 13:30:03 UTC,2989,Fortunately I don't need to as I've already done that but this can also take a little bit of time. Not too long though and you get a nice progress bar hopefully as well. Okay so now that is all set up we can initialize our data loader. So all we're doing here is using the PyTorch data loader object and we just pass in our training data set. The batch size so how many we want to train on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3027.6800000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3181, 'published': '2021-02-12 13:30:03 UTC', 'start': 3027, 'text': ""on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3181,2021-02-12 13:30:03 UTC,3027,on at once in parallel before updating the model weights which will be 16. And we also would like to shuffle the data because we don't want to train the model on a single batch and it just learned about Beyonce and then the next one it's learning about Chopin and it will keep switching between those but never within a single batch having a good mix of different things to learn about. So it is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3058.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3211, 'published': '2021-02-12 13:30:03 UTC', 'start': 3058, 'text': ""is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3211,2021-02-12 13:30:03 UTC,3058,is data set seems a bit of a weird name to me so I'm just going to change it. And they also can't spell. There we go. And that is everything we can actually begin our training loop. So we're gonna go for three parts and what we want to start with here is a loop object. So we do this mainly because we're using TQDM as a progress bar otherwise we wouldn't need to do this. There would be no point in doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3108.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3257, 'published': '2021-02-12 13:30:03 UTC', 'start': 3108, 'text': ""doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3257,2021-02-12 13:30:03 UTC,3108,doing it and all this is doing is kind of like pre-initializing our loop that we are going to go through. So we're going to obviously loop through every batch within the train loader so we just add that in here and then there's this other parameter which I don't know if we... So let's leave it but essentially you can add a leave equals true in order to leave your progress bar in the same place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3138.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3329, 'published': '2021-02-12 13:30:03 UTC', 'start': 3138, 'text': ""place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3329,2021-02-12 13:30:03 UTC,3138,place with every epoch. Whereas at the moment with every epoch what it will do is create a new progress bar. We are going to create a new progress bar but if you don't do that and you want it to just stay in the same place you add leave equals true into this function here. So after that we need to go through each batch within our loop and the first thing that we need to do is set all of our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3168.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3398, 'published': '2021-02-12 13:30:03 UTC', 'start': 3168, 'text': ""our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3398,2021-02-12 13:30:03 UTC,3168,"our calculated gradients to zero. So with every iteration that we go through here or every batch at the end of it we are going to calculate gradients which tells the model in which direction to change the weights within the model and obviously when we go into the next iteration we don't want those gradients to still be there. So all we're doing here is reinitializing those gradients at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3198.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3453, 'published': '2021-02-12 13:30:03 UTC', 'start': 3198, 'text': ""at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3453,2021-02-12 13:30:03 UTC,3198,"at the start of every loop so we have a fresh set of gradients to work with every time and here we just want to pull in our data. So this is everything that is relevant that we're going to be feeding into the training process. So everything within our batch and then in here we have all of our different items. So we can actually see go here we want to add in all of these and we also want to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3242.1200000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3487, 'published': '2021-02-12 13:30:03 UTC', 'start': 3242, 'text': ""to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar. Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3487,2021-02-12 13:30:03 UTC,3242,"to move them across to the GPU in my case or whatever device you are working on. I would do that for the attention mass start positions and end positions. So these start and end positions are essentially the labels that are targets that we want our model to optimize for and the input IDs and attention mass are the inputs. So now we have those defined we just need to feed them into our model for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar. Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3311.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3533, 'published': '2021-02-12 13:30:03 UTC', 'start': 3311, 'text': ""for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar. Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3533,2021-02-12 13:30:03 UTC,3311,"for training and we will output the results of that training batch to the outputs variable. Our model, the IDs, we need the attention mask. We also want our start positions and end positions. Now from our training batch we want to extract the loss. And then we want to calculate loss for every parameter and this is for our gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar. Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here.",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3383.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3589, 'published': '2021-02-12 13:30:03 UTC', 'start': 3383, 'text': ""gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar. Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here. And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done."", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3589,2021-02-12 13:30:03 UTC,3383,gradient update and then we use the step method here to actually update those gradients. And then this final little bit here is purely for us to see this is our progress bar. So we call the loop we set the description which is going to be our epoch. And then it would probably be quite useful to also see the loss in there as well. We will set that as a post fix so it will appear after the progress bar. Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here. And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done.,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3438.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3633, 'published': '2021-02-12 13:30:03 UTC', 'start': 3438, 'text': ""Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here. And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done. Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3633,2021-02-12 13:30:03 UTC,3438,Okay and that should be everything. Okay so that looks pretty good. We have our model training and as I said this will take a little bit of time. So I will let that run. Okay so we have this non type error here and this is because within our end positions we normally expect integers but we're also getting some non values because the code that we used earlier where we're checking if end position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here. And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done. Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3476.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3664, 'published': '2021-02-12 13:30:03 UTC', 'start': 3476, 'text': ""position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here. And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done. Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3664,2021-02-12 13:30:03 UTC,3476,"position is non essentially wasn't good enough. So as a fix for that we'll just go back and we'll add like a while loop which will keep checking if it's non and every time it is non reduce the value that we are seeing by one. So go back up here and this is where the problem is coming from. So we're just going to change this to be a while loop and just initialize a essentially a counter here. And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done. Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3514.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3701, 'published': '2021-02-12 13:30:03 UTC', 'start': 3514, 'text': ""And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done. Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3701,2021-02-12 13:30:03 UTC,3514,"And we'll use this as our go back value and every time the end position is still non we'll just add one to go back and this should work. Just need to remember to rerun anything we need to rerun. Yeah. Okay and that looks like it solved the issue. So great we can just leave that training for a little while and I will see you when it's done. Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3572.6000000000004,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3734, 'published': '2021-02-12 13:30:03 UTC', 'start': 3572, 'text': ""Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3734,2021-02-12 13:30:03 UTC,3572,"Okay so the model is finished and we'll go ahead and just save it. So obviously we'll need to do that whenever actually doing this on any other projects. So I'm just going to call it the Silbert custom and it's super easy to save we just do save pre-trained and the model path. Now as well as this we might also want to save the tokenizer so we have everything in one place. So to do that we also just use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3614.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3768, 'published': '2021-02-12 13:30:03 UTC', 'start': 3614, 'text': ""use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3768,2021-02-12 13:30:03 UTC,3614,"use tokenizer and save pre-trained again. Okay so if we go back into our folder here, receive models and we have this Silbert custom and then in here we have all of the files we need to build our PyTorch model. It's a little bit different if we're using TensorFlow but the actual saving process is practically the same. So now we've finished training we want to switch it out of the training mode so we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3651.8399999999997,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3809, 'published': '2021-02-12 13:30:03 UTC', 'start': 3651, 'text': ""we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3809,2021-02-12 13:30:03 UTC,3651,we use a model eval. I'm just get all this information about our model as well we don't actually need any of that and just like before we want to create a data loader. So for that I'm gonna call it val loader and it's exactly the same code as before in fact it's probably better if we just copy and paste some of this. At least the loop. So what we're gonna do here is take the same loop and apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3687.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3852, 'published': '2021-02-12 13:30:03 UTC', 'start': 3687, 'text': ""apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3852,2021-02-12 13:30:03 UTC,3687,apply it as a validation run with our validation data. Just paste that there we'll initialize this data loader this time of course with the validation set we'll stick with the same batch size. Now this time we do want to keep a log of accuracy so we will keep that there and we also don't need to run most for epochs because we're not training this time we're just running through all the batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3723.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3888, 'published': '2021-02-12 13:30:03 UTC', 'start': 3723, 'text': ""batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3888,2021-02-12 13:30:03 UTC,3723,batches within our loop of validation data. So this is now a validation loader and we just loop through each of those batches so we don't need to do anything with the gradients here and because we're not doing anything to gradients we actually add this in to stop PyTorch from calculating any gradients. This will obviously save us a bit of time when we're processing all of this. And we put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3756.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3933, 'published': '2021-02-12 13:30:03 UTC', 'start': 3756, 'text': ""put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3933,2021-02-12 13:30:03 UTC,3756,put those in there. The outputs we do so want this but of course we don't need to be putting in the start and end positions so we can remove those and this time we want to pull out the start prediction and end prediction. So if we have a look at what our outputs look like before you see we have this model output and within here we're a few different tensors which each have a accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3790.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3984, 'published': '2021-02-12 13:30:03 UTC', 'start': 3790, 'text': ""accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,3984,2021-02-12 13:30:03 UTC,3790,accessible name. So the ones that we care about are startLogits and that will give us the logits for our start position which is essentially like a set of predictions where the highest value within that vector represents the token ID. So we can do that for both. You'll see we get these tensors. Now we only want the largest value in each one of these vectors here because that will give us the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3833.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 4024, 'published': '2021-02-12 13:30:03 UTC', 'start': 3833, 'text': ""the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,4024,2021-02-12 13:30:03 UTC,3833,the input ID. So to get that we use the argmax function and if we just use it by itself that will give us the maximum index within the whole thing. But we don't want that we want one for every single vector or every row and to do that we just set dim equal to 1 and there you go we get a full batch of outputs. So these are our starting positions and then we also want to do the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3870.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 4073, 'published': '2021-02-12 13:30:03 UTC', 'start': 3870, 'text': ""the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,4073,2021-02-12 13:30:03 UTC,3870,the same for our ending positions. So we just change start to end. So it's pretty easy. Now obviously we want to be doing this within our loop because this is only doing one batch and we need to do this for every single batch. So we're just going to assign them to a variable and there we have our predictions and all we need to do now is check for an exact match. So what I mean by exact match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end,How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3919.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 4120, 'published': '2021-02-12 13:30:03 UTC', 'start': 3919, 'text': ""match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,4120,2021-02-12 13:30:03 UTC,3919,"match is we want to see whether the start positions here which we can rename to the true values whether these are equal to the predicted values down here and to calculate that so let me just run this so we have one batch. That shouldn't take too long to process and we can just write the code out. So to check this we just use the double equal syntax here and this will just check for matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t3960.6800000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 4162, 'published': '2021-02-12 13:30:03 UTC', 'start': 3960, 'text': ""matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and divide by the length. And we get 63.6% for an exact match accuracy. So what I mean by exact match is say if we take a look at a few of these that do not match so we have a 75% match on the fourth batch although that won't be particularly useful because we can't see that batch right now. So let's just take the last batch because we have these values here. Now if we look at what start true is we"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,4162,2021-02-12 13:30:03 UTC,3960,"matches between two arrays. So we have the start predictions and the start true values so we'll check for those. Okay so if we just have a look at what we have here we get this array of true or false. So these ones don't look particularly good but that's fine we just want to calculate the accuracy here. So take the sum and we also want to divide that by the length. Okay so that will give us our accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and divide by the length. And we get 63.6% for an exact match accuracy. So what I mean by exact match is say if we take a look at a few of these that do not match so we have a 75% match on the fourth batch although that won't be particularly useful because we can't see that batch right now. So let's just take the last batch because we have these values here. Now if we look at what start true is we",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t4014.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 4190, 'published': '2021-02-12 13:30:03 UTC', 'start': 4014, 'text': ""accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and divide by the length. And we get 63.6% for an exact match accuracy. So what I mean by exact match is say if we take a look at a few of these that do not match so we have a 75% match on the fourth batch although that won't be particularly useful because we can't see that batch right now. So let's just take the last batch because we have these values here. Now if we look at what start true is we get these values. Then if we look at start pred we get this. So none of these match but a couple of them do get pretty close. So these final four all of these count as 0% on the exact match. But in reality if you look at what we predicted for every single one of them it's predicting just one token before so it's getting quite close but it's not an exact match so it scores"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,4190,2021-02-12 13:30:03 UTC,4014,"accuracy within the tensor and we just take it out using the item method but we also just need to include brackets around this because at the moment we're trying to take item of the length value. Okay and that gives us our very poor accuracy on this final batch. So we can take that and within here we want to append that to our accuracy list and then we also want to do that for the end the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and divide by the length. And we get 63.6% for an exact match accuracy. So what I mean by exact match is say if we take a look at a few of these that do not match so we have a 75% match on the fourth batch although that won't be particularly useful because we can't see that batch right now. So let's just take the last batch because we have these values here. Now if we look at what start true is we get these values. Then if we look at start pred we get this. So none of these match but a couple of them do get pretty close. So these final four all of these count as 0% on the exact match. But in reality if you look at what we predicted for every single one of them it's predicting just one token before so it's getting quite close but it's not an exact match so it scores",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
ZIRmXkHp0-c-t4047.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 4232, 'published': '2021-02-12 13:30:03 UTC', 'start': 4047, 'text': ""the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and divide by the length. And we get 63.6% for an exact match accuracy. So what I mean by exact match is say if we take a look at a few of these that do not match so we have a 75% match on the fourth batch although that won't be particularly useful because we can't see that batch right now. So let's just take the last batch because we have these values here. Now if we look at what start true is we get these values. Then if we look at start pred we get this. So none of these match but a couple of them do get pretty close. So these final four all of these count as 0% on the exact match. But in reality if you look at what we predicted for every single one of them it's predicting just one token before so it's getting quite close but it's not an exact match so it scores zero. So when you consider that with our 63.6% accuracy here that means that this model is actually probably doing pretty well. It's not perfect of course but it's doing pretty well. So overall that's I mean that's everything for this video we've gone all the way through this. If you do want to code for this I'm gonna make sure I keep a link to it in the description so check that out if you"", 'title': 'How to Build Custom Q&A Transformer Models in Python', 'url': 'https://youtu.be/ZIRmXkHp0-c'}",UCv83tO5cePwHMt1952IVVHw,4232,2021-02-12 13:30:03 UTC,4047,"the prediction as well. And we'll just let that run through and then we can calculate our accuracy from the end of that. And then we can have a quick look at our accuracy here. We can see fortunately it's not as bad as it first seemed. So we're getting a lot of 93%, 100%, 81% that's generally pretty good. So of course if we want to get the overall accuracy all we do is sum that and divide by the length. And we get 63.6% for an exact match accuracy. So what I mean by exact match is say if we take a look at a few of these that do not match so we have a 75% match on the fourth batch although that won't be particularly useful because we can't see that batch right now. So let's just take the last batch because we have these values here. Now if we look at what start true is we get these values. Then if we look at start pred we get this. So none of these match but a couple of them do get pretty close. So these final four all of these count as 0% on the exact match. But in reality if you look at what we predicted for every single one of them it's predicting just one token before so it's getting quite close but it's not an exact match so it scores zero. So when you consider that with our 63.6% accuracy here that means that this model is actually probably doing pretty well. It's not perfect of course but it's doing pretty well. So overall that's I mean that's everything for this video we've gone all the way through this. If you do want to code for this I'm gonna make sure I keep a link to it in the description so check that out if you",How to Build Custom Q&A Transformer Models in Python,https://youtu.be/ZIRmXkHp0-c
scJsty_DR3o-t12.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 124, 'published': '2021-02-19 15:00:21 UTC', 'start': 12, 'text': ""And then we're going to look at the Q&A pipeline. So we're going to look at actually loading a model in python using the transformers library. We're going to look at tokenization, how we load a tokenizer and what exactly tokenizer is actually doing. And then we're going to take a look at the pipeline class, which is essentially a wrapper made available by the Hugging Face Transformers library. And it basically just makes our job in terms of building a Q&A pipeline incredibly easy. So we're going to cover all those, it's going to be quite straightforward and quite simple. So let's just get straight into it. Okay so when we're doing question answering, we're essentially asking the model a question and passing a context, which is what you can see here for the model to use to answer that question. So you can see down here we have these three questions. So what organization is the IPCC a part of? And then the model will read through this and use its language modeling to figure out which organization the IPCC is part of, which is not inherently clear from reading this. We can see we've got IPCC here and is a scientific intergovernmental body under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,124,2021-02-19 15:00:21 UTC,12,"And then we're going to look at the Q&A pipeline. So we're going to look at actually loading a model in python using the transformers library. We're going to look at tokenization, how we load a tokenizer and what exactly tokenizer is actually doing. And then we're going to take a look at the pipeline class, which is essentially a wrapper made available by the Hugging Face Transformers library. And it basically just makes our job in terms of building a Q&A pipeline incredibly easy. So we're going to cover all those, it's going to be quite straightforward and quite simple. So let's just get straight into it. Okay so when we're doing question answering, we're essentially asking the model a question and passing a context, which is what you can see here for the model to use to answer that question. So you can see down here we have these three questions. So what organization is the IPCC a part of? And then the model will read through this and use its language modeling to figure out which organization the IPCC is part of, which is not inherently clear from reading this. We can see we've got IPCC here and is a scientific intergovernmental body under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t34.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 145, 'published': '2021-02-19 15:00:21 UTC', 'start': 34, 'text': ""And it basically just makes our job in terms of building a Q&A pipeline incredibly easy. So we're going to cover all those, it's going to be quite straightforward and quite simple. So let's just get straight into it. Okay so when we're doing question answering, we're essentially asking the model a question and passing a context, which is what you can see here for the model to use to answer that question. So you can see down here we have these three questions. So what organization is the IPCC a part of? And then the model will read through this and use its language modeling to figure out which organization the IPCC is part of, which is not inherently clear from reading this. We can see we've got IPCC here and is a scientific intergovernmental body under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website. And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers,"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,145,2021-02-19 15:00:21 UTC,34,"And it basically just makes our job in terms of building a Q&A pipeline incredibly easy. So we're going to cover all those, it's going to be quite straightforward and quite simple. So let's just get straight into it. Okay so when we're doing question answering, we're essentially asking the model a question and passing a context, which is what you can see here for the model to use to answer that question. So you can see down here we have these three questions. So what organization is the IPCC a part of? And then the model will read through this and use its language modeling to figure out which organization the IPCC is part of, which is not inherently clear from reading this. We can see we've got IPCC here and is a scientific intergovernmental body under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website. And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers,",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t62.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 169, 'published': '2021-02-19 15:00:21 UTC', 'start': 62, 'text': ""So you can see down here we have these three questions. So what organization is the IPCC a part of? And then the model will read through this and use its language modeling to figure out which organization the IPCC is part of, which is not inherently clear from reading this. We can see we've got IPCC here and is a scientific intergovernmental body under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website. And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,169,2021-02-19 15:00:21 UTC,62,"So you can see down here we have these three questions. So what organization is the IPCC a part of? And then the model will read through this and use its language modeling to figure out which organization the IPCC is part of, which is not inherently clear from reading this. We can see we've got IPCC here and is a scientific intergovernmental body under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website. And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t88.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 190, 'published': '2021-02-19 15:00:21 UTC', 'start': 88, 'text': ""under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website. And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,190,2021-02-19 15:00:21 UTC,88,"under the auspices of the United Nations. So clearly the IPCC is a part of the United Nations, but it's not clear. It's not definitively saying that in this, but once we've actually built this model, it will quite easily be able to answer each one of these questions without any issues. So the first thing we want to do is go over to the Hugging Face website. And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t114.47999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 205, 'published': '2021-02-19 15:00:21 UTC', 'start': 114, 'text': ""And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering,"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,205,2021-02-19 15:00:21 UTC,114,"And on the Hugging Face website, we just want to go over to the Models page. So it's here. Okay and on this Models page, the thing that we want to be looking at is this question and answering task. So here we have all these tasks because when you're working with transformers, they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering,",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t136.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 227, 'published': '2021-02-19 15:00:21 UTC', 'start': 136, 'text': ""they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering, and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,227,2021-02-19 15:00:21 UTC,136,"they can work with a lot of different things. Text summarization, text classification, generation, loads of different things. But what we want to do is question answering. So we click on here and this filters all of the models that are available to us just purely for question and answering. So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering, and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t158.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 245, 'published': '2021-02-19 15:00:21 UTC', 'start': 158, 'text': ""So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering, and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2. But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,245,2021-02-19 15:00:21 UTC,158,"So this is the sort of power of using the Hugging Face Transformers library. It already has all these pre-trained models that we can just download and start using. Now, when you want to go and apply these to specific use cases, you probably want to fine tune it, which means you want to train it a little bit more than what it is already trained. But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering, and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2. But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t181.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 264, 'published': '2021-02-19 15:00:21 UTC', 'start': 181, 'text': ""But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering, and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2. But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2. BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,264,2021-02-19 15:00:21 UTC,181,"But for actually getting used to how all of this works, all you need to do is download this model and start asking questions and understanding how everything is actually functioning. So obviously there's a lot of models here. We've got 262 models for question answering, and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2. But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2. BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t199.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 287, 'published': '2021-02-19 15:00:21 UTC', 'start': 199, 'text': ""and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2. But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2. BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words. The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,287,2021-02-19 15:00:21 UTC,199,"and there's new ones being added all the time. A few of the ones that I would recommend using are the DeepSets models. So here are the DeepSets models, eight of them for question answering. The one that we will be using is this BERT Base Case Squad 2. Another one that I would definitely recommend trying out is this Electra Base Squad 2. But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2. BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words. The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t219.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 309, 'published': '2021-02-19 15:00:21 UTC', 'start': 219, 'text': ""But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2. BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words. The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model. So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models,"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,309,2021-02-19 15:00:21 UTC,219,"But we will be sticking with BERT Base. Now, it's called DeepSet here because it's from the DeepSet AI company, and this model is being pulled directly from their GitHub repository. So DeepSet is actually the GitHub organization, and then this is the repository BERT Base Case Squad 2. BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words. The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model. So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models,",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t238.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 333, 'published': '2021-02-19 15:00:21 UTC', 'start': 238, 'text': ""BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words. The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model. So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models, not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,333,2021-02-19 15:00:21 UTC,238,"BERT is obviously the model BERT from Google AI. Base is the base version of BERT. So you can see here we have BERT large, that's just a large model. We're using the base model. Case just refers to the fact that this model will differentiate between uppercase and lowercase characters or words. The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model. So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models, not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t258.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 355, 'published': '2021-02-19 15:00:21 UTC', 'start': 258, 'text': ""The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model. So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models, not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case. And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,355,2021-02-19 15:00:21 UTC,258,"The alternative to this would be uncased here where there's no differentiation between uppercase and lowercase. And then Squad 2 refers to the question answering data set that this model has been trained on, which is the Squad 2 version. So we're going to take this model. So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models, not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case. And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t275.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 376, 'published': '2021-02-19 15:00:21 UTC', 'start': 275, 'text': ""So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models, not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case. And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,376,2021-02-19 15:00:21 UTC,275,"So you see DeepSet BERT Base Case Squad 2, and we are going to load it into here. And all we need to do to do that is from transformers. So this is the Hug & Face Transformers library. We want to import BERT for question and answer. So this is a specific class and using this class we can initialize a few different models, not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case. And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t305.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 398, 'published': '2021-02-19 15:00:21 UTC', 'start': 305, 'text': ""not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case. And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes. In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,398,2021-02-19 15:00:21 UTC,305,"not just this specific model. So you can see here we have this BERT base case. We can also initialize this BERT large uncased Roberta. And if there's a Distill BERT as well, we can also load those in. And what this does is it loads that BERT base case. And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes. In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t324.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 420, 'published': '2021-02-19 15:00:21 UTC', 'start': 324, 'text': ""And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes. In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method. And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,420,2021-02-19 15:00:21 UTC,324,"And what this does is it loads that specific model with its question and answering layer added on there as well. So this model has been trained with the extra layer specifically for question answering. And we need to use BERT for question answering to load that. Otherwise, if you are not using it with a specific use case and you're just wanting to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes. In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method. And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t347.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 435, 'published': '2021-02-19 15:00:21 UTC', 'start': 347, 'text': ""to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes. In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method. And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model. That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,435,2021-02-19 15:00:21 UTC,347,"to get the model itself, you can just use the auto model class like that. But we want it for question answering, so we load this one. Another thing to note is that we are using the PyTorch implementation of BERT here. So Transformers works by having both TensorFlow and PyTorch as alternative frameworks working behind the scenes. In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method. And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model. That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t369.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 459, 'published': '2021-02-19 15:00:21 UTC', 'start': 369, 'text': ""In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method. And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model. That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data. So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,459,2021-02-19 15:00:21 UTC,369,"In this case, we're using PyTorch. If you want to switch over to TensorFlow, all you do is add TF in front of that class. So that is our model. And to actually load that in, all we do is copy this and we use the from pre-trained method. And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model. That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data. So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t392.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 480, 'published': '2021-02-19 15:00:21 UTC', 'start': 392, 'text': ""And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model. That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data. So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,480,2021-02-19 15:00:21 UTC,392,"And then this is where the model name from over here comes into play. So we've got deep set BERT base case squad 2. And we just enter that in there. And we just enter that in there. Okay, and with that, we've actually just loaded the model. That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data. So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t417.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 496, 'published': '2021-02-19 15:00:21 UTC', 'start': 417, 'text': ""That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data. So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class. Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,496,2021-02-19 15:00:21 UTC,417,"That's all we had to do. Of course, there are a few other steps. This is just a model. But there are a few steps before we actually get the data to the model. So we need to actually process the data. So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class. Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t430.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 524, 'published': '2021-02-19 15:00:21 UTC', 'start': 430, 'text': ""So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class. Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there. So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,524,2021-02-19 15:00:21 UTC,430,"So we have this context here and this is just a string. BERT doesn't understand strings. BERT understands an array of integers where each integer represents a token ID. And that token ID is very specific to BERT. And each one is unique and represents a specific word or piece of syntax, punctuation, or so on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class. Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there. So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t451.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 542, 'published': '2021-02-19 15:00:21 UTC', 'start': 451, 'text': ""on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class. Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there. So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method. And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,542,2021-02-19 15:00:21 UTC,451,"on. So we need to convert this string into that specific BERT ready format. And to do that, we need to use a tokenizer. So again, we're going to go from transformers. I'm going to import the auto tokenizer class. Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there. So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method. And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t473.46,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 570, 'published': '2021-02-19 15:00:21 UTC', 'start': 473, 'text': ""Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there. So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method. And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions. So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,570,2021-02-19 15:00:21 UTC,473,"Here we can use, for example, the BERT tokenizer. But for this, we don't need anything specific. It's quite generic. It will just load all of those mappings from the string or the word into the tokens. There's no real issue there. So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method. And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions. So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t491.22,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 592, 'published': '2021-02-19 15:00:21 UTC', 'start': 491, 'text': ""So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method. And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions. So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true. So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,592,2021-02-19 15:00:21 UTC,491,"So we import our auto tokenizer. And to initialize it, we just see this. It's practically the same syntax as what we used for the first one. The same syntax as what we used before. We use this fromPretrain method. And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions. So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true. So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t513.62,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 622, 'published': '2021-02-19 15:00:21 UTC', 'start': 513, 'text': ""And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions. So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true. So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token. And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there,"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,622,2021-02-19 15:00:21 UTC,513,"And then again, we're using the same model. OK. And then with this, we can actually tokenize our data. So all we need to do is write tokenizer and code. And then let's just pass in one of these questions. So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true. So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token. And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there,",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t537.54,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 642, 'published': '2021-02-19 15:00:21 UTC', 'start': 537, 'text': ""So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true. So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token. And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there, we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,642,2021-02-19 15:00:21 UTC,537,"So we'll pass in the first one. So questions. And the first question there. And two variables that we will need to add in here are the truncation, which we will set to true. And the padding, which we also set to true. So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token. And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there, we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t557.6199999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 674, 'published': '2021-02-19 15:00:21 UTC', 'start': 557, 'text': ""So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token. And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there, we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this. And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,674,2021-02-19 15:00:21 UTC,557,"So when we are setting up these models and the data going into them, Bert in particular will expect 512 tokens with every input. Now here, when we look at this, we can see there's probably one. So each one of these words is most likely to be truncation. One, so each one of these words is most likely to be a token. And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there, we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this. And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t586.42,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 695, 'published': '2021-02-19 15:00:21 UTC', 'start': 586, 'text': ""And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there, we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this. And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence. So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,695,2021-02-19 15:00:21 UTC,586,"And then this question mark at the end will also be a token. So we have around 10 tokens in there. Now, because we have padding, this will add a set of padding tokens onto the end of it to bring that total number of tokens up to 512. Now, alternatively, say if we had 600 tokens in there, we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this. And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence. So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t610.66,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 723, 'published': '2021-02-19 15:00:21 UTC', 'start': 610, 'text': ""we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this. And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence. So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well. And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,723,2021-02-19 15:00:21 UTC,610,"we would be relying on the truncation to cut the final 88 tokens to make it a total of 512. And that's why we need those two arguments in there. So let's see what we get from this. You can see here that we have our tokenized input. So Bert will be able to read and understand this. And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence. So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well. And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t632.02,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 748, 'published': '2021-02-19 15:00:21 UTC', 'start': 632, 'text': ""And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence. So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well. And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator. So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,748,2021-02-19 15:00:21 UTC,632,"And essentially what we have, so this 1327 is the equivalent to what? This 2369 is equivalent to organization and so on and so on. Now, what you might not see here is why we have this 101. So 101 for Bert actually refers to a special token, which looks like this. And this just signifies the start of any sequence. So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well. And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator. So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t661.3,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 775, 'published': '2021-02-19 15:00:21 UTC', 'start': 661, 'text': ""So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well. And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator. So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs. And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,775,2021-02-19 15:00:21 UTC,661,"So if we were to just take this, we can see that, OK, we get the same again. We get this 101, which is the start sequence. Then we get the start sequence token again, because that's all we've put into here. And the Bert, the tokenizer is reading that and converting into the 101. And then we also get this final special token as well. And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator. So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs. And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t687.54,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 801, 'published': '2021-02-19 15:00:21 UTC', 'start': 687, 'text': ""And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator. So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs. And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like. We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,801,2021-02-19 15:00:21 UTC,687,"And we can also see that it's here. So this is another special token which signifies the end of a sequence, or it signifies a separator point. So if we write this out, you see here that the separator is 102. And what I mean by it signifies a separation point or a separator. So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs. And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like. We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t711.6199999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 827, 'published': '2021-02-19 15:00:21 UTC', 'start': 711, 'text': ""So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs. And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like. We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,827,2021-02-19 15:00:21 UTC,711,"So when we feed this context and this question into our Bert, model Bert will expect it to be within the format something like this. So we have the start sequence token. Then we will have our context tokens. So this will just be a list of integers, which are the token IDs. And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like. We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t736.8199999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 844, 'published': '2021-02-19 15:00:21 UTC', 'start': 736, 'text': ""And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like. We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,844,2021-02-19 15:00:21 UTC,736,"And then what we will see is a separator token here, followed by our question. Which again, after this is followed by a separator token. And again, after this, we get a set of padding tokens, which look like this. And that will just take us up to the 512 token amount. And that's how the data going into Bert will look like. We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t767.86,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 863, 'published': '2021-02-19 15:00:21 UTC', 'start': 767, 'text': ""We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,863,2021-02-19 15:00:21 UTC,767,"We have that start sequence, we have the context, we have a separate, we have a question, we have a separate and then we have padding. It's always going to look like that when it's going into a Bert Q&A model. So if we just remove that and this here. And what we want to do now is actually set up this tokenizer and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t790.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 882, 'published': '2021-02-19 15:00:21 UTC', 'start': 790, 'text': ""and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,882,2021-02-19 15:00:21 UTC,790,"and our model into a pipeline, into a Q&A pipeline. So again, we get this pipeline from the Transformers library. So we come down here, do from Transformers, import pipeline. And now what we want to do is just initialize a pipeline object. So to do that, we just write pipeline. And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t818.3399999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 902, 'published': '2021-02-19 15:00:21 UTC', 'start': 818, 'text': ""And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer. And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,902,2021-02-19 15:00:21 UTC,818,"And then in here, what we need to add is a model type. So obviously, you can see up here, we have all of these different tasks. So summarization, text generation and so on. The Transformers library needs to understand, or this pipeline object needs to understand which one of those pipelines or functions we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer. And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t838.9000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 918, 'published': '2021-02-19 15:00:21 UTC', 'start': 838, 'text': ""we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer. And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,918,2021-02-19 15:00:21 UTC,838,"we are intending to use. So to tell it that we want to do question answering, we just write question answering. And that basically sets the wrapper of the pipeline to handle question answering formats. So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer. And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t854.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 940, 'published': '2021-02-19 15:00:21 UTC', 'start': 854, 'text': ""So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer. And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well. And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,940,2021-02-19 15:00:21 UTC,854,"So we'll see our input and for our input, we will be passing a context and a question. So we'll see that it will convert into the right structure that we need for question answering, which is the CLS context separator, question separator and padding. It will convert into that, feed it into our tokenizer. And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well. And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t873.54,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 966, 'published': '2021-02-19 15:00:21 UTC', 'start': 873, 'text': ""And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well. And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context. So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,966,2021-02-19 15:00:21 UTC,873,"And the output of that tokenizer, our token IDs will be fed into BERT. BERT will return us a span start and span end, which is essentially two numbers, which signify the start position and end position of our answer within the context. And this pipeline will take those two numbers and apply them to our context to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well. And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context. So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t894.34,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 988, 'published': '2021-02-19 15:00:21 UTC', 'start': 894, 'text': ""to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well. And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context. So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here. Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,988,2021-02-19 15:00:21 UTC,894,"to get the text, which is our answer from that. So it's essentially just a little wrapper and it adds a few functionalities so that we don't have to worry about converting all of these things. So now we just need to pass in our model. And the tokenizer as well. And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context. So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here. Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t914.98,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1019, 'published': '2021-02-19 15:00:21 UTC', 'start': 914, 'text': ""And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context. So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here. Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations. So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1019,2021-02-19 15:00:21 UTC,914,"And it's as simple as that. That's our pipeline set up. So if we want to use that now, all we need to do is write NLP. And then here we pass a dictionary. And this dictionary, like I said before, needs to contain our question and context. So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here. Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations. So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t934.1800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1046, 'published': '2021-02-19 15:00:21 UTC', 'start': 934, 'text': ""So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here. Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations. So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character. If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1046,2021-02-19 15:00:21 UTC,934,"So the question. And for this, we will just pass the first of our questions up here again. So this questions at the index zero. And then we also pass our context, which is inside the context variable up here. Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations. So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character. If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t957.38,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1076, 'published': '2021-02-19 15:00:21 UTC', 'start': 957, 'text': ""Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations. So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character. If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before. And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1076,2021-02-19 15:00:21 UTC,957,"Okay. And this will output a dictionary containing the, well, we can see the score of the answer. So that is the model's confidence that this is actually an answer. Like I said before, the start index and end index and what those start index and end index map to, which is United Nations. So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character. If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before. And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t981.3800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1103, 'published': '2021-02-19 15:00:21 UTC', 'start': 981, 'text': ""So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character. If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before. And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988 by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1103,2021-02-19 15:00:21 UTC,981,"So our question was, what is the answer? And we got United Nations, which is correct. So let me just show you what I mean with this start and end. So if we do 118 here, we get the first letter of our answer because we are going through here and it is pulling out this specific character. If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before. And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988 by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t1007.3000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1131, 'published': '2021-02-19 15:00:21 UTC', 'start': 1007, 'text': ""If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before. And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988 by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one. So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC"", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1131,2021-02-19 15:00:21 UTC,1007,"If we then add the first letter of our answer, if we then add this and go all the way up to our end, which is at 132, we get the full set because what we're doing here is pulling out all the characters from you or character 118 all the way up to character 132, which is actually this comma here. But obviously with Python list indexing, we get the character before. And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988 by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one. So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t1039.62,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1154, 'published': '2021-02-19 15:00:21 UTC', 'start': 1039, 'text': ""And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988 by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one. So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC is to stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. So again, we are getting the answer, stabilize greenhouse gas concentrations. So our model has gone through each one of those questions and successfully answered them. And all we've done is written a few lines of code."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1154,2021-02-19 15:00:21 UTC,1039,"And that gives us United Nations, which is our answer. So let's ask another question. We have what UN organizations established the IPCC? And we get this WMO and United Nations Environment Program, UNEP. So if we go in here, we can see it was first established in 1988 by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one. So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC is to stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. So again, we are getting the answer, stabilize greenhouse gas concentrations. So our model has gone through each one of those questions and successfully answered them. And all we've done is written a few lines of code.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t1066.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1174, 'published': '2021-02-19 15:00:21 UTC', 'start': 1066, 'text': ""by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one. So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC is to stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. So again, we are getting the answer, stabilize greenhouse gas concentrations. So our model has gone through each one of those questions and successfully answered them. And all we've done is written a few lines of code. And this is without us fine tuning them at all. Now, when you do go and apply these to your own problems, sometimes you won't need to do any fine tuning and the model as is will be more than enough. But a lot of the time you will need to fine tune it. And in that case, the answer is yes."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1174,2021-02-19 15:00:21 UTC,1066,"by two United Nations organizations, the World Meteorological Organization, WMO, and the United Nations Environment Program, UNEP. So here we have two organizations and it is only actually pulling out one of those. So I think the reason for that is all that is reading is WMO and United Nations Environment Program. So it is pulling out those two organizations in the end, just not the full name of the first one. So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC is to stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. So again, we are getting the answer, stabilize greenhouse gas concentrations. So our model has gone through each one of those questions and successfully answered them. And all we've done is written a few lines of code. And this is without us fine tuning them at all. Now, when you do go and apply these to your own problems, sometimes you won't need to do any fine tuning and the model as is will be more than enough. But a lot of the time you will need to fine tune it. And in that case, the answer is yes.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
scJsty_DR3o-t1097.8600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1188, 'published': '2021-02-19 15:00:21 UTC', 'start': 1097, 'text': ""So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC is to stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. So again, we are getting the answer, stabilize greenhouse gas concentrations. So our model has gone through each one of those questions and successfully answered them. And all we've done is written a few lines of code. And this is without us fine tuning them at all. Now, when you do go and apply these to your own problems, sometimes you won't need to do any fine tuning and the model as is will be more than enough. But a lot of the time you will need to fine tune it. And in that case, the answer is yes. You fine tune it and in that case, there are a few extra steps. But for this introduction, that's everything I wanted to cover there. In terms of fine tuning, I have covered that in another video. So I will put a link to that in the description. But that's everything for this video."", 'title': 'How to Build Q&A Models in Python (Transformers)', 'url': 'https://youtu.be/scJsty_DR3o'}",UCv83tO5cePwHMt1952IVVHw,1188,2021-02-19 15:00:21 UTC,1097,"So it's still a pretty good result. And let's go down to this final question. So what does the UN want to stabilize? And here we're getting the answer of greenhouse gas concentrations in the atmosphere. So if we go down here, we can see the ultimate objective of the UNFCCC is to stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system. So again, we are getting the answer, stabilize greenhouse gas concentrations. So our model has gone through each one of those questions and successfully answered them. And all we've done is written a few lines of code. And this is without us fine tuning them at all. Now, when you do go and apply these to your own problems, sometimes you won't need to do any fine tuning and the model as is will be more than enough. But a lot of the time you will need to fine tune it. And in that case, the answer is yes. You fine tune it and in that case, there are a few extra steps. But for this introduction, that's everything I wanted to cover there. In terms of fine tuning, I have covered that in another video. So I will put a link to that in the description. But that's everything for this video.",How to Build Q&A Models in Python (Transformers),https://youtu.be/scJsty_DR3o
YvVQgvAz9dY-t6.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 236, 'published': '2020-11-24 14:22:46 UTC', 'start': 6, 'text': ""Now this is actually incredibly easy to do and we can build this entire model including the imports, the tokenizer model and outputting our generated text with just seven lines of code which is pretty insane. Now the only libraries we need for this are PyTorch and Transformers. So we'll go ahead and import them now. Now all we need from the Transformers library are the GPT-2 LMHead model and GPT-2 tokenizer. So we can initialize both of those as well now and both will be from pre-trained. So now we have initialized our tokenizer and model. We just need a sequence of text to feed in and get our model going. So I've taken a snippet of text from the Wikipedia page of Winston Churchill which is here and it's just a small little snippet talking about when he took office during World War II. Now from this I've tested it briefly and it seems to give some pretty interesting results. So we will go ahead use this. All we need to do is tokenize it. Now all we're doing here is taking each of these words, splitting them into tokens. So that would be a list where each word is its own item. So he began his premiership. Each one of those would be a separate value within that list. Once we have them in that tokenized format our tokenizer will then convert them into numerical IDs which map to a word vector that's been trained to work with GPT-2. Now because we're using PyTorch we just need to remember to return PT tensors here. So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and"", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,236,2020-11-24 14:22:46 UTC,6,"Now this is actually incredibly easy to do and we can build this entire model including the imports, the tokenizer model and outputting our generated text with just seven lines of code which is pretty insane. Now the only libraries we need for this are PyTorch and Transformers. So we'll go ahead and import them now. Now all we need from the Transformers library are the GPT-2 LMHead model and GPT-2 tokenizer. So we can initialize both of those as well now and both will be from pre-trained. So now we have initialized our tokenizer and model. We just need a sequence of text to feed in and get our model going. So I've taken a snippet of text from the Wikipedia page of Winston Churchill which is here and it's just a small little snippet talking about when he took office during World War II. Now from this I've tested it briefly and it seems to give some pretty interesting results. So we will go ahead use this. All we need to do is tokenize it. Now all we're doing here is taking each of these words, splitting them into tokens. So that would be a list where each word is its own item. So he began his premiership. Each one of those would be a separate value within that list. Once we have them in that tokenized format our tokenizer will then convert them into numerical IDs which map to a word vector that's been trained to work with GPT-2. Now because we're using PyTorch we just need to remember to return PT tensors here. So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and",Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
YvVQgvAz9dY-t63.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 277, 'published': '2020-11-24 14:22:46 UTC', 'start': 63, 'text': ""pre-trained. So now we have initialized our tokenizer and model. We just need a sequence of text to feed in and get our model going. So I've taken a snippet of text from the Wikipedia page of Winston Churchill which is here and it's just a small little snippet talking about when he took office during World War II. Now from this I've tested it briefly and it seems to give some pretty interesting results. So we will go ahead use this. All we need to do is tokenize it. Now all we're doing here is taking each of these words, splitting them into tokens. So that would be a list where each word is its own item. So he began his premiership. Each one of those would be a separate value within that list. Once we have them in that tokenized format our tokenizer will then convert them into numerical IDs which map to a word vector that's been trained to work with GPT-2. Now because we're using PyTorch we just need to remember to return PT tensors here. So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going"", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,277,2020-11-24 14:22:46 UTC,63,"pre-trained. So now we have initialized our tokenizer and model. We just need a sequence of text to feed in and get our model going. So I've taken a snippet of text from the Wikipedia page of Winston Churchill which is here and it's just a small little snippet talking about when he took office during World War II. Now from this I've tested it briefly and it seems to give some pretty interesting results. So we will go ahead use this. All we need to do is tokenize it. Now all we're doing here is taking each of these words, splitting them into tokens. So that would be a list where each word is its own item. So he began his premiership. Each one of those would be a separate value within that list. Once we have them in that tokenized format our tokenizer will then convert them into numerical IDs which map to a word vector that's been trained to work with GPT-2. Now because we're using PyTorch we just need to remember to return PT tensors here. So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going",Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
YvVQgvAz9dY-t126.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 317, 'published': '2020-11-24 14:22:46 UTC', 'start': 126, 'text': ""Now all we're doing here is taking each of these words, splitting them into tokens. So that would be a list where each word is its own item. So he began his premiership. Each one of those would be a separate value within that list. Once we have them in that tokenized format our tokenizer will then convert them into numerical IDs which map to a word vector that's been trained to work with GPT-2. Now because we're using PyTorch we just need to remember to return PT tensors here. So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts."", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,317,2020-11-24 14:22:46 UTC,126,"Now all we're doing here is taking each of these words, splitting them into tokens. So that would be a list where each word is its own item. So he began his premiership. Each one of those would be a separate value within that list. Once we have them in that tokenized format our tokenizer will then convert them into numerical IDs which map to a word vector that's been trained to work with GPT-2. Now because we're using PyTorch we just need to remember to return PT tensors here. So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts.",Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
YvVQgvAz9dY-t169.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 349, 'published': '2020-11-24 14:22:46 UTC', 'start': 169, 'text': ""So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts. So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output."", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,349,2020-11-24 14:22:46 UTC,169,"So now we have our inputs we just need to feed them into our model. So we can do that using model.generate. We add our inputs. Now we also need to tell PyTorch how long we want our generate sequence to be. So all we do for that is add a max length. And this will act as the cut off point. Anything longer than this will simply be cut off. And now here we are just generating our output. We also need to pass this into the outputs variable here so that we can actually read from it and decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts. So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output.",Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
YvVQgvAz9dY-t223.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 388, 'published': '2020-11-24 14:22:46 UTC', 'start': 223, 'text': ""decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts. So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output. Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from"", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,388,2020-11-24 14:22:46 UTC,223,"decode it. So to decode our output IDs because it will output numerical IDs representing words just like we fed into it we need to use the tokenizer decode method. And our output IDs are in the zero index of the outputs object. And we also want to skip any special tokens. So this would be stuff like end of sequence tokens, padding tokens, unknown word tokens and so on. And then we can print the text. Now we can see here that it's basically just going over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts. So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output. Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from",Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
YvVQgvAz9dY-t265.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 428, 'published': '2020-11-24 14:22:46 UTC', 'start': 265, 'text': ""over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts. So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output. Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from the original model. Now you can toy around with this and see what produces more interesting results. Generally higher temperature will create more creative outputs. And the other parameter we can also use is the top k parameter. Now top k limits the sample tokens to the top rated tokens that the model is predicting. So we can add 50 for example and this will alter our output. Generally I've found top k tends to make the text a little more coherent and I would assume this is"", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,428,2020-11-24 14:22:46 UTC,265,over and over again saying the same things which is not really what we want. So this is a pretty common problem and all we need to do to fix this is add a new output. So we can just add a new argument to our generator method here. So we simply do sample equals true. And then we can rerun this. And this looks pretty good now. So we can add more randomness and restrict the number of possible texts. So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output. Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from the original model. Now you can toy around with this and see what produces more interesting results. Generally higher temperature will create more creative outputs. And the other parameter we can also use is the top k parameter. Now top k limits the sample tokens to the top rated tokens that the model is predicting. So we can add 50 for example and this will alter our output. Generally I've found top k tends to make the text a little more coherent and I would assume this is,Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
YvVQgvAz9dY-t300.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 464, 'published': '2020-11-24 14:22:46 UTC', 'start': 300, 'text': ""So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output. Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from the original model. Now you can toy around with this and see what produces more interesting results. Generally higher temperature will create more creative outputs. And the other parameter we can also use is the top k parameter. Now top k limits the sample tokens to the top rated tokens that the model is predicting. So we can add 50 for example and this will alter our output. Generally I've found top k tends to make the text a little more coherent and I would assume this is because it is sticking within a smaller space of possible tokens or words that you can output. So now here we can see pretty understandable logical text again. And we can see here it mentions Lord a lot which makes sense because this is Britain. So if we put the temperature back up to one we should get a slightly more random output again. And then here we can see that there's a little more weird text coming in. So we have here that the first Australian Prime Minister"", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,464,2020-11-24 14:22:46 UTC,300,So we can add more randomness and restrict the number of possible tokens for the model to use using the temperature and top k parameters respectively. Now temperature acts as the amount of randomness input into the model. So a high temperature above one will create more random tokens than the default. Anything below one makes the model less random. So say if we put a super high number like five we will probably get a pretty weird output. Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from the original model. Now you can toy around with this and see what produces more interesting results. Generally higher temperature will create more creative outputs. And the other parameter we can also use is the top k parameter. Now top k limits the sample tokens to the top rated tokens that the model is predicting. So we can add 50 for example and this will alter our output. Generally I've found top k tends to make the text a little more coherent and I would assume this is because it is sticking within a smaller space of possible tokens or words that you can output. So now here we can see pretty understandable logical text again. And we can see here it mentions Lord a lot which makes sense because this is Britain. So if we put the temperature back up to one we should get a slightly more random output again. And then here we can see that there's a little more weird text coming in. So we have here that the first Australian Prime Minister,Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
YvVQgvAz9dY-t337.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 505, 'published': '2020-11-24 14:22:46 UTC', 'start': 337, 'text': ""Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from the original model. Now you can toy around with this and see what produces more interesting results. Generally higher temperature will create more creative outputs. And the other parameter we can also use is the top k parameter. Now top k limits the sample tokens to the top rated tokens that the model is predicting. So we can add 50 for example and this will alter our output. Generally I've found top k tends to make the text a little more coherent and I would assume this is because it is sticking within a smaller space of possible tokens or words that you can output. So now here we can see pretty understandable logical text again. And we can see here it mentions Lord a lot which makes sense because this is Britain. So if we put the temperature back up to one we should get a slightly more random output again. And then here we can see that there's a little more weird text coming in. So we have here that the first Australian Prime Minister was sacked by a Labour Minister which obviously is a little bit strange. But it just shows that we can add more randomness or we can try and restrict our model to become more coherent. And we can do this super easily using the generate parameters. So with just a few more lines of code we built our model up and running and actually generating text incredibly easily. So I hope this has been insightful and useful. If you have any questions or suggestions please just"", 'title': ""Language Generation with OpenAI's GPT-2 in Python"", 'url': 'https://youtu.be/YvVQgvAz9dY'}",UCv83tO5cePwHMt1952IVVHw,505,2020-11-24 14:22:46 UTC,337,Okay so we can see here initially skimming over it doesn't look too bad but then when you start reading it it's practically impossible to follow. There's no structure and there's just a couple of random words in there that are just completely irrelevant. Now we can also see here as an end bracket and there's starting bracket that pairs with it. And generally just some really weird syntax. So we turn the temperature down to 0.7 and we will actually decrease the randomness from the original model. Now you can toy around with this and see what produces more interesting results. Generally higher temperature will create more creative outputs. And the other parameter we can also use is the top k parameter. Now top k limits the sample tokens to the top rated tokens that the model is predicting. So we can add 50 for example and this will alter our output. Generally I've found top k tends to make the text a little more coherent and I would assume this is because it is sticking within a smaller space of possible tokens or words that you can output. So now here we can see pretty understandable logical text again. And we can see here it mentions Lord a lot which makes sense because this is Britain. So if we put the temperature back up to one we should get a slightly more random output again. And then here we can see that there's a little more weird text coming in. So we have here that the first Australian Prime Minister was sacked by a Labour Minister which obviously is a little bit strange. But it just shows that we can add more randomness or we can try and restrict our model to become more coherent. And we can do this super easily using the generate parameters. So with just a few more lines of code we built our model up and running and actually generating text incredibly easily. So I hope this has been insightful and useful. If you have any questions or suggestions please just,Language Generation with OpenAI's GPT-2 in Python,https://youtu.be/YvVQgvAz9dY
4Jmq28RQ3hU-t21.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 160, 'published': '2021-04-09 15:22:50 UTC', 'start': 21, 'text': ""thinking through this morning and I thought okay today we're gonna start it. So what you can see on the screen right now is the GitHub repo for this project which is completely empty almost. There's a read me a gitignore and this one architecture drawing that I sell literally you can see four minutes ago. So I'm gonna show you that and I'm going to explain what we're actually going to go through and I think we're gonna cover a lot of different things. So I think this is I think that is one of the reasons why I think this is such a cool project. So at the moment this is like the basic architecture of what we would need to build a Q&A model and the end goal is to have is to have a front end which looks kind of like this. So we'll have like a search bar here and we'll have some visual up here. It's gonna be a little bit better than the six man. I'm gonna show you what I already have and maybe we can use that or maybe we'll do something different I don't know. And what we're gonna be able to do is ask a question here and we're going to be able to answer the question based on a Soic philosophy books. So I haven't really read any of these I've read like little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca."", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,160,2021-04-09 15:22:50 UTC,21,thinking through this morning and I thought okay today we're gonna start it. So what you can see on the screen right now is the GitHub repo for this project which is completely empty almost. There's a read me a gitignore and this one architecture drawing that I sell literally you can see four minutes ago. So I'm gonna show you that and I'm going to explain what we're actually going to go through and I think we're gonna cover a lot of different things. So I think this is I think that is one of the reasons why I think this is such a cool project. So at the moment this is like the basic architecture of what we would need to build a Q&A model and the end goal is to have is to have a front end which looks kind of like this. So we'll have like a search bar here and we'll have some visual up here. It's gonna be a little bit better than the six man. I'm gonna show you what I already have and maybe we can use that or maybe we'll do something different I don't know. And what we're gonna be able to do is ask a question here and we're going to be able to answer the question based on a Soic philosophy books. So I haven't really read any of these I've read like little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca.,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t48.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 193, 'published': '2021-04-09 15:22:50 UTC', 'start': 48, 'text': ""go through and I think we're gonna cover a lot of different things. So I think this is I think that is one of the reasons why I think this is such a cool project. So at the moment this is like the basic architecture of what we would need to build a Q&A model and the end goal is to have is to have a front end which looks kind of like this. So we'll have like a search bar here and we'll have some visual up here. It's gonna be a little bit better than the six man. I'm gonna show you what I already have and maybe we can use that or maybe we'll do something different I don't know. And what we're gonna be able to do is ask a question here and we're going to be able to answer the question based on a Soic philosophy books. So I haven't really read any of these I've read like little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca. And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,193,2021-04-09 15:22:50 UTC,48,go through and I think we're gonna cover a lot of different things. So I think this is I think that is one of the reasons why I think this is such a cool project. So at the moment this is like the basic architecture of what we would need to build a Q&A model and the end goal is to have is to have a front end which looks kind of like this. So we'll have like a search bar here and we'll have some visual up here. It's gonna be a little bit better than the six man. I'm gonna show you what I already have and maybe we can use that or maybe we'll do something different I don't know. And what we're gonna be able to do is ask a question here and we're going to be able to answer the question based on a Soic philosophy books. So I haven't really read any of these I've read like little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca. And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t85.03999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 232, 'published': '2021-04-09 15:22:50 UTC', 'start': 85, 'text': ""have some visual up here. It's gonna be a little bit better than the six man. I'm gonna show you what I already have and maybe we can use that or maybe we'll do something different I don't know. And what we're gonna be able to do is ask a question here and we're going to be able to answer the question based on a Soic philosophy books. So I haven't really read any of these I've read like little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca. And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,232,2021-04-09 15:22:50 UTC,85,have some visual up here. It's gonna be a little bit better than the six man. I'm gonna show you what I already have and maybe we can use that or maybe we'll do something different I don't know. And what we're gonna be able to do is ask a question here and we're going to be able to answer the question based on a Soic philosophy books. So I haven't really read any of these I've read like little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca. And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t111.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 273, 'published': '2021-04-09 15:22:50 UTC', 'start': 111, 'text': ""little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca. And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,273,2021-04-09 15:22:50 UTC,111,little bits but they're pretty interesting and I think quite unique. So as far as I know there's definitely not anything like this out there at the moment where you ask a question and you get the answer back from some ancient Soic philosophy book. And there's only really two books that I've thought of so far which is Meditations by Marcus Aurelius and Letters from a Stoic by Seneca. And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t146.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 309, 'published': '2021-04-09 15:22:50 UTC', 'start': 146, 'text': ""And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,309,2021-04-09 15:22:50 UTC,146,And the good thing with both of these is that we can find both of them online for free so we can use Python requests to get these. So I'll just kind of put a little list here what I think we're gonna need. So the first one is actually extracting and downloading this data so we're using requests for that. And then once we have actually got that data we need to pre-process it. And when we're pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t180.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 339, 'published': '2021-04-09 15:22:50 UTC', 'start': 180, 'text': ""pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,339,2021-04-09 15:22:50 UTC,180,pre-processing it I think that will just be a case of using regex more than anything else but I'm not sure yet so let's see. So after we pre-process it and that's when we get into this stuff over here so this whole sort of stack that you can see without the API. So this is a typical it's called a reader or retriever reader and what we do is we use this up here this is our database it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t218.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 377, 'published': '2021-04-09 15:22:50 UTC', 'start': 218, 'text': ""it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,377,2021-04-09 15:22:50 UTC,218,it's a document store elasticsearch document store and what we're gonna do is feed all of these. Sorry it's getting a little bit messy with the color so let me change it. So what we're gonna do is take these and we're gonna feed them into our document store. And once we have that what we want to do is build this retriever reader stack and it will allow us to query the retriever down here and what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t259.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 419, 'published': '2021-04-09 15:22:50 UTC', 'start': 259, 'text': ""what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,419,2021-04-09 15:22:50 UTC,259,what the retriever will do is send that query to elasticsearch here which is what you can see happening there and returning from that we'll get so many different contexts. So all of the texts from meditations and letters from Stoic will split them up by maybe paragraph and store them in here and what these contexts will be are the most relevant paragraphs. And once we've done that this retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t294.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 458, 'published': '2021-04-09 15:22:50 UTC', 'start': 294, 'text': ""retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,458,2021-04-09 15:22:50 UTC,294,retriever will pass on the context to our reader model down here and what the reader model will do is say it's given a long sentence like this or paragraph it will say okay the actual answer that you want is actually only these three words here and it will return those three words and what we want to do is return those three words in our answer back to our API but alongside the answer we're also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t325.84000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 495, 'published': '2021-04-09 15:22:50 UTC', 'start': 325, 'text': ""also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,495,2021-04-09 15:22:50 UTC,325,also going to include the the full context here as well. So we get a few things back and I think that that's like going to be more machine learning side of it but obviously we need to support the machine learning side and I mean the very first part of that that you can obviously see here is the API so let me so let me write down so we have the ML part and for that we're going to use in something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t357.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 521, 'published': '2021-04-09 15:22:50 UTC', 'start': 357, 'text': ""something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a lot that I think we would have to do to build this and I think it'd be pretty interesting so that is what we're going to be covering in sort of the next few videos and the one or the final little thing okay so over here we have the Marcus Aurelius Superman and I thought maybe something like this would be cool I don't know this is something I drew ages ago and this is this sorry so this"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,521,2021-04-09 15:22:50 UTC,357,something called Haysec and once we get out of that part we move on so we use different color here move on to our API the API we'll just we'll use probably fast API to set that up then once we set that up we go on to our front end part and the front end I don't I'm not a front-end developer I just mainly use Python but I do know Angular a little bit so what we're gonna do is build all of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a lot that I think we would have to do to build this and I think it'd be pretty interesting so that is what we're going to be covering in sort of the next few videos and the one or the final little thing okay so over here we have the Marcus Aurelius Superman and I thought maybe something like this would be cool I don't know this is something I drew ages ago and this is this sorry so this,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t402.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 550, 'published': '2021-04-09 15:22:50 UTC', 'start': 402, 'text': ""of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a lot that I think we would have to do to build this and I think it'd be pretty interesting so that is what we're going to be covering in sort of the next few videos and the one or the final little thing okay so over here we have the Marcus Aurelius Superman and I thought maybe something like this would be cool I don't know this is something I drew ages ago and this is this sorry so this is like this is Marcus Aurelius and I think something like this maybe this or something like it would be pretty cool to just have in the middle of the web page and underneath we have a search bar and keep it pretty simple so I think that's everything really for the plan and I mean the first thing we're going to do in the in the next video is actually sell requests and download that data and"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,550,2021-04-09 15:22:50 UTC,402,of this part using Angular so this will be our angular front end and that's essentially everything we'll be covering but there's no there's quite a lot in here in particular as well what I've missed is alongside you know we have our reader model down here but what I want to try and do is rather than just taking the reader model from Hugging Face transformers as we normally would I want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a lot that I think we would have to do to build this and I think it'd be pretty interesting so that is what we're going to be covering in sort of the next few videos and the one or the final little thing okay so over here we have the Marcus Aurelius Superman and I thought maybe something like this would be cool I don't know this is something I drew ages ago and this is this sorry so this is like this is Marcus Aurelius and I think something like this maybe this or something like it would be pretty cool to just have in the middle of the web page and underneath we have a search bar and keep it pretty simple so I think that's everything really for the plan and I mean the first thing we're going to do in the in the next video is actually sell requests and download that data and,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
4Jmq28RQ3hU-t441.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 574, 'published': '2021-04-09 15:22:50 UTC', 'start': 441, 'text': ""want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a lot that I think we would have to do to build this and I think it'd be pretty interesting so that is what we're going to be covering in sort of the next few videos and the one or the final little thing okay so over here we have the Marcus Aurelius Superman and I thought maybe something like this would be cool I don't know this is something I drew ages ago and this is this sorry so this is like this is Marcus Aurelius and I think something like this maybe this or something like it would be pretty cool to just have in the middle of the web page and underneath we have a search bar and keep it pretty simple so I think that's everything really for the plan and I mean the first thing we're going to do in the in the next video is actually sell requests and download that data and maybe pre-process it as well or they they might be two videos so that's everything for this video I hope that you're as excited about this as I am because I'm really looking forward to actually building all of this I think it'll be super cool and I mean ideally at the end it's one it's gonna look cool and two we're gonna learn like a huge amount of stuff if you even put all"", 'title': 'How-to Structure a Q&A ML App', 'url': 'https://youtu.be/4Jmq28RQ3hU'}",UCv83tO5cePwHMt1952IVVHw,574,2021-04-09 15:22:50 UTC,441,want to actually try training it and for that we need to use something called MLM which is mass language modeling so we would need to train a BERT model using MLM or fine-tune the BERT model I should say on the data from our books up here and then we'd also want to train it so that it performs Q&A and for that we need to use the squad data set probably squad anyway so you know there's quite a lot that I think we would have to do to build this and I think it'd be pretty interesting so that is what we're going to be covering in sort of the next few videos and the one or the final little thing okay so over here we have the Marcus Aurelius Superman and I thought maybe something like this would be cool I don't know this is something I drew ages ago and this is this sorry so this is like this is Marcus Aurelius and I think something like this maybe this or something like it would be pretty cool to just have in the middle of the web page and underneath we have a search bar and keep it pretty simple so I think that's everything really for the plan and I mean the first thing we're going to do in the in the next video is actually sell requests and download that data and maybe pre-process it as well or they they might be two videos so that's everything for this video I hope that you're as excited about this as I am because I'm really looking forward to actually building all of this I think it'll be super cool and I mean ideally at the end it's one it's gonna look cool and two we're gonna learn like a huge amount of stuff if you even put all,How-to Structure a Q&A ML App,https://youtu.be/4Jmq28RQ3hU
SGazDb8o-to-t13.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 189, 'published': '2022-02-17 15:45:58 UTC', 'start': 13, 'text': ""So there are already plenty of components that you can use directly from Streamlit and also from the Streamlit community. Obviously you may find yourself needing to use something that doesn't already exist. So we'll go through that process. Fortunately it's not too difficult. It does require a little bit of front end code. But Streamlit does make it pretty straightforward. So let's have a quick look at what this card might look like. This component is a card. So all we have for our custom component is this little card in the middle here. The rest of this is just Streamlit. And then this bit here is based on a Material UI component. So if I go back we just have a title, subtitle, text or body of text and a link here which is a Material UI button. Click on it and for now it's just taken to Google. But we can essentially like an object in Python we can pass whatever values we like to this component. So here are the actual cards from Material UI that we're basing this on. And with these cards we can include much more than what I have shown you. This is just the absolute basics. Like we can include these little profile images, drop downs, options and so on. Even little expanders, pictures. So there's a lot we can do with it. And as far as I know there isn't anything like this available directly within Streamlit. So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,189,2022-02-17 15:45:58 UTC,13,"So there are already plenty of components that you can use directly from Streamlit and also from the Streamlit community. Obviously you may find yourself needing to use something that doesn't already exist. So we'll go through that process. Fortunately it's not too difficult. It does require a little bit of front end code. But Streamlit does make it pretty straightforward. So let's have a quick look at what this card might look like. This component is a card. So all we have for our custom component is this little card in the middle here. The rest of this is just Streamlit. And then this bit here is based on a Material UI component. So if I go back we just have a title, subtitle, text or body of text and a link here which is a Material UI button. Click on it and for now it's just taken to Google. But we can essentially like an object in Python we can pass whatever values we like to this component. So here are the actual cards from Material UI that we're basing this on. And with these cards we can include much more than what I have shown you. This is just the absolute basics. Like we can include these little profile images, drop downs, options and so on. Even little expanders, pictures. So there's a lot we can do with it. And as far as I know there isn't anything like this available directly within Streamlit. So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website.",Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t56.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 211, 'published': '2022-02-17 15:45:58 UTC', 'start': 56, 'text': ""So all we have for our custom component is this little card in the middle here. The rest of this is just Streamlit. And then this bit here is based on a Material UI component. So if I go back we just have a title, subtitle, text or body of text and a link here which is a Material UI button. Click on it and for now it's just taken to Google. But we can essentially like an object in Python we can pass whatever values we like to this component. So here are the actual cards from Material UI that we're basing this on. And with these cards we can include much more than what I have shown you. This is just the absolute basics. Like we can include these little profile images, drop downs, options and so on. Even little expanders, pictures. So there's a lot we can do with it. And as far as I know there isn't anything like this available directly within Streamlit. So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website. So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,211,2022-02-17 15:45:58 UTC,56,"So all we have for our custom component is this little card in the middle here. The rest of this is just Streamlit. And then this bit here is based on a Material UI component. So if I go back we just have a title, subtitle, text or body of text and a link here which is a Material UI button. Click on it and for now it's just taken to Google. But we can essentially like an object in Python we can pass whatever values we like to this component. So here are the actual cards from Material UI that we're basing this on. And with these cards we can include much more than what I have shown you. This is just the absolute basics. Like we can include these little profile images, drop downs, options and so on. Even little expanders, pictures. So there's a lot we can do with it. And as far as I know there isn't anything like this available directly within Streamlit. So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website. So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit.",Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t98.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 261, 'published': '2022-02-17 15:45:58 UTC', 'start': 98, 'text': ""So here are the actual cards from Material UI that we're basing this on. And with these cards we can include much more than what I have shown you. This is just the absolute basics. Like we can include these little profile images, drop downs, options and so on. Even little expanders, pictures. So there's a lot we can do with it. And as far as I know there isn't anything like this available directly within Streamlit. So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website. So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit. Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,261,2022-02-17 15:45:58 UTC,98,"So here are the actual cards from Material UI that we're basing this on. And with these cards we can include much more than what I have shown you. This is just the absolute basics. Like we can include these little profile images, drop downs, options and so on. Even little expanders, pictures. So there's a lot we can do with it. And as far as I know there isn't anything like this available directly within Streamlit. So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website. So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit. Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier.",Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t138.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 283, 'published': '2022-02-17 15:45:58 UTC', 'start': 138, 'text': ""So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website. So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit. Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier. So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,283,2022-02-17 15:45:58 UTC,138,"So let's go ahead and set up our environment for developing this component. So if you are on Mac and you have Homebrew installed you can just go brew, install node. And this will install Node.js and the node package manager. I already have it installed so I'm not going to rerun it. If you are not on Mac or you don't have Homebrew you can download it from the Node.js website. So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit. Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier. So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo.",Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t174.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 314, 'published': '2022-02-17 15:45:58 UTC', 'start': 174, 'text': ""So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit. Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier. So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo. We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,314,2022-02-17 15:45:58 UTC,174,"So go over here. This here. Node.js.org. And then download. OK. And you have Windows, Mac and so on. So once you have that installed you also need to install Streamlit. I'm going to assume you already have it installed. But if not you just pip install Streamlit. Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier. So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo. We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository.",Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t202.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 339, 'published': '2022-02-17 15:45:58 UTC', 'start': 202, 'text': ""Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier. So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo. We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository. OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,339,2022-02-17 15:45:58 UTC,202,Now I have Streamlit in another environment so I'm going to activate that. I'm going to activate Streamlit. And the Streamlit side of things I will run from this terminal window. Now to build our component for Streamlit we need to follow a set of structure. And fortunately Streamlit provides us with a few templates that we can start from to make our lives a little bit easier. So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo. We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository. OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t236.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 374, 'published': '2022-02-17 15:45:58 UTC', 'start': 236, 'text': ""So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo. We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository. OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project. So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,374,2022-02-17 15:45:58 UTC,236,So let me show you where you can find those. So just type in Streamlit GitHub or maybe we can go with Streamlit Components template. It's probably easier. Click on here. OK. And we have the Streamlit Component Template repo. We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository. OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project. So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t271.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 407, 'published': '2022-02-17 15:45:58 UTC', 'start': 271, 'text': ""We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository. OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project. So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually. And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,407,2022-02-17 15:45:58 UTC,271,We just want to git clone this so we can code over here. Copy this. And then switch back to our terminal. Navigate to whatever folder you're going to store the template folder within. So I'm going to go to Document Projects and I'm going to write git clone. And then I'm going to clone the component template repository. OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project. So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually. And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t303.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 449, 'published': '2022-02-17 15:45:58 UTC', 'start': 303, 'text': ""OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project. So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually. And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine. We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,449,2022-02-17 15:45:58 UTC,303,OK. Let's open that VS Code and we'll have a look at what we have in here. OK. So. On the right over here we have our directory structure. We have a few different templates. So we're going to go this template here which uses React. And within this template directory this is our actual project. So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually. And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine. We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t331.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 494, 'published': '2022-02-17 15:45:58 UTC', 'start': 331, 'text': ""So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually. And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine. We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different. So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,494,2022-02-17 15:45:58 UTC,331,So we can almost ignore the rest of this stuff here. Just anything within template is what we care about. We have set up py. So we're going to use this and manifest for creating a pip package which will contain our components. So to actually use that component we put in sol.app.component and then we just import it into our streamlit app script. Or app.py usually. And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine. We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different. So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t365.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 544, 'published': '2022-02-17 15:45:58 UTC', 'start': 365, 'text': ""And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine. We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different. So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names. And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,544,2022-02-17 15:45:58 UTC,365,And then use it. And within here we just have the default file names here or default directory names of my component. We're going to change some of these inside. So here we have initpy. So that's where we're initializing our streamlit server or app from. We'll go through that. It's fine. We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different. So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names. And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t391.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 582, 'published': '2022-02-17 15:45:58 UTC', 'start': 391, 'text': ""We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different. So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names. And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports. So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,582,2022-02-17 15:45:58 UTC,391,We have this front end which is anything in here is the React side of things. So if we go a little further we have some styling. The index. And then in here we have these TSX files. So in here we're using TypeScript. So if you know TypeScript it's really good. If not it's not too much different from Python. I mean it's fairly different but it's not unbelievably different. So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names. And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports. So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t431.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 620, 'published': '2022-02-17 15:45:58 UTC', 'start': 431, 'text': ""So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names. And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports. So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those. So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,620,2022-02-17 15:45:58 UTC,431,So I think if you know Python you'll probably read this at least and kind of follow what we're doing. But I also don't really know TypeScript. I can just get through putting something simple together with this. So most of the work we're going to do is going to be my component TSX. But we are going to modify a lot of these files as well. So first let's rename everything because everything at the moment is using default names. And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports. So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those. So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t470.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 658, 'published': '2022-02-17 15:45:58 UTC', 'start': 470, 'text': ""And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports. So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those. So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here. So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,658,2022-02-17 15:45:58 UTC,470,And we obviously are building a custom component. We want to give it our own name. That makes it a little more identifiable. So we'll start by going to my component up here. Rename that to stCardComponent. So st is just Streamlit Card Component. Down here we have myComponent. Rename that to cardComponent. Basically anywhere that we have myComponent we modify to cardComponent. We update these imports. So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those. So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here. So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t520.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 705, 'published': '2022-02-17 15:45:58 UTC', 'start': 520, 'text': ""So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those. So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here. So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename. Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,705,2022-02-17 15:45:58 UTC,520,So inside cardComponent here if we just find and replace. So find myComponent and replace that with cardComponent. Replace all of those. See if we have anything for myComponent. No. OK. Save that and let's have a look at the index. I think we also have something here. Just myComponent here. OK. Replace those. So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here. So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename. Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t572.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 734, 'published': '2022-02-17 15:45:58 UTC', 'start': 572, 'text': ""So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here. So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename. Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good. So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,734,2022-02-17 15:45:58 UTC,572,So we have everything in there. And in set of Pi we also want to update the name here. So this will change it to Streamlit Card Component. OK. So this defines it for our package later on. And this actually should align with the directory or the folder that we have here. So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename. Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good. So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t608.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 767, 'published': '2022-02-17 15:45:58 UTC', 'start': 608, 'text': ""So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename. Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good. So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent. So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,767,2022-02-17 15:45:58 UTC,608,So not Streamlit but st. And in our manifest here we also need to update this to be stCardComponent. So this needs to point to the front end. We don't have the build directory yet but we will. So that needs to go stCardComponent. OK. So that should pretty much be everything we need to rename. Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good. So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent. So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t642.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 797, 'published': '2022-02-17 15:45:58 UTC', 'start': 642, 'text': ""Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good. So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent. So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment. So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,797,2022-02-17 15:45:58 UTC,642,Maybe other little bits are actually in init. Here we also want to change this to stCardComponent. So here stCardComponent. Anything else? OK. That looks good. So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent. So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment. So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t674.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 825, 'published': '2022-02-17 15:45:58 UTC', 'start': 674, 'text': ""So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent. So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment. So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need. But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,825,2022-02-17 15:45:58 UTC,674,So let's go ahead and actually initialize the current or basically default other than the things we've renamed version of this component and see how that looks. So we'll go to and here we have the component template. Inside the template. See the template. And then we have stCardComponent. So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment. So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need. But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t719.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 846, 'published': '2022-02-17 15:45:58 UTC', 'start': 719, 'text': ""So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment. So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need. But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment. In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,846,2022-02-17 15:45:58 UTC,719,So in here we also have front end directory and inside here we have all of our packages and everything. So like the node side of stuff. So first thing we need to do is actually install the node packages we need for our CARD component. So. So we just do npm install and this is just going to install everything from this package.json file. So run that. It might take a moment. So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need. But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment. In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t762.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 876, 'published': '2022-02-17 15:45:58 UTC', 'start': 762, 'text': ""So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need. But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment. In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material. Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,876,2022-02-17 15:45:58 UTC,762,So just give it a second. OK. So that's done. And next thing we want to do is install the node packages that are required specifically for the CARD that we're going to build. So these are all the node packages we've just installed. They're extremely to function like the core of the packages that we need. But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment. In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material. Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t785.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 911, 'published': '2022-02-17 15:45:58 UTC', 'start': 785, 'text': ""But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment. In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material. Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error. We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,911,2022-02-17 15:45:58 UTC,785,But because we are using this material UI CARD thing we need a few extra things. So we need to npm install again. This is kind of similar to like a pip install. Although specific to this directory. So when we pip install we install them to our Python environment. In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material. Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error. We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t813.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 930, 'published': '2022-02-17 15:45:58 UTC', 'start': 813, 'text': ""In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material. Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error. We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time. But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,930,2022-02-17 15:45:58 UTC,813,In this case it's almost like the environment is this directory in this project. So npm install. We need MUI material. MUI icons material. Icons material. Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error. We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time. But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t839.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 962, 'published': '2022-02-17 15:45:58 UTC', 'start': 839, 'text': ""Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error. We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time. But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here. We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,962,2022-02-17 15:45:58 UTC,839,Emotion react. And also emotion styled. Now I think this is probably going to give us an error. Let's see. OK. So yes we get this error. We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time. But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here. We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t858.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 980, 'published': '2022-02-17 15:45:58 UTC', 'start': 858, 'text': ""We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time. But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here. We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory. So cd documents projects. It is component template. Templates. OK. cd template."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,980,2022-02-17 15:45:58 UTC,858,We have a dependency conflict with I think if I remember correctly. So we have this 3mic component template and this throws some dependency conflicts with the MUI material stuff. So it's annoying but we can just get around it by adding this legacy pia-deps. Now I haven't been playing around with this and I haven't noticed any issues pop up from using this legacy pia-deps. But obviously you just need to be aware. Don't just throw it in there all the time. But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here. We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory. So cd documents projects. It is component template. Templates. OK. cd template.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t902.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1023, 'published': '2022-02-17 15:45:58 UTC', 'start': 902, 'text': ""But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here. We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory. So cd documents projects. It is component template. Templates. OK. cd template. And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,1023,2022-02-17 15:45:58 UTC,902,But in this case it seems to work fine. OK. So with that we should be able to run everything. So let's go ahead and do that. First thing we want to do is so we're going to run two things here. We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory. So cd documents projects. It is component template. Templates. OK. cd template. And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t925.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1043, 'published': '2022-02-17 15:45:58 UTC', 'start': 925, 'text': ""We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory. So cd documents projects. It is component template. Templates. OK. cd template. And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py. This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,1043,2022-02-17 15:45:58 UTC,925,We're going to run within this front end directory. We write npm start. This is going to initialize or execute the server that hosts our react component. And then we also need to open another terminal window. We also need to navigate to our template directory. So cd documents projects. It is component template. Templates. OK. cd template. And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py. This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t950.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1061, 'published': '2022-02-17 15:45:58 UTC', 'start': 950, 'text': ""So cd documents projects. It is component template. Templates. OK. cd template. And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py. This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution. So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,1061,2022-02-17 15:45:58 UTC,950,So cd documents projects. It is component template. Templates. OK. cd template. And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py. This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution. So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t970.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1095, 'published': '2022-02-17 15:45:58 UTC', 'start': 970, 'text': ""And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py. This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution. So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that. We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,1095,2022-02-17 15:45:58 UTC,970,And in here we have a sd card component. OK. We have this initpy. Initpy is like a template app for just running and testing our component. So we just write streamlit run init.py. So previous videos you've probably seen me write streamlit run app.py. This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution. So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that. We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t1008.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1115, 'published': '2022-02-17 15:45:58 UTC', 'start': 1008, 'text': ""This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution. So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that. We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1. Now we have this template component. So we can click here and it updates this basically. Maybe even change this. Enter. We get this. So that's cool."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,1115,2022-02-17 15:45:58 UTC,1008,This is kind of acting as our app.py whilst we're in the development stage. But once we switch to a release version this initpy we will modify a little bit. And it will not be for doing this streamlit run. It will do something slightly different. It will extract everything from a compiled build distribution. So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that. We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1. Now we have this template component. So we can click here and it updates this basically. Maybe even change this. Enter. We get this. So that's cool.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t1038.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1137, 'published': '2022-02-17 15:45:58 UTC', 'start': 1038, 'text': ""So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that. We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1. Now we have this template component. So we can click here and it updates this basically. Maybe even change this. Enter. We get this. So that's cool. But obviously we want to build a custom component. And we'll go ahead and we'll do that in the next video. For now I think we'll leave it there. We've set up the environment. We've started running the default component."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,1137,2022-02-17 15:45:58 UTC,1038,So now we have these. We're hosting two servers. We have this localhost 3001. If we go over there. Open that. We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1. Now we have this template component. So we can click here and it updates this basically. Maybe even change this. Enter. We get this. So that's cool. But obviously we want to build a custom component. And we'll go ahead and we'll do that in the next video. For now I think we'll leave it there. We've set up the environment. We've started running the default component.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
SGazDb8o-to-t1052.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1154, 'published': '2022-02-17 15:45:58 UTC', 'start': 1052, 'text': ""We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1. Now we have this template component. So we can click here and it updates this basically. Maybe even change this. Enter. We get this. So that's cool. But obviously we want to build a custom component. And we'll go ahead and we'll do that in the next video. For now I think we'll leave it there. We've set up the environment. We've started running the default component. And I think that's good enough for now. And in the next video we'll be able to really focus on actually building the component itself. Which means we're going to be toying with code over in the card component TSX quite a bit. And cleaning this up. And yeah."", 'title': 'Streamlit for ML #5.1 - Custom React Components in Streamlit Setup', 'url': 'https://youtu.be/SGazDb8o-to'}",UCv83tO5cePwHMt1952IVVHw,1154,2022-02-17 15:45:58 UTC,1052,We're going to see nothing. So this is just hosting our React component. But our React component by itself doesn't actually show anything. So we actually need to go over and open the streamlit localhost. So that is in 8.5.0.1. Now we have this template component. So we can click here and it updates this basically. Maybe even change this. Enter. We get this. So that's cool. But obviously we want to build a custom component. And we'll go ahead and we'll do that in the next video. For now I think we'll leave it there. We've set up the environment. We've started running the default component. And I think that's good enough for now. And in the next video we'll be able to really focus on actually building the component itself. Which means we're going to be toying with code over in the card component TSX quite a bit. And cleaning this up. And yeah.,Streamlit for ML #5.1 - Custom React Components in Streamlit Setup,https://youtu.be/SGazDb8o-to
3IPCEeh4xTg-t7.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 120, 'published': '2021-12-17 14:24:40 UTC', 'start': 7, 'text': ""language data augmentation strategies and training approaches. More specifically we're going to focus on something called augmented experts. So you may or may not be aware that the past decade has been sort of a renaissance or explosion in the field of machine learning and data science and a lot of that, especially the early progress with things like perceptron and neural networks, a lot of that was researched and discovered back in the 50s and 60s and 70s, but we didn't see that really applied in industry or anywhere really until the past decade and there are two main reasons for this. So the first is that we didn't have enough compute power back in the 50s, 60s, 70s to train the models that we needed to train and we also didn't have the data to actually train those models. Now compute power is not really a problem anymore. We sort of look at this graph, it depends on what model you're training of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,120,2021-12-17 14:24:40 UTC,7,"language data augmentation strategies and training approaches. More specifically we're going to focus on something called augmented experts. So you may or may not be aware that the past decade has been sort of a renaissance or explosion in the field of machine learning and data science and a lot of that, especially the early progress with things like perceptron and neural networks, a lot of that was researched and discovered back in the 50s and 60s and 70s, but we didn't see that really applied in industry or anywhere really until the past decade and there are two main reasons for this. So the first is that we didn't have enough compute power back in the 50s, 60s, 70s to train the models that we needed to train and we also didn't have the data to actually train those models. Now compute power is not really a problem anymore. We sort of look at this graph, it depends on what model you're training of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t35.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 142, 'published': '2021-12-17 14:24:40 UTC', 'start': 35, 'text': ""perceptron and neural networks, a lot of that was researched and discovered back in the 50s and 60s and 70s, but we didn't see that really applied in industry or anywhere really until the past decade and there are two main reasons for this. So the first is that we didn't have enough compute power back in the 50s, 60s, 70s to train the models that we needed to train and we also didn't have the data to actually train those models. Now compute power is not really a problem anymore. We sort of look at this graph, it depends on what model you're training of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,142,2021-12-17 14:24:40 UTC,35,"perceptron and neural networks, a lot of that was researched and discovered back in the 50s and 60s and 70s, but we didn't see that really applied in industry or anywhere really until the past decade and there are two main reasons for this. So the first is that we didn't have enough compute power back in the 50s, 60s, 70s to train the models that we needed to train and we also didn't have the data to actually train those models. Now compute power is not really a problem anymore. We sort of look at this graph, it depends on what model you're training of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t59.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 161, 'published': '2021-12-17 14:24:40 UTC', 'start': 59, 'text': ""So the first is that we didn't have enough compute power back in the 50s, 60s, 70s to train the models that we needed to train and we also didn't have the data to actually train those models. Now compute power is not really a problem anymore. We sort of look at this graph, it depends on what model you're training of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,161,2021-12-17 14:24:40 UTC,59,"So the first is that we didn't have enough compute power back in the 50s, 60s, 70s to train the models that we needed to train and we also didn't have the data to actually train those models. Now compute power is not really a problem anymore. We sort of look at this graph, it depends on what model you're training of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t83.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 186, 'published': '2021-12-17 14:24:40 UTC', 'start': 83, 'text': ""of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,186,2021-12-17 14:24:40 UTC,83,"of course if you are open AI and you're training GPT-4 or 5 or whatever yeah, maybe compute power is pretty relevant, but for most of us we can get access to cloud machinery, personal machines and we can wait a few hours or a couple of days and fine tune or pre-train a transform model that is good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t111.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 210, 'published': '2021-12-17 14:24:40 UTC', 'start': 111, 'text': ""good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back. It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,210,2021-12-17 14:24:40 UTC,111,"good performance for what we need. Now that obviously wasn't always the case until very recently back in 1960s, you see on this graph here we have the IBM 704 and you can see under the Y axes we have floating point operations per second and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back. It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t134.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 230, 'published': '2021-12-17 14:24:40 UTC', 'start': 134, 'text': ""and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back. It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,230,2021-12-17 14:24:40 UTC,134,"and that's a logarithmic scale. So linear scale just basically looks like a straight line until a few years ago and then shoots up. It's pretty impressive how much progress is made in terms of computing power. Now like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back. It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t153.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 259, 'published': '2021-12-17 14:24:40 UTC', 'start': 153, 'text': ""like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back. It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,259,2021-12-17 14:24:40 UTC,153,"like I said, that's not really an issue for us anymore. We have the compute in most cases to do what we need to do and and data is not as much of a problem anymore, but we'll talk about that in a moment. So data again, we have a very big increase in data, not quite as big as the computing power and this graph here doesn't go quite as far back. It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t179.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 280, 'published': '2021-12-17 14:24:40 UTC', 'start': 179, 'text': ""It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big. For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,280,2021-12-17 14:24:40 UTC,179,"It's only 2010 where I believe it was at 2 zettabytes and now 71 or so in 2021. So there's a fairly big increase, not quite as much as computing power over time, but still pretty massive. Now the thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big. For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t202.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 311, 'published': '2021-12-17 14:24:40 UTC', 'start': 202, 'text': ""thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big. For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well. Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,311,2021-12-17 14:24:40 UTC,202,"thing with data is yes, there's a lot of data out there, but is there that much data out there for what we need to train models to do and in a lot of cases, yes, there is. But it really depends on what you're doing. If you are focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big. For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well. Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t221.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 338, 'published': '2021-12-17 14:24:40 UTC', 'start': 221, 'text': ""focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big. For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well. Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,338,2021-12-17 14:24:40 UTC,221,"focusing on a more niche domain. So what I have here on the left over here are a couple of niche domains. There's not that much data out there on sentence pairs for climate evidence and claims, for example. So where you have a piece of evidence and a claim and whether the claim supports evidence or not, there is a very small data set called climate fever data set, but it's not big. For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well. Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t251.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 372, 'published': '2021-12-17 14:24:40 UTC', 'start': 251, 'text': ""For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well. Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well. But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,372,2021-12-17 14:24:40 UTC,251,"For agriculture, I assume within that industry, there's not that much data, although I have never worked in that industry. So I am not fully aware. I just assume there's probably not that much. And then also niche finance, which I do at least have a bit more experience with and I imagine this is probably something that a lot of you will find useful as well. Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well. But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t274.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 406, 'published': '2021-12-17 14:24:40 UTC', 'start': 274, 'text': ""Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well. But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work. So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,406,2021-12-17 14:24:40 UTC,274,"Because finance is a big industry. There's a lot of finance data out there, but there's a lot of niche projects and problems in finance where you find much less data. So yes, we have a lot more data nowadays, but we don't have enough for a lot of the data that we need. On the right here, we have a couple of examples of low resource data sets. So we have Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well. But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work. So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t299.79999999999995,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 441, 'published': '2021-12-17 14:24:40 UTC', 'start': 299, 'text': ""Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well. But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work. So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision. And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,441,2021-12-17 14:24:40 UTC,299,"Adave from the Maldives and also the Navajo languages as well. So with these we kind of need to find a different approach. Now we can investigate, depending on your use case, unsupervised learning, TSEA, which we have covered in a previous video article and that does work when you're trying to build a model that recognizes generic similarity. It works very well as well. But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work. So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision. And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t328.71999999999997,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 486, 'published': '2021-12-17 14:24:40 UTC', 'start': 328, 'text': ""But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work. So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision. And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced. So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,486,2021-12-17 14:24:40 UTC,328,"But for example with the climate claims data, we are not necessarily trying to match sentence A and B based on their semantic similarity. But we're trying to match sentence A, which is a claim, to sentence B, which is a claim. As to whether that evidence supports the claim or not. So in that case, unsupervised approach like TSEA doesn't really work. So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision. And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced. So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t360.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 524, 'published': '2021-12-17 14:24:40 UTC', 'start': 360, 'text': ""So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision. And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced. So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,524,2021-12-17 14:24:40 UTC,360,"So what we have is very little data and there aren't really any alternative training approaches that we can use. So basically what we need to do is create more data. Now data orientation is difficult, particularly for language. So data orientation is not specific to NLP. It's used across ML and it's more established in the field of computer vision. And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced. So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t392.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 559, 'published': '2021-12-17 14:24:40 UTC', 'start': 392, 'text': ""And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced. So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,559,2021-12-17 14:24:40 UTC,392,"And that makes sense because computer vision, say you have an image, you can modify that image using a few different approaches. And a person can still look at that image and think, OK, that is the same image. It's just maybe it's rotated a little bit. We've changed the color grading, the brightness, or something along those lines. We just modified it slightly. But it's still in essence the same image. Now for language it's a bit difficult because language is very abstract and nuanced. So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t428.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 596, 'published': '2021-12-17 14:24:40 UTC', 'start': 428, 'text': ""So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,596,2021-12-17 14:24:40 UTC,428,"So if you start randomly changing certain words, the chances are you're going to produce something that doesn't make any sense. And we, when we're augmenting our data, we don't want to just throw rubbish into our model. We want something that makes sense. So there are some data augmentation techniques. And we'll look at a couple of the simpler ones now. So there is a library called NLP.org which I think is very good for this sort of thing. It's essentially a library that allows us to do data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t465.59999999999997,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 641, 'published': '2021-12-17 14:24:40 UTC', 'start': 465, 'text': ""data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,641,2021-12-17 14:24:40 UTC,465,"data augmentation for NLP. And what you can see here is two methods using word2vec vectors and similarity. And what we're doing is taking this original sentence. So the quick brown fox jumps over the lazy dog. And we're just inserting some words using word2vec. So we're trying to find what words word2vec thinks could go in here, which words are the most similar to the surrounding words. And we have this al-Ziari, which I don't know. I think it seems like a name to me. But I am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t508.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 684, 'published': '2021-12-17 14:24:40 UTC', 'start': 508, 'text': ""am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame. And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,684,2021-12-17 14:24:40 UTC,508,"am not sure. That I don't think really fits there. So it's not great. It's not perfect. Lazy superintendents dog. That does kind of make sense. I feel like a lazy superintendents dog is maybe a stereotype or I'm sure it's been in The Simpsons or something before. So, okay, fair enough. I can see how that can fit in there. Which again, it's a bit weird. It's not great. Substitution for me seems to work better. So rather than the quick brown fox, we have the easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame. And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t545.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 719, 'published': '2021-12-17 14:24:40 UTC', 'start': 545, 'text': ""easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame. And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,719,2021-12-17 14:24:40 UTC,545,"easy brown fox. And rather than jumping over the lazy dog, jumps around the lazy dog. Which changes the meaning slightly. Easy is a bit weird there to be fair. But we still have a sentence that kind of makes sense. So that's good, I think. Now we don't have to use words to vet. We can also use contextual word embeddings like with Bert. And for me, I think the results look better. So for insertion, we get even the quick brown fox usually jumps over the lazy dog. So we're adding some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame. And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t583.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 762, 'published': '2021-12-17 14:24:40 UTC', 'start': 583, 'text': ""some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame. And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here,"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,762,2021-12-17 14:24:40 UTC,583,"some words there. It makes sense. That's I think good for substitution. And we're only doing one word here. And we're changing that to a little quick brown fox instead of just quick brown fox. So I think that makes sense. And this is a good way of augmenting your data and bring more data from less. But for us, because we are using sentence pairs, we can basically just take all of the data from say we have A and B over here. Imagine this is a data frame. And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here,",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t624.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 794, 'published': '2021-12-17 14:24:40 UTC', 'start': 624, 'text': ""And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here, or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer,"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,794,2021-12-17 14:24:40 UTC,624,"And we have all of these sentence A's and we have all these sentence B's. Now if we take one sentence A, it's already matched up to one sentence B. And what we can do is say, OK, I want to randomly sample some other sentence B's and match them up to our sentence A. So we have three more pairs now. OK, so if we did this, if we took three sentence A's, three sentence B's, and we made new pairs from all of them, not really random sampling, just taking all the possible pairs, we end up with nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here, or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer,",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t669.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 840, 'published': '2021-12-17 14:24:40 UTC', 'start': 669, 'text': ""nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here, or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer, which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,840,2021-12-17 14:24:40 UTC,669,"nine new or nine pairs in total, which is much better if you extend that a little further. So from just a thousand pairs, we can end up with one million pairs. So you can see quite quickly, you can take a small data set and very quickly create a big data set with it. Now this is just one part of the problem though, because our smaller data set will have similarity scores or natural language inference labels, but the new data set that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here, or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer, which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t707.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 884, 'published': '2021-12-17 14:24:40 UTC', 'start': 707, 'text': ""that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here, or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer, which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,884,2021-12-17 14:24:40 UTC,707,"that we've just created, the augmented data set, doesn't have any of those, just randomly sampled new sentence pairs. So there's no scores or labels there and we need those to actually train and model. So what we can do is take a slightly different approach or add another step into here. Now that other set is using something called a cross encoder. So in semantic similarity, we can use two different types of models. We can use a cross encoder, which is over here, or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer, which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t745.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 920, 'published': '2021-12-17 14:24:40 UTC', 'start': 745, 'text': ""or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer, which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,920,2021-12-17 14:24:40 UTC,745,"or we can use a bi-encoder or what I would usually call a sentence transporter. Now a cross encoder is the sort of old way of doing it and it works by simply putting sentence A and sentence B into a BERT model together at once. So we have sentence A, separate a token, sentence B, feed that into a BERT model and from that BERT model we will get all of our embeddings, output embeddings over here and they all get fed into a linear layer, which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t780.5600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 969, 'published': '2021-12-17 14:24:40 UTC', 'start': 780, 'text': ""which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,969,2021-12-17 14:24:40 UTC,780,"which converts all of those into a similarity score up here. Now that similarity score is typically going to be more accurate than a similarity score that you get from a bi-encoder or a sentence transformer. But the problem here is from our sentence transformer we are outputting sentence vectors and if we have two sentence vectors we can perform a cosine similarity or a Buclidean distance calculation to get the similarity of those two vectors. And the cosine similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t822.1600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1018, 'published': '2021-12-17 14:24:40 UTC', 'start': 822, 'text': ""similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1018,2021-12-17 14:24:40 UTC,822,"similarity calculation or operation is much quicker than a full BERT inference set, which is what we need with a cross encoder. So I think it is something like a BERT model so I think it is something like for maybe 10 maybe clustering 10,000 vectors using a cross encoder, an expert cross encoder, would take you something like 65 hours whereas with a bi-encoder it's going to take you about five seconds. So it's much much quicker. And that's why we use bi-encoders or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t868.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1060, 'published': '2021-12-17 14:24:40 UTC', 'start': 868, 'text': ""or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set. That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1060,2021-12-17 14:24:40 UTC,868,"or sentence transformers. Now the reason I'm talking about cross encoders is because we get this more accurate similarity score which we can use as a label. And another very key thing here is that we need less data to train a cross encoder. With a bi-encoder if we I think the SBERT model itself was trained on something like one million sentence pairs and some new models are training a billion or more. Whereas a cross encoder we can train a reasonable cross encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set. That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t907.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1105, 'published': '2021-12-17 14:24:40 UTC', 'start': 907, 'text': ""encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set. That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1105,2021-12-17 14:24:40 UTC,907,"encoder on something like 5k or maybe even less sentence pairs. So we need much less data and that works quite well what we've been talking about with data orientation. We can take a small data set we can augment it to create more sentence pairs and then what we do is train on that original data set which we call the gold data set. We train our cross encoder using that and then we use that fine-tuned cross encoder to label the augmented data set without labels and that creates a augmented label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set. That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t946.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1140, 'published': '2021-12-17 14:24:40 UTC', 'start': 946, 'text': ""label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set. That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1140,2021-12-17 14:24:40 UTC,946,"label data set that we call the silver data set. So that sort of strategy of creating a silver data set which we would then use to fine-tune our bi-encoder model is what we refer to as the in-domain augmented SBERT training strategy. And this sort of what you can see this flow diagram is basically every set that we need to do to create an in-domain or SBERT training process. So we've already described most of this so we get our gold data set, the original data set. That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1006.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1180, 'published': '2021-12-17 14:24:40 UTC', 'start': 1006, 'text': ""That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1180,2021-12-17 14:24:40 UTC,1006,That's going to be quite small let's say one to five thousand sentence pairs that are labeled. From that we're going to use something like random sampling which I'll just call random sample. We're going to use that to create a larger data set. Let's say we create something like a hundred thousand sentence pairs but these are not labeled. We don't have any similarity scores or natural language inference labels for these. So what we do is we take that gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1046.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1220, 'published': '2021-12-17 14:24:40 UTC', 'start': 1046, 'text': ""gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data. We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1220,2021-12-17 14:24:40 UTC,1046,gold data set and we take it down here and we fine-tune a cross encoder using that gold data because we need less data to train a reasonably good cross encoder. So we take that and we fine-tune cross encoder and then we use that cross encoder alongside our unlabeled data set to create a new silver data set. Now the cross encoder is going to predict the similarity scores or NLI labels for every pair in that data set. So with that we have our silver data. We also have the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data. We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1086.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1260, 'published': '2021-12-17 14:24:40 UTC', 'start': 1086, 'text': ""the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data. We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1260,2021-12-17 14:24:40 UTC,1086,the gold data which is up here and we actually take both those together and we fine-tune the by encoder or the sentence transformer on both the gold data and the silver data. Now one thing I would say here is it's useful to separate some of your gold data at the very start so don't even train your cross encoder on those. It's good to separate them as your evaluation or test set and evaluate both the cross encoder performance and also your by encoder performance on that separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data. We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1129.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1303, 'published': '2021-12-17 14:24:40 UTC', 'start': 1129, 'text': ""separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data. We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1303,2021-12-17 14:24:40 UTC,1129,separate set. So don't include that in your training data for any of your models. Keep that separate and then you can use that to figure out is this working or is it not working. So that is in the main org fermented experts and sort of see this is the same as what you saw before just another this is the training approach. So we have the gold trained cross encoder. We have our unlabeled pairs which have come from random sampling our gold data. We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1164.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1344, 'published': '2021-12-17 14:24:40 UTC', 'start': 1164, 'text': ""We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1344,2021-12-17 14:24:40 UTC,1164,We process those for a cross encoder to create the silver data set and then the silver and the gold come over here to fine-tune a by encoder. So that's it for the theory and the concepts and now what I want to do is actually go through the code and and we'll work through an example of how we can actually do this. Okay so we have downloaded the both the training and the validation set for our scsb data and let's have a look at what some of that data looks like. So scsb zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1205.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1383, 'published': '2021-12-17 14:24:40 UTC', 'start': 1205, 'text': ""zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator. I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1383,2021-12-17 14:24:40 UTC,1205,zero. So we have sentence pair sentence one sentence two just a simple sentence and we have a label which is our similarity score. Now that similarity score varies from between zero to five where zero is no similarity no relation between the two sentence pairs and five is they mean that same thing. Now see here these two mean the same thing as we Now we can see here that these two mean the same thing as we would expect. So we first want to modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator. I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1244.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1425, 'published': '2021-12-17 14:24:40 UTC', 'start': 1244, 'text': ""modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator. I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1425,2021-12-17 14:24:40 UTC,1244,modify that score a little bit because we are going to be training using cosine similarity loss and we would expect our label to not go up to a value of five but we would expect it to go up to a value of one. So all I'm doing here is changing that score so that we are dividing everything by five normalizing everything. So we do that and no problem and now what we can do is load our training data into a data loader. So to do that we first need to load our training data into a data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator. I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1285.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1464, 'published': '2021-12-17 14:24:40 UTC', 'start': 1285, 'text': ""data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator. I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1464,2021-12-17 14:24:40 UTC,1285,data loader. So to do that we first form everything into a input example and then load that into into our PyTorch data loader. So I'll run that and then at the same time during training I also want to output a evaluation source. So how the cross encoder do on the evaluation data. So to do that I import. So here we're importing from sentence transformers cross encoder evaluation. I'm importing the cross encoder CE correlation evaluator. I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1332.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1507, 'published': '2021-12-17 14:24:40 UTC', 'start': 1332, 'text': ""I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1507,2021-12-17 14:24:40 UTC,1332,I again am using input examples with working sentence transformers library and I am importing both text and labels. And here I am putting all that development or I'm putting all that validation of that data into that evaluator. Okay now I can run that and then we can move on to initializing a cross encoder and training it and also evaluating it. So to do that we're going to import from sentence transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1371.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1546, 'published': '2021-12-17 14:24:40 UTC', 'start': 1371, 'text': ""transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class. We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1546,2021-12-17 14:24:40 UTC,1371,transformers. So from sentence transformers and I'll just make sure I'm working in Python. I'm going to import from cross encoder a cross encoder. Okay and to initialize that cross encoder model I'll call it C. All I need to do is write cross encoder very similar to when we write sentence transformer initializer and model. We specify the model from the face transformers that we like to initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class. We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1410.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1582, 'published': '2021-12-17 14:24:40 UTC', 'start': 1410, 'text': ""initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class. We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1582,2021-12-17 14:24:40 UTC,1410,initialize a cross encoder from. So that based on case and also a number of labels that we'd like to use. So in this case we are just targeting a similarity as well between 0 and 1. So we just want a single label there. If we were doing for example NLI labels where we have entailment contradiction and neutral labels or some other labels we would change this to for example 3. But in this case 1. We can initialize our cross encoder and then from there we move on to actually training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class. We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1450.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1633, 'published': '2021-12-17 14:24:40 UTC', 'start': 1450, 'text': ""training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class. We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1633,2021-12-17 14:24:40 UTC,1450,training. So we call model or C.fit and we want to specify the data loader. So this is slightly different to the fit function we usually use with sentence transformers. So we want train data loader. We specify our loader that we initialize just up here the data loader. We don't need to do this but if you are going to evaluate your model during training you also want to add in evaluator as well. So this is from the C correlation evaluator. Make sure here using a cross encoder evaluation class. We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1490.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1675, 'published': '2021-12-17 14:24:40 UTC', 'start': 1490, 'text': ""We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6. And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1675,2021-12-17 14:24:40 UTC,1490,We would like to run for say one epoch and we should define this because I would also like to while we're training I would also like to include some warm up sets as well. I'm going to include a lot of warm up sets actually. Although I'll mention it I'll talk about it in a moment. So I would say number of epochs is equal to one and for the warm up I would like to take integer. So the length of loader. So the number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6. And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1527.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1719, 'published': '2021-12-17 14:24:40 UTC', 'start': 1527, 'text': ""number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6. And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1719,2021-12-17 14:24:40 UTC,1527,number of batches that we have in our data set. I'm going to multiply this by 0.4. So I'm going to do a warm up or do warm up sets for 40 percent of our total data set size or batch or 40 percent of our total number of batches. And we also need to multiply that by number of epochs. Say we're training two epochs we multiply that in this case just one. So not necessary but it's there. So we're actually performing warm up for 40 percent of the training steps and I found this works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6. And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1567.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1759, 'published': '2021-12-17 14:24:40 UTC', 'start': 1567, 'text': ""works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6. And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1759,2021-12-17 14:24:40 UTC,1567,works better than something like 10 percent 15 percent 20 percent. However that being said I think you could also achieve a similar result by just decreasing the learning rate of your model. So by default. So if I write in the epochs here we'll define the warm up sets. So by default this will use optimizer params with a learning rate of 2e to the minus 5. OK. So if you say want to decrease that a little bit you could go let's say go to the minus 6 5e to minus 6. And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1620.8000000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1804, 'published': '2021-12-17 14:24:40 UTC', 'start': 1620, 'text': ""And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1804,2021-12-17 14:24:40 UTC,1620,And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1660.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1845, 'published': '2021-12-17 14:24:40 UTC', 'start': 1660, 'text': ""run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1845,2021-12-17 14:24:40 UTC,1660,run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1702.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1880, 'published': '2021-12-17 14:24:40 UTC', 'start': 1702, 'text': ""we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well. OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1880,2021-12-17 14:24:40 UTC,1702,we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well. OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1744.7199999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1927, 'published': '2021-12-17 14:24:40 UTC', 'start': 1744, 'text': ""using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well. OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet. But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1927,2021-12-17 14:24:40 UTC,1744,using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well. OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet. But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1794.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1968, 'published': '2021-12-17 14:24:40 UTC', 'start': 1794, 'text': ""label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well. OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet. But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,1968,2021-12-17 14:24:40 UTC,1794,label in there. Although I wouldn't say this is really necessary. Or add it in. So our label is just label. And if I have a look here. So we have. I'm going to overwrite anything called gold. So OK. I'm going to have a look at that as well. So we can see a few examples of what we're actually working with. I'll just go ahead and actually rerun these as well. OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet. But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1830.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2009, 'published': '2021-12-17 14:24:40 UTC', 'start': 1830, 'text': ""OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet. But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2009,2021-12-17 14:24:40 UTC,1830,OK. So there we have our gold data. And now what we can do because we've reformatted that into a kind of data frame. We can use the sample method to randomly sample different sentences. So to do that what I will want to do is create a new data frame. So this is going to be our unlabeled silver data set. It's not going to be a silver data set. Because we don't have the labels or scores yet. But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1865.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2056, 'published': '2021-12-17 14:24:40 UTC', 'start': 1865, 'text': ""But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2056,2021-12-17 14:24:40 UTC,1865,But this is going to be where we will put them. And in here we again will have sentence one. And also sentence two. But at the moment they're empty. There's nothing in there yet. So what we need to do is actually iterate through all of the rows in here. So before that I'm just going to do from or import TQDM.auto from TQDM.auto import TQDM. And that's just a progress bar. So we can see where we are. I don't really like to wait and have no idea how long this is taking to process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1910.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2101, 'published': '2021-12-17 14:24:40 UTC', 'start': 1910, 'text': ""process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while. So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2101,2021-12-17 14:24:40 UTC,1910,process. And for sentence one in TQDM. So we have the progress bar. And I want to take a list of a set. So we're taking all the unique values in the gold data frame for sentence one. Okay so that will just loop through every single unique sentence one item in there. And I'm going to use that and I'm going to randomly sample five sentences from the other column sentence two to be paired with that sentence one. And here I'll sample the sentence two phrases that we're going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while. So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1953.8400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2151, 'published': '2021-12-17 14:24:40 UTC', 'start': 1953, 'text': ""going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while. So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2151,2021-12-17 14:24:40 UTC,1953,going to sample are going to come from the gold data of course. And we only want to sample from rows where sentence one is not equal to the current sentence one because otherwise we are possibly going to introduce duplicates. And we're going to remove duplicates anyway but let's just remove them from the sampling in the first place. So we're going to take that so all of the gold data set that where sentence one is not equal to sentence one. And what I'm going to do is just sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while. So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t1990.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2205, 'published': '2021-12-17 14:24:40 UTC', 'start': 1990, 'text': ""sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while. So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case. And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2205,2021-12-17 14:24:40 UTC,1990,sample five of those rows like that. Now from that I'm just going to extract sentence two. So the five sentence two phrases that we have there. And I'm going to convert them into a list. And now for sentence two in the sampled list that we just created I'm going to take my pairs. I'm going to append new pairs. So pairs are append and I want sentence one to be sentence one. And also sentence two is going to be equal to sentence two. Now this will take a little while. So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case. And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2038.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2257, 'published': '2021-12-17 14:24:40 UTC', 'start': 2038, 'text': ""So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case. And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2257,2021-12-17 14:24:40 UTC,2038,"So what I'm going to do is actually maybe not include the full data set here. So let me possibly just go maybe the first 500. Yeah let's go to the first 500. See how long that takes. And I will also want to just have a look at what we get from that. So yes it's much quicker. So we have sentence one. Let me remove that from there. And let's just say that top 10. So because we are taking five of sentence one every time and random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case. And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2087.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2301, 'published': '2021-12-17 14:24:40 UTC', 'start': 2087, 'text': ""random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case. And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2301,2021-12-17 14:24:40 UTC,2087,"random sampling it we can see that we have a few of those. And another thing that we might want to do is remove any duplicates. Now there probably isn't any duplicates here but we can check. So pairs equals pairs.drop duplicates. And then we'll check the length of pairs again. And also print. Let me run this again and print. Okay so there were not any duplicates anyway but it's a good idea to add that in just in case. And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2132.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2347, 'published': '2021-12-17 14:24:40 UTC', 'start': 2132, 'text': ""And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2347,2021-12-17 14:24:40 UTC,2132,"And now what I want to do is actually take the cross encoder. In fact actually let's go back to our little flowcharts. So we have now created our larger unlabeled data set. So it's good. And now we go on to predicting the labels of our cross encoder. So down here what I'm going to do is take the cross encoder code here. And what I've done is I've trained this already and I've uploaded it to the Hugin base models. So what you can do and what I can do is this. So I'm going to write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2180.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2389, 'published': '2021-12-17 14:24:40 UTC', 'start': 2180, 'text': ""write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this. And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2389,2021-12-17 14:24:40 UTC,2180,"write James Callum and it is called BERT STSB cross encoder. Okay so that's our cross encoder. And now what I want to do is use that cross encoder to create our labels. So that will create our silver data set. Now to do that I'm going to call it silver. For now I mean this isn't really the silver data set but it's fine. And what I'm going to do is create a list and I'm going to zip both of the columns from our pairs. So pairs sentence one, pairs sentence two. Pairs sentence one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this. And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2229.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2425, 'published': '2021-12-17 14:24:40 UTC', 'start': 2229, 'text': ""one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this. And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that. So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2425,2021-12-17 14:24:40 UTC,2229,"one and pairs sentence two. Okay so that will give us all of our pairs again. You can look at those. Okay so it's just like this. And what we want to do now is actually create our score. So just take the cross encoder. What did we load it as? CE.predict and we just pass in that silver data. So do that. Let's run it. It might take a moment. Okay so it's definitely taking a moment. So let me pause it. I'm going to just do let's say 10 because I already have the full data set so I can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this. And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that. So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2283.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2467, 'published': '2021-12-17 14:24:40 UTC', 'start': 2283, 'text': ""can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this. And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that. So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2467,2021-12-17 14:24:40 UTC,2283,"can show you that somewhere else. And let's have a look at what you have in those scores. So three of them. So we have an array and we have these scores. Okay so that they are our predictions, our similarity predictions for the first three. Now because they're randomly sampled a lot of these are negative. So if we go silver, say negative. I mean more. They're not relevant. So yeah we can see not particularly relevant. And that's just one must first issue with this. And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that. So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2324.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2521, 'published': '2021-12-17 14:24:40 UTC', 'start': 2324, 'text': ""And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that. So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo. And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2521,2021-12-17 14:24:40 UTC,2324,"And you can try and modify that by after creating your scores. If you oversample and got a lot of values or a lot of records and then just go ahead and remove most of the low scoring samples and keep all of your high scoring samples that will help you deal with that imbalance in your data. So what I'm going to do is I'm going to add to the labels column those scores which will not actually cover all of them because we only have 10 in here. So let me maybe multiply that. So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo. And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2374.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2561, 'published': '2021-12-17 14:24:40 UTC', 'start': 2374, 'text': ""So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo. And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look. What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2561,2021-12-17 14:24:40 UTC,2374,"So this isn't, you shouldn't do this obviously. It's just so they fit. Okay and let's have a look. Okay so we now have sense one, sense two and some labels. And what you do, although I'm not going to run this, is you would write pairs.to csv. Don't necessarily need to do this if you're running everything in the same notebook. But it's probably a good idea. So with csv, I'm going to say the silver data is a tab separated file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo. And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look. What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2407.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2603, 'published': '2021-12-17 14:24:40 UTC', 'start': 2407, 'text': ""file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo. And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look. What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold. Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2603,2021-12-17 14:24:40 UTC,2407,"file. And obviously the separator for that type of file is a tab character. And I don't want to include those. Okay and that will create the silver data file that we can train with. Which I do already have. So if we come over here, we can see that I have this file and we have all of these different sentence pairs and the scores that our encoder has assigned to that. So I'm going to close that and I'm going to go back to the demo. And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look. What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold. Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2457.2799999999997,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2639, 'published': '2021-12-17 14:24:40 UTC', 'start': 2457, 'text': ""And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look. What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold. Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame. It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2639,2021-12-17 14:24:40 UTC,2457,"And what I'm now going to do is actually, well first go back to the flow chart that we had. I'm going to cross off predict labels. And we're going to go ahead and fine tune the buy encoder on both gold and silver data. So we have the gold data. Let's have a look at what we have. Yes and the silver. I'm going to load that from file. So pd.read csv. Silver.tsv. And separator is a tab character. And let's have a look. What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold. Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame. It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2502.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2677, 'published': '2021-12-17 14:24:40 UTC', 'start': 2502, 'text': ""What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold. Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame. It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2677,2021-12-17 14:24:40 UTC,2502,"What we have. Make sure it's all loaded correctly. Looks good. Now what I'm going to do is put both those together. So all data is equal to gold.append silver. And we ignore the index. So we're going to get an index error. Sorry. True. And all data.head. Okay we can see that we hopefully now have all of the data in there. So let's check the length. Yeah so it's definitely a bigger data set now than before with just gold. Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame. It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2547.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2708, 'published': '2021-12-17 14:24:40 UTC', 'start': 2547, 'text': ""Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame. It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2708,2021-12-17 14:24:40 UTC,2547,"Okay so we now have a larger data set. We can go ahead and use that to fine tune the the buy encoder or sentence transformer. So what I'm going to do is take the code from up here. So we have this train data. And I think I've already run this before so I don't need to import the import example here. But what I want to do here is for row in all data. And what we actually want to do here is for i row in all data because this is a data frame. It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2587.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2745, 'published': '2021-12-17 14:24:40 UTC', 'start': 2587, 'text': ""It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer. So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2745,2021-12-17 14:24:40 UTC,2587,"It iterates through each row. We have row, sentence one, sentence two, and also a label. So we load them into our train data. And we can have a look at that train data. See what it looks like. Okay we see that we get all these input example objects. If you want to see what one of those has inside you can access the text like this. Should probably do that on a in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer. So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2622.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2788, 'published': '2021-12-17 14:24:40 UTC', 'start': 2622, 'text': ""in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer. So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension. And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2788,2021-12-17 14:24:40 UTC,2622,"in a new cell. So let me pull this down here. And you can also access a label to see what we what we have in there. Okay so that looks good. And we can now take that like we did before and load it into a data loader. So let me go up again and we'll copy that. Where are you? Take this. Bring it down here. And we run this. Creates our data loader. And we can move on to actually initializing the sentence transformer or by encoder and actually training it. So once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer. So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension. And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2665.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2824, 'published': '2021-12-17 14:24:40 UTC', 'start': 2665, 'text': ""once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer. So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension. And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules. And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2824,2021-12-17 14:24:40 UTC,2665,"once you run from sentence transformers. We're going to import models and we're also going to import sentence transformer. Now to initialize our sentence transformer if you've been following along with the series of videos and articles. You will know that we do something looks like this. So we're going to convert and we're going to import the sentence transformer. And we're going to import the sentence transformer. So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension. And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules. And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2696.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2860, 'published': '2021-12-17 14:24:40 UTC', 'start': 2696, 'text': ""So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension. And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules. And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2860,2021-12-17 14:24:40 UTC,2696,"So we're going to convert and that is going to be models.transformer. And here we're just loading a model from copy paste transformers. So that base in case. And we also have our pooling layer. So models again and we have pooling. And in here we want to include the dimensionality of the vectors that the pooling layer should expect. Which is just going to be vert.get word embedding dimension. And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules. And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2732.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2900, 'published': '2021-12-17 14:24:40 UTC', 'start': 2732, 'text': ""And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules. And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2900,2021-12-17 14:24:40 UTC,2732,"And also it needs to know what type of pooling we're going to use. Are we going to use CLS pooling? Are we going to use mean pooling, max pooling or so on. Now we are going to use pooling and we're going to use a mean. So mode mean tokens. Let me set that to true. So there are the two let's say components in our sentence transformer. And we need to now put those together. So we're going to call model equals sentence transformer. And we write modules. And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different.",Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2772.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2938, 'published': '2021-12-17 14:24:40 UTC', 'start': 2772, 'text': ""And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different. And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2938,2021-12-17 14:24:40 UTC,2772,And then we just pass as a list vert and also pooling. Okay. So we run that. We can also have a look at what our model looks like. Okay. And we have a sentence transformer object. And inside there we have two layers or components. First one is our transformer. It's a vert model. And the second one is our pooling. And we can see here the only pooling method that is set to true is the mode mean tokens. Which means we're going to take the mean across all the word embeddings output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different. And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2806.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 2977, 'published': '2021-12-17 14:24:40 UTC', 'start': 2806, 'text': ""output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different. And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does. Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,2977,2021-12-17 14:24:40 UTC,2806,output by vert and use that to create our sentence embedding or vector. So with that model now defined we can initialize our loss function. So we do want to write from sentence transformers dot losses import cosine similarity loss. So cosine similarity loss. And in here we need to pass the model so it understands which parameters to actually optimize. And initialize that. And then we sell our training function or the fit function. And that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different. And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does. Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2849.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3024, 'published': '2021-12-17 14:24:40 UTC', 'start': 2849, 'text': ""that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different. And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does. Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well. Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3024,2021-12-17 14:24:40 UTC,2849,that's similar to before the cross encoder although slightly different. So let me take that. That's a little further up from here. Then take that and we're just going to modify it. So warm up. I'm going to warm up for 15% of the number of steps that we're going to run through. We change this to model. It's not C anymore. And like I said there are some differences here. So we have a training objectives. That's different. And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does. Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well. Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2885.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3063, 'published': '2021-12-17 14:24:40 UTC', 'start': 2885, 'text': ""And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does. Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well. Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine. So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3063,2021-12-17 14:24:40 UTC,2885,And this is just a list of all the training objectives we have. We are only using one. And we just pass loader and loss into that. Evaluator. We could use an evaluator. I'm not going to. For this one I'm going to evaluate everything afterwards. The epochs and warm steps are the same. The only thing that's different is the output path which is going to be vert stsp.org. That's it. So go ahead and run that. It should run. Let's check that it does. Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well. Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine. So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2925.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3105, 'published': '2021-12-17 14:24:40 UTC', 'start': 2925, 'text': ""Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well. Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine. So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3105,2021-12-17 14:24:40 UTC,2925,Okay so I've got this error here. So it's lucky that we checked. And this runtime error found dtype long but expected to float. And if we come up here it's going to be in the data loader or in the data that we've initialized. So here I've put int for some reason. I'm not sure why that is. So this should be a float. The label in your training data. And that should be the same up here as well. Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine. So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t2956.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3144, 'published': '2021-12-17 14:24:40 UTC', 'start': 2956, 'text': ""Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine. So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3144,2021-12-17 14:24:40 UTC,2956,Okay so here as well the cross encoder. We would expect a float value. So just be aware that I'll make sure there's a note in the video earlier on for that. Okay and okay let's continue through that and try and rerun it. Should be okay now. Oh I need to actually rerun everything else as well. So rerun this. Okay label 1.0. Okay it's better. This is this. I'll just leave this for a moment. Just to be sure that is actually running this time. But it does look good. So yeah that's fine. So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t3012.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3195, 'published': '2021-12-17 14:24:40 UTC', 'start': 3012, 'text': ""So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model"", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3195,2021-12-17 14:24:40 UTC,3012,So it looks good. When for some reason in the notebook I'm actually seeing the number of iterations. But okay yeah pause it now and we can see that it's actually running. I'm actually seeing the number of iterations. But okay pause it now and we can see that yes it did run through two iterations. So it is running correctly now. That's good. So that's great. What I want to do now is actually show you okay evaluation of these models. So back to our flow chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t3047.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3240, 'published': '2021-12-17 14:24:40 UTC', 'start': 3047, 'text': ""chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model correlate to the actual scores with a 50% correlation. So they do correlate. It's not bad. It's not great either. Let's have a look at the cross encoder. So again cross encoder. Okay and we get score of 0.58. So as we would expect training on just the gold data the cross encoder does outperform the the by encoder or sentence transformer. And the final one would be okay with the augmented data how does the sentence transformer perform."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3240,2021-12-17 14:24:40 UTC,3047,chart quickly. Okay so fine tune by encoder. We've just done it. So we've now finished with our in the main augmented expert training strategy. And yeah let's move on to the evaluation. Okay so my evaluation script here is maybe not the easiest to read. But basically all we're doing is we're importing the embedding similarity evaluated from down here. I'm loading the the glue data. SDSP again and we're taking the validation split which we didn't train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model correlate to the actual scores with a 50% correlation. So they do correlate. It's not bad. It's not great either. Let's have a look at the cross encoder. So again cross encoder. Okay and we get score of 0.58. So as we would expect training on just the gold data the cross encoder does outperform the the by encoder or sentence transformer. And the final one would be okay with the augmented data how does the sentence transformer perform.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t3088.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3277, 'published': '2021-12-17 14:24:40 UTC', 'start': 3088, 'text': ""train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model correlate to the actual scores with a 50% correlation. So they do correlate. It's not bad. It's not great either. Let's have a look at the cross encoder. So again cross encoder. Okay and we get score of 0.58. So as we would expect training on just the gold data the cross encoder does outperform the the by encoder or sentence transformer. And the final one would be okay with the augmented data how does the sentence transformer perform. So let's run that again. Not a way to download. And we get a much better score of 0.69. So yeah the correlation there is much higher than this for the augmented data set than if we had just used a gold data set. So it really really has improved the performance a lot. Now this is maybe an atypical performance increase. It's something like 90% or 19 point increase in performance and that's good. But if you look at the the original paper from Nils Reimers and Co."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3277,2021-12-17 14:24:40 UTC,3088,train on. We are converting it into input examples feeding it into our embedding similarity evaluator. And loading the model. The model name I pass through some command line arguments from up here. And then it just prints out the score. So let me switch across to the command line. We can see how that actually performs. Okay so just switch across to my other desktop because this is much faster. So I can actually run this quickly. So python and 03. So we're going to run that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model correlate to the actual scores with a 50% correlation. So they do correlate. It's not bad. It's not great either. Let's have a look at the cross encoder. So again cross encoder. Okay and we get score of 0.58. So as we would expect training on just the gold data the cross encoder does outperform the the by encoder or sentence transformer. And the final one would be okay with the augmented data how does the sentence transformer perform. So let's run that again. Not a way to download. And we get a much better score of 0.69. So yeah the correlation there is much higher than this for the augmented data set than if we had just used a gold data set. So it really really has improved the performance a lot. Now this is maybe an atypical performance increase. It's something like 90% or 19 point increase in performance and that's good. But if you look at the the original paper from Nils Reimers and Co.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
3IPCEeh4xTg-t3129.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 3318, 'published': '2021-12-17 14:24:40 UTC', 'start': 3129, 'text': ""that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model correlate to the actual scores with a 50% correlation. So they do correlate. It's not bad. It's not great either. Let's have a look at the cross encoder. So again cross encoder. Okay and we get score of 0.58. So as we would expect training on just the gold data the cross encoder does outperform the the by encoder or sentence transformer. And the final one would be okay with the augmented data how does the sentence transformer perform. So let's run that again. Not a way to download. And we get a much better score of 0.69. So yeah the correlation there is much higher than this for the augmented data set than if we had just used a gold data set. So it really really has improved the performance a lot. Now this is maybe an atypical performance increase. It's something like 90% or 19 point increase in performance and that's good. But if you look at the the original paper from Nils Reimers and Co. they found a sort of expected performance increase of I believe seven or nine points. So this is definitely pretty significant. This is definitely a bit more than that. But I think it goes to show how good these models or this training strategy can actually be. So that's it for this video. I hope this has been useful and I hope this helps a few of you kind of overcome the sometimes lack of data that we find in I think a lot of our particular use cases."", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}",UCv83tO5cePwHMt1952IVVHw,3318,2021-12-17 14:24:40 UTC,3129,that evaluation script. And what we're going to pass here is we have all three. All three the cross encoder the sentence transformer train using augmented expert and also a sentence transformer trained purely on the gold data set. So first let's have a look at the STSB gold data set trained model. So run this. It might take a moment to download it. Okay so everything's downloaded and then we've got a score of 0.506. So it correlates to the predictions of the model correlate to the actual scores with a 50% correlation. So they do correlate. It's not bad. It's not great either. Let's have a look at the cross encoder. So again cross encoder. Okay and we get score of 0.58. So as we would expect training on just the gold data the cross encoder does outperform the the by encoder or sentence transformer. And the final one would be okay with the augmented data how does the sentence transformer perform. So let's run that again. Not a way to download. And we get a much better score of 0.69. So yeah the correlation there is much higher than this for the augmented data set than if we had just used a gold data set. So it really really has improved the performance a lot. Now this is maybe an atypical performance increase. It's something like 90% or 19 point increase in performance and that's good. But if you look at the the original paper from Nils Reimers and Co. they found a sort of expected performance increase of I believe seven or nine points. So this is definitely pretty significant. This is definitely a bit more than that. But I think it goes to show how good these models or this training strategy can actually be. So that's it for this video. I hope this has been useful and I hope this helps a few of you kind of overcome the sometimes lack of data that we find in I think a lot of our particular use cases.,Making The Most of Data: Augmented SBERT,https://youtu.be/3IPCEeh4xTg
DgGFhQmfxHo-t22.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 139, 'published': '2020-11-22 20:29:27 UTC', 'start': 22, 'text': ""You can try and import the kaggle module. And you will get this error here. So this error simply tells you that you could not find the kaggle.json and you need to add it to this location here. Now the reason it's telling you this is because we use kaggle.json to authenticate our API access. Obviously kaggle is not going to let anyone access their API. You need to have a account before you start downloading their data. So to get our kaggle.json credentials we simply go over to kaggle.com. Now if you don't have an account you'll have to go ahead and create one. Once you've created your account you simply go over to this little icon over here in the top right. Click account. And scroll down until you see this API section. Now all you need to do is create a new API token. And this creates the kaggle.json credentials and allows me to save them to my computer. So I'm just going to save them in my documents for now. And then head back to the notebook. And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,139,2020-11-22 20:29:27 UTC,22,You can try and import the kaggle module. And you will get this error here. So this error simply tells you that you could not find the kaggle.json and you need to add it to this location here. Now the reason it's telling you this is because we use kaggle.json to authenticate our API access. Obviously kaggle is not going to let anyone access their API. You need to have a account before you start downloading their data. So to get our kaggle.json credentials we simply go over to kaggle.com. Now if you don't have an account you'll have to go ahead and create one. Once you've created your account you simply go over to this little icon over here in the top right. Click account. And scroll down until you see this API section. Now all you need to do is create a new API token. And this creates the kaggle.json credentials and allows me to save them to my computer. So I'm just going to save them in my documents for now. And then head back to the notebook. And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t52.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 179, 'published': '2020-11-22 20:29:27 UTC', 'start': 52, 'text': ""You need to have a account before you start downloading their data. So to get our kaggle.json credentials we simply go over to kaggle.com. Now if you don't have an account you'll have to go ahead and create one. Once you've created your account you simply go over to this little icon over here in the top right. Click account. And scroll down until you see this API section. Now all you need to do is create a new API token. And this creates the kaggle.json credentials and allows me to save them to my computer. So I'm just going to save them in my documents for now. And then head back to the notebook. And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell. And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,179,2020-11-22 20:29:27 UTC,52,You need to have a account before you start downloading their data. So to get our kaggle.json credentials we simply go over to kaggle.com. Now if you don't have an account you'll have to go ahead and create one. Once you've created your account you simply go over to this little icon over here in the top right. Click account. And scroll down until you see this API section. Now all you need to do is create a new API token. And this creates the kaggle.json credentials and allows me to save them to my computer. So I'm just going to save them in my documents for now. And then head back to the notebook. And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell. And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t81.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 206, 'published': '2020-11-22 20:29:27 UTC', 'start': 81, 'text': ""And scroll down until you see this API section. Now all you need to do is create a new API token. And this creates the kaggle.json credentials and allows me to save them to my computer. So I'm just going to save them in my documents for now. And then head back to the notebook. And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell. And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it. Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,206,2020-11-22 20:29:27 UTC,81,And scroll down until you see this API section. Now all you need to do is create a new API token. And this creates the kaggle.json credentials and allows me to save them to my computer. So I'm just going to save them in my documents for now. And then head back to the notebook. And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell. And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it. Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t110.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 226, 'published': '2020-11-22 20:29:27 UTC', 'start': 110, 'text': ""And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell. And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it. Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition. We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,226,2020-11-22 20:29:27 UTC,110,And we're going to see that we need to save it here. So I'm going to copy and paste that across. And here we have the directory that we need to put our kaggle.json. I'm going to take my kaggle.json and simply move it into here. OK so to check that it's worked we simply rerun this cell. And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it. Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition. We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t130.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 247, 'published': '2020-11-22 20:29:27 UTC', 'start': 130, 'text': ""And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it. Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition. We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition. And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,247,2020-11-22 20:29:27 UTC,130,And there we can see that our kaggle API is now functional. Now we don't actually need this import kaggle. Instead we need to import the kaggle API class from the kaggle API extended module. Once we've imported that we simply initialize our API. And then authenticate it. Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition. We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition. And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t171.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 268, 'published': '2020-11-22 20:29:27 UTC', 'start': 171, 'text': ""Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition. We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition. And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova. And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,268,2020-11-22 20:29:27 UTC,171,Now we're ready to start downloading datasets. And the kaggle API gives us several options for doing this. The two that you're most likely to use are for downloading the competition datasets or standalone datasets. Now a competition dataset is related to a current or past competition. So for example there is a sentiment analysis on movie reviews competition. We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition. And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova. And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t198.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 303, 'published': '2020-11-22 20:29:27 UTC', 'start': 198, 'text': ""We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition. And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova. And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these. We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,303,2020-11-22 20:29:27 UTC,198,We can actually find it over here. And you can see here in the URL kaggle.com is followed by this C. And this C essentially means that this is a competition. And we can also see playground prediction competition. Everything is telling us that this is a competition. And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova. And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these. We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t216.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 340, 'published': '2020-11-22 20:29:27 UTC', 'start': 216, 'text': ""And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova. And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these. We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset. So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,340,2020-11-22 20:29:27 UTC,216,And in this competition it comes with some data. Now this is different to a standalone dataset. And these standalone datasets can simply be uploaded by anyone. So if we go to sentiment 140 dataset here. You look in the URL and you can see that this dataset has been uploaded by Casanova. And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these. We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset. So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t241.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 378, 'published': '2020-11-22 20:29:27 UTC', 'start': 241, 'text': ""And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these. We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset. So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets. We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,378,2020-11-22 20:29:27 UTC,241,And there is a slightly different structure to the dataset page as well. And you can see here it's a dataset. First tab takes us to data. And we can scroll down and see the data that we can get here. So there are two different methods for downloading each one of these. We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset. So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets. We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t258.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 401, 'published': '2020-11-22 20:29:27 UTC', 'start': 258, 'text': ""We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset. So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets. We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140. We also need to specify the file name. Which in this case is this text here. And then we just execute that. And now we can see that we have downloaded both files here. Now you will notice that both of these files are actually zipped."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,401,2020-11-22 20:29:27 UTC,258,We can't download competition datasets with the standalone dataset method. And we can't download standalone datasets with the competition dataset method. So we'll start with the competition dataset. And to download one of these all we need to do is use the competition download file method. And then we need to pass the competition name followed by the dataset. So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets. We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140. We also need to specify the file name. Which in this case is this text here. And then we just execute that. And now we can see that we have downloaded both files here. Now you will notice that both of these files are actually zipped.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t291.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 440, 'published': '2020-11-22 20:29:27 UTC', 'start': 291, 'text': ""So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets. We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140. We also need to specify the file name. Which in this case is this text here. And then we just execute that. And now we can see that we have downloaded both files here. Now you will notice that both of these files are actually zipped. So we can just quickly unzip them using Python. All we need to do is import zip file. And with zip file. We specify the path to the data. Which in this case is just the file name."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,440,2020-11-22 20:29:27 UTC,291,So head back over here. And you can see the competition name is this. And the data that we would like is train.tsv.zip. And that is downloaded into our current directory. You can see it here. Okay so that's how we download the competition datasets. We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140. We also need to specify the file name. Which in this case is this text here. And then we just execute that. And now we can see that we have downloaded both files here. Now you will notice that both of these files are actually zipped. So we can just quickly unzip them using Python. All we need to do is import zip file. And with zip file. We specify the path to the data. Which in this case is just the file name.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
DgGFhQmfxHo-t327.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 478, 'published': '2020-11-22 20:29:27 UTC', 'start': 327, 'text': ""We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140. We also need to specify the file name. Which in this case is this text here. And then we just execute that. And now we can see that we have downloaded both files here. Now you will notice that both of these files are actually zipped. So we can just quickly unzip them using Python. All we need to do is import zip file. And with zip file. We specify the path to the data. Which in this case is just the file name. And we specify that we are simply reading it. And then we simply call the extract all method. And we have our dataset here. And we see everything is in the right format. So that's everything for this tutorial on using the Kaggle API."", 'title': 'How-to use the Kaggle API in Python', 'url': 'https://youtu.be/DgGFhQmfxHo'}",UCv83tO5cePwHMt1952IVVHw,478,2020-11-22 20:29:27 UTC,327,We can also download the standalone datasets. To do so we use the dataset download file method. And then here we need to pass the username followed by the dataset name. So if we head over here. You can find both in the URL. So this one is Casanova slash Sentiment 140. We also need to specify the file name. Which in this case is this text here. And then we just execute that. And now we can see that we have downloaded both files here. Now you will notice that both of these files are actually zipped. So we can just quickly unzip them using Python. All we need to do is import zip file. And with zip file. We specify the path to the data. Which in this case is just the file name. And we specify that we are simply reading it. And then we simply call the extract all method. And we have our dataset here. And we see everything is in the right format. So that's everything for this tutorial on using the Kaggle API.,How-to use the Kaggle API in Python,https://youtu.be/DgGFhQmfxHo
MF75aNH3Gjs-t20.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 180, 'published': '2021-10-07 14:52:32 UTC', 'start': 20, 'text': ""but we're going to go through the essentials and we're going to step by step put an API together that is very similar to this. Now, what this API is doing is pulling information from these two CSV files and either presenting that information to us, that data, allowing us to modify, allowing us to add new entries or allowing us to delete it. So, we get to use get, post, put and delete methods. Now, if you are following along with this tutorial, you'll probably want this data so you can get it using this code over here. So, over here we have, I'll make sure there's a link to this in the description so you can get it. All we're doing is downloading the code or those two data files from here and then storing it in a local directory. I mean, you can change that directory to whatever you want, of course. Inside those two files, we have locations here, which is just a list of, I think these are actually real coffee cafes, but I put this, I made this CSV quite a long time ago, so I'm not 100% sure. Then over here, we just have some made up user data as well. And the end result will be something like this. So, this is a program called Insomnia. I'll leave a download link to that in the description, but it just allows us to send API requests very easily. So, I'm going to send an API request to this address here, which is the API I just showed you. And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,180,2021-10-07 14:52:32 UTC,20,"but we're going to go through the essentials and we're going to step by step put an API together that is very similar to this. Now, what this API is doing is pulling information from these two CSV files and either presenting that information to us, that data, allowing us to modify, allowing us to add new entries or allowing us to delete it. So, we get to use get, post, put and delete methods. Now, if you are following along with this tutorial, you'll probably want this data so you can get it using this code over here. So, over here we have, I'll make sure there's a link to this in the description so you can get it. All we're doing is downloading the code or those two data files from here and then storing it in a local directory. I mean, you can change that directory to whatever you want, of course. Inside those two files, we have locations here, which is just a list of, I think these are actually real coffee cafes, but I put this, I made this CSV quite a long time ago, so I'm not 100% sure. Then over here, we just have some made up user data as well. And the end result will be something like this. So, this is a program called Insomnia. I'll leave a download link to that in the description, but it just allows us to send API requests very easily. So, I'm going to send an API request to this address here, which is the API I just showed you. And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t54.480000000000004,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 230, 'published': '2021-10-07 14:52:32 UTC', 'start': 54, 'text': ""following along with this tutorial, you'll probably want this data so you can get it using this code over here. So, over here we have, I'll make sure there's a link to this in the description so you can get it. All we're doing is downloading the code or those two data files from here and then storing it in a local directory. I mean, you can change that directory to whatever you want, of course. Inside those two files, we have locations here, which is just a list of, I think these are actually real coffee cafes, but I put this, I made this CSV quite a long time ago, so I'm not 100% sure. Then over here, we just have some made up user data as well. And the end result will be something like this. So, this is a program called Insomnia. I'll leave a download link to that in the description, but it just allows us to send API requests very easily. So, I'm going to send an API request to this address here, which is the API I just showed you. And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask. We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,230,2021-10-07 14:52:32 UTC,54,"following along with this tutorial, you'll probably want this data so you can get it using this code over here. So, over here we have, I'll make sure there's a link to this in the description so you can get it. All we're doing is downloading the code or those two data files from here and then storing it in a local directory. I mean, you can change that directory to whatever you want, of course. Inside those two files, we have locations here, which is just a list of, I think these are actually real coffee cafes, but I put this, I made this CSV quite a long time ago, so I'm not 100% sure. Then over here, we just have some made up user data as well. And the end result will be something like this. So, this is a program called Insomnia. I'll leave a download link to that in the description, but it just allows us to send API requests very easily. So, I'm going to send an API request to this address here, which is the API I just showed you. And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask. We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t93.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 263, 'published': '2021-10-07 14:52:32 UTC', 'start': 93, 'text': ""I think these are actually real coffee cafes, but I put this, I made this CSV quite a long time ago, so I'm not 100% sure. Then over here, we just have some made up user data as well. And the end result will be something like this. So, this is a program called Insomnia. I'll leave a download link to that in the description, but it just allows us to send API requests very easily. So, I'm going to send an API request to this address here, which is the API I just showed you. And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask. We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,263,2021-10-07 14:52:32 UTC,93,"I think these are actually real coffee cafes, but I put this, I made this CSV quite a long time ago, so I'm not 100% sure. Then over here, we just have some made up user data as well. And the end result will be something like this. So, this is a program called Insomnia. I'll leave a download link to that in the description, but it just allows us to send API requests very easily. So, I'm going to send an API request to this address here, which is the API I just showed you. And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask. We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t127.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 312, 'published': '2021-10-07 14:52:32 UTC', 'start': 127, 'text': ""And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask. We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,312,2021-10-07 14:52:32 UTC,127,"And it's a user's endpoint. We will have two endpoints, which are essentially just separations of the API. So, users and that's going to return all the user's data. And then we also have locations. And we'll send that as well. And we return all the locations. Now, we're going to start from scratch. And the first thing that we'll need to do is import everything we need. So, we will need to import Flask. So, from Flask, import Flask. We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t159.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 357, 'published': '2021-10-07 14:52:32 UTC', 'start': 159, 'text': ""We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier, we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,357,2021-10-07 14:52:32 UTC,159,"We'll also need to import Flask RESTful. So, this is another Flask library that gives us a few very useful tools for building an API. So, right from Flask RESTful, import resource API and rec pass. Now, specific to what we're doing here, we will also want to import pandas and import AST. Now, you will probably have to pip install these as well. So, the pip for those is pip install Flask and Flask RESTful. Nothing weird there. All right. So, the first thing we need to do is initialize our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier, we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t209.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 405, 'published': '2021-10-07 14:52:32 UTC', 'start': 209, 'text': ""our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier, we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,405,2021-10-07 14:52:32 UTC,209,"our Flask app and then initialize our Flask API. So, we do that. So, this is typical Flask here. So, we write Flask name and then the API is API equals API app. Okay. Super easy. Now, we already touched on it, but our app is going to have two different endpoints. So, we're going to have the users endpoints and we're also going to have the locations. Endpoints. And we're going to go through building the users endpoint, but you'll be able to find the code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier, we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t249.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 454, 'published': '2021-10-07 14:52:32 UTC', 'start': 249, 'text': ""code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier, we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,454,2021-10-07 14:52:32 UTC,249,"code for both of those in the description. But the way that we separate both of these is we use a class object. So, we're going to create our use class first. So, we write class users. And then to initialize this as an endpoint for our API, we actually need to pass the resource object into it. That will inherit the resource object and expose it to different like HTTP requests like get, post and so on. For now, we're not going to add anything in there, but to make it easier, we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t290.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 496, 'published': '2021-10-07 14:52:32 UTC', 'start': 290, 'text': ""we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is, this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,496,2021-10-07 14:52:32 UTC,290,"we need to write API add resource. And then we need to specify the web address of this or the, or the, it's almost like a page of a web address that the resource or endpoint will be. So, we'll say users, which is our class and then the actual endpoint location, which is users. Okay. So, this is saying I want you to map the class users here to this place in our API. So, our web address. So, if for example, the user is a user, then we're going to pass the API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is, this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t338.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 528, 'published': '2021-10-07 14:52:32 UTC', 'start': 338, 'text': ""API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is, this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here. Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,528,2021-10-07 14:52:32 UTC,338,"API. So, our web address. So, if for example, the API was located at, let's say, API.com, this resource would be located at API.com slash users. Okay. And we'll also create another one, although we're not going to fill it out in this video, that will be called locations. So, again, exactly the same thing. Yeah. Resource, we'll just pass for now and we'll copy that and do the same for locations. Okay. Now, that's sort of the structure or the very high level structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is, this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here. Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t388.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 575, 'published': '2021-10-07 14:52:32 UTC', 'start': 388, 'text': ""structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is, this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here. Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there. We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,575,2021-10-07 14:52:32 UTC,388,"structure of our API itself. So, what I like to do is whilst we are building or writing the code for our API, I like to also test it as we go along. I think it makes things a lot easier. So, to run our API, we need to write this. If name is equal to main, we write app.run. Okay. It's that easy. And now what we can do is just press the execute button up here. Okay. Okay. So, we got this error because I, okay, I don't know why I did that. So, this is, this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here. Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there. We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t438.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 611, 'published': '2021-10-07 14:52:32 UTC', 'start': 438, 'text': ""this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here. Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there. We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So, to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,611,2021-10-07 14:52:32 UTC,438,"this needs to be the class, not a string of the class. Okay. So, now let's run that again. It should work. Okay. So, we get this. This is a development server. Do not use it in production environment. That's fine because we're just testing it. And then we can see this is running at this address. Okay. So, if we take this, we'll copy it and we're going to take it over into Insomnia. Okay. And I'll just clear everything that I was doing before. And we'll go into here. Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there. We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So, to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t478.40000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 653, 'published': '2021-10-07 14:52:32 UTC', 'start': 478, 'text': ""Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there. We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So, to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example, had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,653,2021-10-07 14:52:32 UTC,478,"Okay. And if we send this, we won't return anything yet. So, we'll just get this not found. But if we go to users, we should at least return this internal server error. So, this means that, okay, we are not getting anything back because there's an error on server side. So, in our API, but there is something there. So, before when we just had this, so remember we have those two endpoints. We're not actually using the base URL. So, there's not actually anything there. We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So, to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example, had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t516.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 687, 'published': '2021-10-07 14:52:32 UTC', 'start': 516, 'text': ""We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So, to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example, had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,687,2021-10-07 14:52:32 UTC,516,"We get this not found because there just isn't anything there. But if we type in the locations or users, we will return this internal server error. Now, we're getting this because we haven't written any code. So, it doesn't know what to do when we send that request. So, let's go back to our code and we'll start writing something. So, first thing I want to do is we'll define a get request. So, write find get bus self. And in here, what I want to do is load the users CSV. So, to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example, had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t556.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 723, 'published': '2021-10-07 14:52:32 UTC', 'start': 556, 'text': ""to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example, had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check. So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,723,2021-10-07 14:52:32 UTC,556,"to get that, I'm going to write users path. So, this is the specific directory that I have stored my users data. Of course, it might be different for you. And what I'm going to do is down here, I'm going to use pandas to just read in that data. Whenever I call this get request, I'm going to read it in. So, this is very similar to if we, for example, had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check. So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t597.6800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 763, 'published': '2021-10-07 14:52:32 UTC', 'start': 597, 'text': ""had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check. So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything. So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,763,2021-10-07 14:52:32 UTC,597,"had a database on a server somewhere. When the API gets a request, it would read that data quickly. And return it to us. So, this is a very simple version of that. So, I'm going to do data equals pd.read CSV. And we just go users path. And hopefully that's the right path. I think it is. We can see. So, I come up to here. Yes, we have data. So, I guess that should be fine. Yes, that should be fine. Users CSV. Cool. And then what we want to do, we can't pass a we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check. So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything. So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t638.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 797, 'published': '2021-10-07 14:52:32 UTC', 'start': 638, 'text': ""we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check. So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything. So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So, see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,797,2021-10-07 14:52:32 UTC,638,"we can't pass a pandas, a data frame object through an API. So, we need to convert this into a dictionary. So, write data.toDate. Now, what we want to do is return. We want to return the data. So, we're going to write data, which is equal to data, our dictionary. And we're going to return the 200 code, which means the API request was successful. Here's your data. Okay, I'll save that. And I believe it should, should update automatically. Let me check. So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything. So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So, see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t677.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 835, 'published': '2021-10-07 14:52:32 UTC', 'start': 677, 'text': ""So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything. So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So, see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request, I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,835,2021-10-07 14:52:32 UTC,677,"So, come over here. Let's send that again to users. Okay, no. So, I think we need, we need to restart the API. So, come over here. Here, I'm just going to control C and run it again. Now, you can, you can turn debug mode on. So, I think that will automatically reload everything. So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So, see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request, I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t710.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 892, 'published': '2021-10-07 14:52:32 UTC', 'start': 710, 'text': ""So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So, see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request, I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,892,2021-10-07 14:52:32 UTC,710,"So, to let's do that first. So, down here in app run, we're just going to set debug equals true. Save that. I'm going to press control C down here and execute again. And now, when we make changes, it should reload automatically. So, come over here. Let's send that request again. Now, we see user path is not defined. So, see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request, I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t741.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 940, 'published': '2021-10-07 14:52:32 UTC', 'start': 741, 'text': ""see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request, I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things. Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,940,2021-10-07 14:52:32 UTC,741,"see why that is wrong. So, it's user's path. So, I'll save that and that should reload automatically. Let's try again. Okay, perfect. So, now we get a 200 response there. We get all of our users data. Okay, so we're just returning that data frame in addition to the data frame. Okay, cool. So, that's how get requests. Let's move on to, let's put requests, I think. So, adding more data. Okay, so this time, what we're going to need to do is, so using post request, I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things. Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t783.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 990, 'published': '2021-10-07 14:52:32 UTC', 'start': 783, 'text': ""I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things. Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,990,2021-10-07 14:52:32 UTC,783,"I said put a second ago on and post. And in here, the first thing we need to do is, we need to retrieve information from the user that they want to upload. So, the first thing we need to think about here is we are trying to get data from the user. So, they are going to send us like a new entry. So, what did we have before in our users? We have user ID, name, city, locations. Okay, so we want to use it to be able to add a new row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things. Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t820.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1034, 'published': '2021-10-07 14:52:32 UTC', 'start': 820, 'text': ""row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things. Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1034,2021-10-07 14:52:32 UTC,820,"row. So, each one uses essentially a row in a data frame. So, we have locations, city, name, and user ID. So, we're going to send this to the user. So, we're going to send this to the user. So, name and user ID. Now, we want the user to be able to add locations or location, locations, name and city. Now the user ID, we don't really want the user isn't going to add that, that's going to be generated automatically. So we ignore that, we do want the user to be able to pass those three things. Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t872.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1081, 'published': '2021-10-07 14:52:32 UTC', 'start': 872, 'text': ""Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1081,2021-10-07 14:52:32 UTC,872,"Now to do that we're going to use the recparser, so request parse that we use up here. And to initialize that we need to write parser equals request, sorry what is it up here, recparser, recpass.requestParser here. That initializes our parser and what this is going to do is when we add arguments into the request this is going to read them and what we're going to do is parse them out into variables. So we are going to within this request we want to allow those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t919.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1141, 'published': '2021-10-07 14:52:32 UTC', 'start': 919, 'text': ""those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1141,2021-10-07 14:52:32 UTC,919,"those three, so location, name and city. So write parser add argument, this one will be location and we're just going to do one location at a time. So do location ID and then of course we use a put request later if they want to add more locations. You say required is true, they do need to add one of these and the type of this is a integer so we'll write int. Okay we need to add another one, I'm just going to copy this so let's copy here and we have name city and the type is for both of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t974.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1196, 'published': '2021-10-07 14:52:32 UTC', 'start': 974, 'text': ""of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1196,2021-10-07 14:52:32 UTC,974,"of those strings which I think might be the default so we probably don't even need to include that type string there but it is there so leave it. So now we have the three arguments that I use you can use to post information to our API and what we need to do to extract whatever the user has sent us is we do this we're going to extract them into this dictionary here which is called args and we write parser, parse args like that. Okay so now we have those arguments and let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1013.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1245, 'published': '2021-10-07 14:52:32 UTC', 'start': 1013, 'text': ""let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1245,2021-10-07 14:52:32 UTC,1013,let's first just make sure this is working so I want to just I'm going to return what we just tried to send that so let's return location lock equals args location id name and the city. Okay so let's just make sure it's actually working and we'll set 200 save it and that should reload automatically. Let's go to insomnia and we are sending a post request here so post we have users we enter this question mark so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists.,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1064.5600000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1280, 'published': '2021-10-07 14:52:32 UTC', 'start': 1064, 'text': ""so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists. Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1280,2021-10-07 14:52:32 UTC,1064,so this allows us to start adding our parameters so location id really going to be equal to I'm going to say five and so we we added the ampersand symbol there to add another parameter so location id and what are the other ones user user or name James it's me and for the city oh let's put okay send that cool and then we we're just returning that request back to azure you notice here we put lock and not location id so we can tell it's not just returning what we sent it's returning what we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists. Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id.,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1116.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1320, 'published': '2021-10-07 14:52:32 UTC', 'start': 1116, 'text': ""we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists. Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id. So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1320,2021-10-07 14:52:32 UTC,1116,we have processed through our function and return back to ourselves. Okay cool so now we know we're parsing those arguments correctly now let's use them. So again what I want to do is read our data so we do data equals data pandas dot read csv again and we're reading the user's path. Okay so user's path and we're doing users here so actually do we want location in there at all I don't know. Yeah maybe so no let's let's say the user can specify that user id oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists. Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id. So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1182.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1358, 'published': '2021-10-07 14:52:32 UTC', 'start': 1182, 'text': ""oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists. Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id. So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1358,2021-10-07 14:52:32 UTC,1182,oh we're not returning anymore so let's remove that so I want the user to I want to be able to post this because then if we have a duplicate of a previous user id we're not we can say we can check if that it already exists in the data or not now which we'll do now so right if args user id in the data that we just opened so in data is it user id I think I'm sure I can speak it okay I think it is. I want to return an error message saying this already exists. Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id. So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1226.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1391, 'published': '2021-10-07 14:52:32 UTC', 'start': 1226, 'text': ""Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id. So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1391,2021-10-07 14:52:32 UTC,1226,Let's write message and all we do is write args user id already exists and what we'll return here is the 409 code which indicates that there's some sort of conflict and there is a conflict we already this user id already exists so we're saying there's conflict you can't you can't create a new one it's already there if it doesn't already exist that's great and we go ahead and we create that new user id. So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1264.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1429, 'published': '2021-10-07 14:52:32 UTC', 'start': 1264, 'text': ""So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1429,2021-10-07 14:52:32 UTC,1264,So all we do is write data equals data dot append and then we're just appending the new data that we've retrieved from our args so it's going to be we need user id which is going to be args user id let me copy this make it a little bit quicker for locations it's just empty for now of city and we have name okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1304.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1462, 'published': '2021-10-07 14:52:32 UTC', 'start': 1304, 'text': ""okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1462,2021-10-07 14:52:32 UTC,1304,okay and then we need to I think we need to ignore yes ignore index true true sorry okay and then we'll save that data so to csv it is is it uses path and I'm going to set index equals false so we're not saving the index the row numbers and then I just want to return want to return the data again we can't return a data frame we need to convert it into a dictionary so write to dict and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1343.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1501, 'published': '2021-10-07 14:52:32 UTC', 'start': 1343, 'text': ""and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1501,2021-10-07 14:52:32 UTC,1343,and it's a 200 response I'm going to save that let's test it see if it if it works so I'm going to let's see modify this to user id now send that internal server error so let's see date frame no attribute to date so I need to add an underscore save again and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1376.6399999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1524, 'published': '2021-10-07 14:52:32 UTC', 'start': 1376, 'text': ""and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1524,2021-10-07 14:52:32 UTC,1376,and let's send that again okay so now we can see we have more items in there the only thing is I've added five for all of them oh did I it's in the code right what did I do yeah very smart okay so change that name and this is city and one other thing is that these are all this should also be a string one thing I notice and this should also be a string I'm going to leave that out it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type,API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1413.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1541, 'published': '2021-10-07 14:52:32 UTC', 'start': 1413, 'text': ""it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1541,2021-10-07 14:52:32 UTC,1413,"it's fine so let me send that again five already exists because so at least we know that our conflict code works so let's create another one number six I'm going to call this no Jim and let's go again okay so now we've created number six as well did we create gosh we created a few there didn't we oops so yeah now we've now we've created number six here which is Jim lives in London his user id is six okay so I mean formatting isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1453.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1569, 'published': '2021-10-07 14:52:32 UTC', 'start': 1453, 'text': ""isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1569,2021-10-07 14:52:32 UTC,1453,"isn't perfect but that's good for now we've created our first post request okay so it's it's pretty annoying that we have to do this again so let's go ahead and build that so we need to first define a delete function method and all we want to do is delete the delete function and then we can go ahead and create a new one so let's go ahead and create a new one and let's go ahead and create a new one and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1487.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1588, 'published': '2021-10-07 14:52:32 UTC', 'start': 1487, 'text': ""and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here. Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1588,2021-10-07 14:52:32 UTC,1487,"and let's go ahead and create a new one it tells us what kind of a page this needs to be to have so I'll create a new recognizereiben and that'll just be like this image that brings up us a new item if the picture of the data source that our m ends up having is 6 we areIs going to have to do a new line. So in the beginning we did make a section where we are Doesn't it always say you know that the cyberxual heBeic name your type And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here. Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1517.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1604, 'published': '2021-10-07 14:52:32 UTC', 'start': 1517, 'text': ""And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here. Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID. And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1604,2021-10-07 14:52:32 UTC,1517,"And then what we need to do is pass those as well. So just like before, parser, pass orgs. Yeah, there we go. So now we have, so at this point, the user has given us a user ID that they want to delete and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here. Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID. And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1534.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1630, 'published': '2021-10-07 14:52:32 UTC', 'start': 1534, 'text': ""and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here. Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID. And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified. Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1630,2021-10-07 14:52:32 UTC,1534,"and we need to go ahead and delete that. So to delete it, first we need to say, okay, does it exist? So we say if orgs user ID is in our data, so we need to load the data first. So let me load it here. Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID. And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified. Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1558.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1645, 'published': '2021-10-07 14:52:32 UTC', 'start': 1558, 'text': ""Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID. And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified. Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry, saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1645,2021-10-07 14:52:32 UTC,1558,"Okay, so if that is in data user ID, then we can go ahead and delete it. And to do that, we're just going to use the typical, like pandas data frame logic. So we just write data equals data, data user ID. And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified. Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry, saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1582.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1663, 'published': '2021-10-07 14:52:32 UTC', 'start': 1582, 'text': ""And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified. Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry, saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying, hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1663,2021-10-07 14:52:32 UTC,1582,"And we just want to select the rows that are not equal to the user ID that we just pulled in. So the orgs user ID, right? So that's just going to return all of the rows set from the one that we've specified. Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry, saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying, hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1597.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1680, 'published': '2021-10-07 14:52:32 UTC', 'start': 1597, 'text': ""Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry, saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying, hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows. So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1680,2021-10-07 14:52:32 UTC,1597,"Okay, so we deleted that. Now we just need to save that to file again. To CSV, it is, I keep forgetting the variables, user's path and then index is false. Okay, so we've opened file, deleted the entry, saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying, hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows. So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1624.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1694, 'published': '2021-10-07 14:52:32 UTC', 'start': 1624, 'text': ""saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying, hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows. So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay, it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1694,2021-10-07 14:52:32 UTC,1624,"saved it back to CSV. If we have done all that, we can return to user 200 codes. So we'll just return, let's return all the data again. We don't need to do this every time. We could just return a statement saying, hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows. So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay, it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1638.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1713, 'published': '2021-10-07 14:52:32 UTC', 'start': 1638, 'text': ""hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows. So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay, it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code,"", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1713,2021-10-07 14:52:32 UTC,1638,"hey, you've deleted everything. So I'm just going to return so we can see what we're actually doing in reality. Probably pretty sure you wouldn't want to actually do this. But I suppose it depends, maybe you would, who knows. So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay, it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code,",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1656.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1732, 'published': '2021-10-07 14:52:32 UTC', 'start': 1656, 'text': ""So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay, it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code, which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1732,2021-10-07 14:52:32 UTC,1656,"So return to 100 code because it's successful. Now in the case that the user ID does not exist, here we want to say, okay, we want to check. Well, we've already checked if it exists. And then from there, we're just like, okay, it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code, which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1672.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1741, 'published': '2021-10-07 14:52:32 UTC', 'start': 1672, 'text': ""it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code, which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it. Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1741,2021-10-07 14:52:32 UTC,1672,"it doesn't exist, what are you doing? So we just return, we say, what did we use before when we sent a message, message, just message. Okay, cool. Message, and the message is going to be user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code, which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it. Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1689.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1756, 'published': '2021-10-07 14:52:32 UTC', 'start': 1689, 'text': ""user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code, which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it. Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users. What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1756,2021-10-07 14:52:32 UTC,1689,"user ID does not exist, something along those lines. So args user ID does not exist. Let me format that a little nicer. And then here we need to add the code, which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it. Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users. What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1708.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1774, 'published': '2021-10-07 14:52:32 UTC', 'start': 1708, 'text': ""which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it. Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users. What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those. So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1774,2021-10-07 14:52:32 UTC,1708,"which is, I suppose, not found. So 404. Okay. So I think that's it really. So let's save it. Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users. What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those. So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1723.8400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1797, 'published': '2021-10-07 14:52:32 UTC', 'start': 1723, 'text': ""Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users. What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those. So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens. Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1797,2021-10-07 14:52:32 UTC,1723,"Again, it should reload automatically. Head over to Insomnia. Let's again, so let's go to users. We're just going to look at what we have. So sorry, get users. What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those. So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens. Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1739.8400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1811, 'published': '2021-10-07 14:52:32 UTC', 'start': 1739, 'text': ""What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those. So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens. Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think. Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1811,2021-10-07 14:52:32 UTC,1739,"What do we get? We have all these. Now we want to delete user IDs four and five. Okay, these two are messy. We don't want those. So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens. Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think. Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1749.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1830, 'published': '2021-10-07 14:52:32 UTC', 'start': 1749, 'text': ""So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens. Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think. Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing. Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1830,2021-10-07 14:52:32 UTC,1749,"So we want to specify a user ID. We don't want four. So we change that to a delete request. We don't want four. Let's send that, see what happens. Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think. Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing. Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1763.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1848, 'published': '2021-10-07 14:52:32 UTC', 'start': 1763, 'text': ""Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think. Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing. Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here. So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1848,2021-10-07 14:52:32 UTC,1763,"Why did that not change? Data. Why didn't that change? User ID. Is it because this needs to be a string, I think. Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing. Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here. So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1793.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1866, 'published': '2021-10-07 14:52:32 UTC', 'start': 1793, 'text': ""Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing. Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here. So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove. So let's send that. And now you can see that we've actually removed those entries. So that's pretty good. Now, the one thing I did also notice is if I try and send that again, we do get the 404 not found."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1866,2021-10-07 14:52:32 UTC,1793,"Okay, I think. Yeah, probably. Let me try again. Send. No, still nothing. Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here. So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove. So let's send that. And now you can see that we've actually removed those entries. So that's pretty good. Now, the one thing I did also notice is if I try and send that again, we do get the 404 not found.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1806.3200000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1890, 'published': '2021-10-07 14:52:32 UTC', 'start': 1806, 'text': ""Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here. So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove. So let's send that. And now you can see that we've actually removed those entries. So that's pretty good. Now, the one thing I did also notice is if I try and send that again, we do get the 404 not found. So that's pretty cool, right? We've got everything that we would expect with it. Now, I think that's it for the code. The only other thing I wanted to show you was this other. So this is the actual full API script here."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1890,2021-10-07 14:52:32 UTC,1806,"Okay, so that took me far too long to actually figure out what was wrong. But there wasn't anything wrong. There was the string issue. So we did need to convert over to a string to compare them. But also, so what I was doing here was reading the key value here. So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove. So let's send that. And now you can see that we've actually removed those entries. So that's pretty good. Now, the one thing I did also notice is if I try and send that again, we do get the 404 not found. So that's pretty cool, right? We've got everything that we would expect with it. Now, I think that's it for the code. The only other thing I wanted to show you was this other. So this is the actual full API script here.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
MF75aNH3Gjs-t1822.9599999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1901, 'published': '2021-10-07 14:52:32 UTC', 'start': 1822, 'text': ""So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove. So let's send that. And now you can see that we've actually removed those entries. So that's pretty good. Now, the one thing I did also notice is if I try and send that again, we do get the 404 not found. So that's pretty cool, right? We've got everything that we would expect with it. Now, I think that's it for the code. The only other thing I wanted to show you was this other. So this is the actual full API script here. So in here, I also included another method here or another endpoint locations. And we run that and we can mess around with the cafe locations as well. So that's everything. So I think there's nothing else I want to cover in this video. So we'll leave it there."", 'title': 'API Series #2 - Building an API with Flask in Python', 'url': 'https://youtu.be/MF75aNH3Gjs'}",UCv83tO5cePwHMt1952IVVHw,1901,2021-10-07 14:52:32 UTC,1822,"So the row number and thinking four was user ID is not. We have two fives that we messed up and made those earlier. So there is no four for us to remove now. We don't need to worry about it. So let's test it with five instead, which are the two messy ones that we need to remove. So let's send that. And now you can see that we've actually removed those entries. So that's pretty good. Now, the one thing I did also notice is if I try and send that again, we do get the 404 not found. So that's pretty cool, right? We've got everything that we would expect with it. Now, I think that's it for the code. The only other thing I wanted to show you was this other. So this is the actual full API script here. So in here, I also included another method here or another endpoint locations. And we run that and we can mess around with the cafe locations as well. So that's everything. So I think there's nothing else I want to cover in this video. So we'll leave it there.",API Series #2 - Building an API with Flask in Python,https://youtu.be/MF75aNH3Gjs
e_SBq3s20M8-t16.080000000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 171, 'published': '2021-08-20 16:00:16 UTC', 'start': 16, 'text': ""efficient similarity search. Now there are a huge number of companies that use similarity search. I mean you have big names like Google. I mean Google is built from similarity search. Then you have Netflix, Amazon, Spotify. All of them are constantly recommending you different products, films, music and they do that by comparing you to other customers. So they are performing a similar search between you other customers and identifying the most similar ones. Now you have two approaches. You have exhaustive which is comparing all of the data points. I'm just going to call them vectors from now on because that's what we'll be using. So comparing all these vectors and obviously it's slow. Approximate search allows us to approximate those vectors, restrict our scope to a more relevant range of vectors and so on. So it cause a lot of different techniques. It's not just one technique here. The one we're going to be covering today is low-pass sensitive hashing. So at its core LSH is a hashing algorithm which attempts to maximize hash collisions. So what we see on screen right now is a dictionary, like a typical Python dictionary in the way that it hashes different items. So we have our keys which are items that we're hashing. We process them through a hashing function and that hashing function attempts to minimize hashing collisions e.g. to not put keys in the same bucket. It wants every key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys,"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,171,2021-08-20 16:00:16 UTC,16,"efficient similarity search. Now there are a huge number of companies that use similarity search. I mean you have big names like Google. I mean Google is built from similarity search. Then you have Netflix, Amazon, Spotify. All of them are constantly recommending you different products, films, music and they do that by comparing you to other customers. So they are performing a similar search between you other customers and identifying the most similar ones. Now you have two approaches. You have exhaustive which is comparing all of the data points. I'm just going to call them vectors from now on because that's what we'll be using. So comparing all these vectors and obviously it's slow. Approximate search allows us to approximate those vectors, restrict our scope to a more relevant range of vectors and so on. So it cause a lot of different techniques. It's not just one technique here. The one we're going to be covering today is low-pass sensitive hashing. So at its core LSH is a hashing algorithm which attempts to maximize hash collisions. So what we see on screen right now is a dictionary, like a typical Python dictionary in the way that it hashes different items. So we have our keys which are items that we're hashing. We process them through a hashing function and that hashing function attempts to minimize hashing collisions e.g. to not put keys in the same bucket. It wants every key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys,",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t48.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 211, 'published': '2021-08-20 16:00:16 UTC', 'start': 48, 'text': ""you have two approaches. You have exhaustive which is comparing all of the data points. I'm just going to call them vectors from now on because that's what we'll be using. So comparing all these vectors and obviously it's slow. Approximate search allows us to approximate those vectors, restrict our scope to a more relevant range of vectors and so on. So it cause a lot of different techniques. It's not just one technique here. The one we're going to be covering today is low-pass sensitive hashing. So at its core LSH is a hashing algorithm which attempts to maximize hash collisions. So what we see on screen right now is a dictionary, like a typical Python dictionary in the way that it hashes different items. So we have our keys which are items that we're hashing. We process them through a hashing function and that hashing function attempts to minimize hashing collisions e.g. to not put keys in the same bucket. It wants every key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys, so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,211,2021-08-20 16:00:16 UTC,48,"you have two approaches. You have exhaustive which is comparing all of the data points. I'm just going to call them vectors from now on because that's what we'll be using. So comparing all these vectors and obviously it's slow. Approximate search allows us to approximate those vectors, restrict our scope to a more relevant range of vectors and so on. So it cause a lot of different techniques. It's not just one technique here. The one we're going to be covering today is low-pass sensitive hashing. So at its core LSH is a hashing algorithm which attempts to maximize hash collisions. So what we see on screen right now is a dictionary, like a typical Python dictionary in the way that it hashes different items. So we have our keys which are items that we're hashing. We process them through a hashing function and that hashing function attempts to minimize hashing collisions e.g. to not put keys in the same bucket. It wants every key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys, so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t80.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 252, 'published': '2021-08-20 16:00:16 UTC', 'start': 80, 'text': ""low-pass sensitive hashing. So at its core LSH is a hashing algorithm which attempts to maximize hash collisions. So what we see on screen right now is a dictionary, like a typical Python dictionary in the way that it hashes different items. So we have our keys which are items that we're hashing. We process them through a hashing function and that hashing function attempts to minimize hashing collisions e.g. to not put keys in the same bucket. It wants every key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys, so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,252,2021-08-20 16:00:16 UTC,80,"low-pass sensitive hashing. So at its core LSH is a hashing algorithm which attempts to maximize hash collisions. So what we see on screen right now is a dictionary, like a typical Python dictionary in the way that it hashes different items. So we have our keys which are items that we're hashing. We process them through a hashing function and that hashing function attempts to minimize hashing collisions e.g. to not put keys in the same bucket. It wants every key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys, so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t124.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 291, 'published': '2021-08-20 16:00:16 UTC', 'start': 124, 'text': ""key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys, so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,291,2021-08-20 16:00:16 UTC,124,"key to go to a separate bucket and then these are connected. Then they don't contain the values but they're connected to the values that we relate back to our keys. So that's a Python dictionary. That's our Python dictionary. But we're not wanting to minimize collisions. We are wanting to maximize the collisions. So what we see here is a hashing function that maximizes those collisions. So this is essentially what LSH is doing. So we are attempting to for any similar keys, so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t159.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 328, 'published': '2021-08-20 16:00:16 UTC', 'start': 159, 'text': ""so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,328,2021-08-20 16:00:16 UTC,159,"so these here and these here, they're all similar enough for us to want to put them into the same buckets. So we've put two of them into here and then the other three into this bucket. Now there are quite a few different ways of doing this and there are a lot of different LSH methods. In fact, LSH is a very generic term that applies to a lot of different algorithms. And the one that we will be covering is what I see as the traditional version. So it is the original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t192.72000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 363, 'published': '2021-08-20 16:00:16 UTC', 'start': 192, 'text': ""original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature. So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,363,2021-08-20 16:00:16 UTC,192,"original version of LSH. And what we'll be covering in this video is shingling, min hashing, and that LSH function. So we'll get to understand why very soon. So here is the overview of the process that we're going to be walking through. So we have shingling. So we have at the very start, we have this text. So flying fish flew by the space station. Now that's just a string. And what we want to do is extract all of the unique pairs of texts. So when we say shingling, it's k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature. So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t237.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 402, 'published': '2021-08-20 16:00:16 UTC', 'start': 237, 'text': ""k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature. So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,402,2021-08-20 16:00:16 UTC,237,"k shingling. And in this case, our k value is two because we're taking two characters at once. If we were to take k equals four, for example, then we would take like pace and then move on, we take ace and a space and so on. So that's the shingling. And from that, we create a set. So if we have duplicate shingles, we remove those. So we just end up with one. So in this, I don't know if we do have any duplicates, but say maybe down here, we have i n again, because we also have it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature. So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t276.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 442, 'published': '2021-08-20 16:00:16 UTC', 'start': 276, 'text': ""it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature. So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,442,2021-08-20 16:00:16 UTC,276,"it up here. We would end up with just a single i n in the set. You wouldn't have two. And then we want to encode those. So that means we take a vocabulary from all of our texts and not just this one sentence, but we'll have more than one sentence, obviously, that we're comparing. And we'll use that to build a one vector from the vocab and our shingle set. Then we process that through something called a min hash function, which produces this dense vector or signature. So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t309.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 482, 'published': '2021-08-20 16:00:16 UTC', 'start': 309, 'text': ""So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,482,2021-08-20 16:00:16 UTC,309,"So this thing down here, OK, that is what's called a signature. And then we band that into this final bit here. This is our actual LSH process. So we band that vector into multiple sub vectors and then we hash them. So where we find that we have any two sub vectors go to the same hash bucket, then that means that the full vector that they both come from is considered, or the two full vectors that they both come from are considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t350.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 523, 'published': '2021-08-20 16:00:16 UTC', 'start': 350, 'text': ""considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,523,2021-08-20 16:00:16 UTC,350,"considered a candidate pair. And we take those and we then calculate some other, we calculate the similarity between them. OK, so first step in our process, like we discussed, is the shingling operation. So shingling is simply where we take a window of length K characters and we simply move that down through our text, like you can see here. And from that, we create the shingle set. So in Python, what we do to shingle these three sentences we have here is we'll create a shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t389.67999999999995,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 570, 'published': '2021-08-20 16:00:16 UTC', 'start': 389, 'text': ""shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either. And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now,"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,570,2021-08-20 16:00:16 UTC,389,"shingle function here. And this is going to take some text, which is a string. And we're going to say we're going to define the K values of the number of characters we take within each window, which is obviously an integer. Now we initialize the value of the string. Now we initialize our shingle set here. We'll make a string initially. And then what we do is for i in range. And then here we want to go from the, or we want to go to the length of our text minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either. And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now,",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t427.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 602, 'published': '2021-08-20 16:00:16 UTC', 'start': 427, 'text': ""minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either. And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now, or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,602,2021-08-20 16:00:16 UTC,427,"minus K. So minus that window length plus one, because we want to go right up to the end of that. And then here, all we do is shingle set dot append. And then we write, so we have the text and we want to go from i up until i plus K. Okay. That's our shingle list, I suppose. And then we want to return a set. So this will remove any duplicates that we have. So shingle set. Okay. So that's our shingle function. And we just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either. And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now, or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t467.36,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 651, 'published': '2021-08-20 16:00:16 UTC', 'start': 467, 'text': ""just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either. And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now, or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,651,2021-08-20 16:00:16 UTC,467,"just want to process each one of our sentences through that. So we'll go A equals shingle A. Also, we need to define K, which can be two. I'll just define K here. Okay. And then let's have a look at what we have. And we see that we have this, it's shuffle. There's no order to our set here. And we see that we have all of the pairs of words in there. So we have S for the sort of the space part here or station actually could be either. And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now, or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t509.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 690, 'published': '2021-08-20 16:00:16 UTC', 'start': 509, 'text': ""And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now, or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,690,2021-08-20 16:00:16 UTC,509,"And if we try and find, okay, so here we have the very sorts of fly or flying. You have the L Y there as well. Iron. So that's our shingle set. And with this, we have all of our shingles. So the next step is to create our vocabulary, which is just all of our shingles, our shingle sets, a union together. So to create that, all we do is we go A union B dot union like that. You can see again, we have just a lot more text in there now, or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t553.3599999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 725, 'published': '2021-08-20 16:00:16 UTC', 'start': 553, 'text': ""or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,725,2021-08-20 16:00:16 UTC,553,"or a lot more, many more shingles. That is our vocab. So now we have our shingle set and we have our vocab. So we can tick both of those off. Now what we need to do is create our one encoding over here. And the only other thing we need is a zero vector. So there's two, well, I mean, there's more than two ways to do this, but I think there's two ways of thinking about it. Normally the more efficient way would be to create a NumPy array full of zeros and then just add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t589.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 766, 'published': '2021-08-20 16:00:16 UTC', 'start': 589, 'text': ""add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,766,2021-08-20 16:00:16 UTC,589,"add the ones in where we have matches between our vocab and shingle set. But I'm not going to do that. I'm just going to keep things like incredibly simple in the code that we're writing. So I'm going to do A, it's one hot. Or the one thing we should do is make this a list because we want order in our vocab and not have it shuffled. So what we do here is we say one for x in A, or sorry, no, one if x is in A else zero for x in vocab. So what we're doing here is what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t637.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 813, 'published': '2021-08-20 16:00:16 UTC', 'start': 637, 'text': ""what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account. We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,813,2021-08-20 16:00:16 UTC,637,"what we're doing here is looping through the vocab and every single shingle within there, we're saying if that exists in our signature, make that point in our list a one, otherwise make it a zero. So that's simply our one hot encoding. So if we do ABC and then we have a look at our A one hot, we see that we have this one hot encoded or this sparse array. Now, min hashing is the next step in our process and it allows us to convert our what are currently sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account. We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t678.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 860, 'published': '2021-08-20 16:00:16 UTC', 'start': 678, 'text': ""sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account. We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,860,2021-08-20 16:00:16 UTC,678,"sparse vectors into dense vectors, which we call signatures. Now, what you see here is a run through of how we do this for maybe one signature. We want to do that for multiple signatures though. So we would actually run through this process multiple times. So what we're doing here is we're creating a randomly permuted array, which counts from one to the length of our vocab. And then what we are essentially doing, I know so in this web, we're basically shuffling it and then we're counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account. We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t710.0799999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 901, 'published': '2021-08-20 16:00:16 UTC', 'start': 710, 'text': ""counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account. We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,901,2021-08-20 16:00:16 UTC,710,"counting through until we find the first alignment to one within our vector. In reality, you just take all of your values and you find a minimum one that aligns to one. So that's if you're using NumPy, which we'll see later on. I'll just show you the code, I'm not going to actually write all of it though. So in code, that would look something like this. So we would start with a list, which is the range from one to the length of our vocab. And if we have a look at that, we just say account. We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t749.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 937, 'published': '2021-08-20 16:00:16 UTC', 'start': 749, 'text': ""We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two. We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,937,2021-08-20 16:00:16 UTC,749,"We're going to shuffle that. So from random import shuffle. And we just do it like this. So it modifies it in place. So we don't need to do anything there. So let's view that. Okay, so now we've shuffled that, shuffled it twice now, but that's fine. And let's just loop through five of those. So four, we can look for more. Four i in range from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two. We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t788.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 978, 'published': '2021-08-20 16:00:16 UTC', 'start': 788, 'text': ""from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two. We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors. Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,978,2021-08-20 16:00:16 UTC,788,"from one to ten. What we're going to say is, I just want to print i, which aligns to the hash sample index for that value. Okay, if we print that, we see so one, the value one, where is it? Here is at index 85. Two is at 53 and so on. And essentially what we're doing here is saying, loop through these, identify this index and this index in our one-hot vector. Does it align to a one? And see that here, we find the first one at eight. And that means our signature value for this point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two. We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors. Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t844.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1017, 'published': '2021-08-20 16:00:16 UTC', 'start': 844, 'text': ""point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two. We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors. Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1017,2021-08-20 16:00:16 UTC,844,"point is for this min hash vector and our one-hot sparse vector here. That signature value will be eight. And we repeat that for multiple min hash vectors. Which is what you can see here. So if we were to work through this, so we saw one here, that does not align to a one. So we work up to two and we find that it does align to a one. So that is why we have this here. And then we go on to this one here. We find one and we work up to two. We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors. Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t883.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1061, 'published': '2021-08-20 16:00:16 UTC', 'start': 883, 'text': ""We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors. Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y. Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1061,2021-08-20 16:00:16 UTC,883,"We have this here. And then we go on to this one here. We find one does not align. Two, still does not align. Three does not align. And four does align. So then we assign a four in our min hash function. We go along and keep doing that to create our signature. Okay. So I'm going to use these functions here. It's just what we wrote before, but put into a cleaner format. And what I'm going to do is create 20 min hash vectors. Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y. Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t922.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1099, 'published': '2021-08-20 16:00:16 UTC', 'start': 922, 'text': ""Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y. Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar. So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors,"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1099,2021-08-20 16:00:16 UTC,922,"Run that. And then here we are going to run each of our one-hot sparse vectors through our create hash function, which is here. And it's going to convert them into our signatures as we described before. And we see here that we have also what did I mention. So here we have 20 min hash vectors, which means we have a length of 20 for each signature. So what we see here are our dense vectors. And these are just compressed versions of our sparse vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y. Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar. So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors,",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t961.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1143, 'published': '2021-08-20 16:00:16 UTC', 'start': 961, 'text': ""vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y. Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar. So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors, it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1143,2021-08-20 16:00:16 UTC,961,"vectors. And we can check that that is true by we'll define a, we'll create a jaccard similarity function. So we take, and here we take x and y both will be sets. And we just return the length of the intersection between both of those. So the intersection between those is divided by the union of both of those. So that is how you calculate jaccard similarity. This should be a y. Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar. So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors, it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1002.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1170, 'published': '2021-08-20 16:00:16 UTC', 'start': 1002, 'text': ""Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar. So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors, it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1170,2021-08-20 16:00:16 UTC,1002,"Okay. And then if we do jaccard on both of those, so we have a sig, b sig. These will have to be converted into sets. Forgot. So like that. And then if we also take the jaccard for, I think it's just a and b, right? So copy that. Okay. So we get, this is 0.6 and this is 1.4. Now, if we look up here, I think it's a and b are not supposed to be very similar. So that's fine. And then b and c should be similar. So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors, it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1048.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1212, 'published': '2021-08-20 16:00:16 UTC', 'start': 1048, 'text': ""So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors, it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1212,2021-08-20 16:00:16 UTC,1048,"So if we swap this for c and then c here, we should both get higher values. And they should be and they should be roughly in the same ballpark. I mean, they're not perfect because we're using a very low number here. We're only using 20 values and typically use a lot more. But that's fine. So you can see that they're both aligned, right? So despite converting these into the signature vectors, it recognizes that they are very similar. And converting these into signature vectors, it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1084.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1245, 'published': '2021-08-20 16:00:16 UTC', 'start': 1084, 'text': ""it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1245,2021-08-20 16:00:16 UTC,1084,"it still recognizes that they are reasonably similar. So that's good. That's what we want. Now the final step in our whole LHS process is the LHS function itself. So this is essentially what it does. So we have our signature over here, which we built using the steps that we just went through, which you can see here. And from that signature, we take a certain number of equal length subvectors. So we define that using this here, this b. So b is 3. So that means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1126.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1284, 'published': '2021-08-20 16:00:16 UTC', 'start': 1126, 'text': ""means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1284,2021-08-20 16:00:16 UTC,1126,"means we split a signature into three different subvectors, which we see over here. And ideally, what we want to be doing here is saying, okay, we process our subvectors each through a, either a different hash function or it can be the same hash function, just as long as we use that same hash function for the equivalent subvector in another signature, which you'll see in a moment, it'll make sense. And once we have multiple signatures going together through those hash functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1160.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1328, 'published': '2021-08-20 16:00:16 UTC', 'start': 1160, 'text': ""functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b. And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1328,2021-08-20 16:00:16 UTC,1160,"functions, you can see here that they're equivalent on both sides, hash one, hash one here. These can all just be a single hash function as well, which is what we're going to do. We're not really going to use a hash function. And what we get here is three opportunities to identify these signatures as being potential candidate pairs, which is where we consider it for further similarity comparisons. In this case, hash threes both collide down here. So we say, okay, that means that a and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b. And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1195.9199999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1366, 'published': '2021-08-20 16:00:16 UTC', 'start': 1195, 'text': ""and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b. And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1366,2021-08-20 16:00:16 UTC,1195,"and b are candidate pairs. I'm just going to put cand pairs. So this act of splitting our signatures up into multiple subvectors just gives us more opportunities to identify similarities because if we were to use the full vectors, the full vector would have to be very similar for them to be put into the same hash bucket. With this, we only part of it to be very similar. So increases the chances of us finding those similar signatures. So we're going to implement a very simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b. And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1232.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1405, 'published': '2021-08-20 16:00:16 UTC', 'start': 1232, 'text': ""simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b. And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1405,2021-08-20 16:00:16 UTC,1232,"simple version of this. I'm going to keep this very simple. Here we're just splitting our signature vector. So we add our signature and b, which is the number of bands. And the first thing we do is just make sure that our signature can be split into b bands equally. So where we take the remainder after the division here, it must be equal to zero. And then we say we need to calculate the rows. So the number of rows within each band, which obviously just the length of the signature divided by b. And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1265.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1432, 'published': '2021-08-20 16:00:16 UTC', 'start': 1265, 'text': ""And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1432,2021-08-20 16:00:16 UTC,1265,"And then we initialize a subvector array or list. And then we loop through and append subvectors. Really simple, simple implementation. And let's apply that to b and c. So we have said that we want 10 bands. So we only have 20 items or 20 numbers within our signature vectors. So obviously we only get bands of two rows at a time. And we should find that at least one of those match. So what we do is we loop through and we say if b rows equals c rows, break. Okay. And we find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1312.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1470, 'published': '2021-08-20 16:00:16 UTC', 'start': 1312, 'text': ""find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine. So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band."", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1470,2021-08-20 16:00:16 UTC,1312,"find very quickly that there is a candidate pair there. So that means that b and c, the full vectors will be considered as a candidate pair. Let's do the same for a. And we should find, okay, so for both a and b and a and c, it's not considered a candidate pair because there's just no similarity there. So that's good. That's exactly what we wanted to happen. So we can now see that that is our implementation of this. So the LSH, traditional LSH approach. Now a few other things that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine. So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band.",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1352.3200000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1510, 'published': '2021-08-20 16:00:16 UTC', 'start': 1352, 'text': ""that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine. So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band. And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1510,2021-08-20 16:00:16 UTC,1352,"that we haven't covered, but we should just touch on quickly. And you can find, so there's an article link in the description that which covers this. I walked through all of this and there will also be a notebook where I'm getting these results from in the first place. So you can also look at that. That includes the NumPy implementations of what we've just done, which is slightly more efficient, although not super efficient because I want it to still be readable. So what we have here is visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine. So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band. And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1386.8799999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1549, 'published': '2021-08-20 16:00:16 UTC', 'start': 1386, 'text': ""visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine. So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band. And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair classifications either up or down. So here we have different B values. The side we have black, which is 50. Then we go 25, 20, which is what we used before, and five. So let's say we found that we're not identifying enough candidate pairs. We could push that down a little bit. Maybe we don't do too much. So we could change B from 20 to 25. And if we do that, we see this. So in green, you have our old results and our old probability line. And then in blue and pink, we have the new ones"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1549,2021-08-20 16:00:16 UTC,1386,"visualization shows the similarity, the cosine similarity of our signature vectors and whether they were considered as candidate pairs or not. So these up here, these are our candidate pairs. This is just a random sample. I think the actual full dataset is really big. So running this, all of them is super inefficient because we're also running everything else through. So I can just have the visualization here. But if you run just that on it, it does work. It's fine. So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band. And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair classifications either up or down. So here we have different B values. The side we have black, which is 50. Then we go 25, 20, which is what we used before, and five. So let's say we found that we're not identifying enough candidate pairs. We could push that down a little bit. Maybe we don't do too much. So we could change B from 20 to 25. And if we do that, we see this. So in green, you have our old results and our old probability line. And then in blue and pink, we have the new ones",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1423.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1587, 'published': '2021-08-20 16:00:16 UTC', 'start': 1423, 'text': ""So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band. And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair classifications either up or down. So here we have different B values. The side we have black, which is 50. Then we go 25, 20, which is what we used before, and five. So let's say we found that we're not identifying enough candidate pairs. We could push that down a little bit. Maybe we don't do too much. So we could change B from 20 to 25. And if we do that, we see this. So in green, you have our old results and our old probability line. And then in blue and pink, we have the new ones again, or blue and magenta. So what we see here is we've pushed that down. So we've changed B to 25. And now we're returning more results. So over here, we have these, for example, which we were not returning before. And there are also more values in here as well. And there are less values down here. So that's the result of us modifying B. So we can visualize that like so. So if we increase B, we move it in this direction, which increases the number of candidate pairs,"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1587,2021-08-20 16:00:16 UTC,1423,"So at the top there, we have our candidates. At the bottom, we have our non-candidates. We have some like, so we can see that high similarity does correlate with them being classified as candidate pairs, which is good. It's obviously what we want. And there is this formula that I did not write down, which I should have done, which is P equals one minus one minus S, which is our similarity down here to the power of R, which is the number of rows in each band. And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair classifications either up or down. So here we have different B values. The side we have black, which is 50. Then we go 25, 20, which is what we used before, and five. So let's say we found that we're not identifying enough candidate pairs. We could push that down a little bit. Maybe we don't do too much. So we could change B from 20 to 25. And if we do that, we see this. So in green, you have our old results and our old probability line. And then in blue and pink, we have the new ones again, or blue and magenta. So what we see here is we've pushed that down. So we've changed B to 25. And now we're returning more results. So over here, we have these, for example, which we were not returning before. And there are also more values in here as well. And there are less values down here. So that's the result of us modifying B. So we can visualize that like so. So if we increase B, we move it in this direction, which increases the number of candidate pairs,",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
e_SBq3s20M8-t1455.0400000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1622, 'published': '2021-08-20 16:00:16 UTC', 'start': 1455, 'text': ""And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair classifications either up or down. So here we have different B values. The side we have black, which is 50. Then we go 25, 20, which is what we used before, and five. So let's say we found that we're not identifying enough candidate pairs. We could push that down a little bit. Maybe we don't do too much. So we could change B from 20 to 25. And if we do that, we see this. So in green, you have our old results and our old probability line. And then in blue and pink, we have the new ones again, or blue and magenta. So what we see here is we've pushed that down. So we've changed B to 25. And now we're returning more results. So over here, we have these, for example, which we were not returning before. And there are also more values in here as well. And there are less values down here. So that's the result of us modifying B. So we can visualize that like so. So if we increase B, we move it in this direction, which increases the number of candidate pairs, which also increases the number of false positives that we're going to return. This line, by the way, is our threshold. So similarity threshold is basically where we want the cutoff to be between things being identified as candidate pairs and not candidate pairs. It's like our target almost. Or if we wanted to reduce the number of candidate pairs, because maybe we're getting too many false positives, we can push it this way, which will result in less candidate pairs, but also results"", 'title': 'Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)', 'url': 'https://youtu.be/e_SBq3s20M8'}",UCv83tO5cePwHMt1952IVVHw,1622,2021-08-20 16:00:16 UTC,1455,"And all of this to the power of B, which is number of bands. Now that correlates to this line here, this probability. Obviously it's P, capital P. So that's where it's coming from. And if we run this with different similarity values, this is the pattern that we get. And obviously that correlates, you can see, with whether something is classified as a candidate pair or not. And what we can do is we can modify B to push the number of candidate pair classifications either up or down. So here we have different B values. The side we have black, which is 50. Then we go 25, 20, which is what we used before, and five. So let's say we found that we're not identifying enough candidate pairs. We could push that down a little bit. Maybe we don't do too much. So we could change B from 20 to 25. And if we do that, we see this. So in green, you have our old results and our old probability line. And then in blue and pink, we have the new ones again, or blue and magenta. So what we see here is we've pushed that down. So we've changed B to 25. And now we're returning more results. So over here, we have these, for example, which we were not returning before. And there are also more values in here as well. And there are less values down here. So that's the result of us modifying B. So we can visualize that like so. So if we increase B, we move it in this direction, which increases the number of candidate pairs, which also increases the number of false positives that we're going to return. This line, by the way, is our threshold. So similarity threshold is basically where we want the cutoff to be between things being identified as candidate pairs and not candidate pairs. It's like our target almost. Or if we wanted to reduce the number of candidate pairs, because maybe we're getting too many false positives, we can push it this way, which will result in less candidate pairs, but also results",Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python),https://youtu.be/e_SBq3s20M8
Ey81KfQ3PQU-t12.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 174, 'published': '2021-05-05 15:00:09 UTC', 'start': 12, 'text': ""really into depth. I'm just going to show you how to actually use the library. Now, if you do want to go into a little more depth, I have another video that I'll be releasing just before this one. And that will go into what is actually happening here, how we are calculating similarity or pulling the how the BERT model that we'll be using is actually creating those embeddings and then how we're actually calculating the similarity there. So if you're interested in that, go check it out. Otherwise, if you just want to get a quick similarity score between two sentences, this is probably the way to go. So we have these six sentences up here and this one, three years later, the coffin was still full of jello. And this one, the person box was packed with jelly many dozens of months later. They're saying the same thing, but the second one is saying it in a way that most of us wouldn't normally say it. Instead of saying coffin, we're saying person box instead of jello, we're saying jelly. I think that's kind of normal, actually. And instead of years, we're saying dozens of months. So it's not really sharing the same words, but we're going to see that we can actually find that these two sentences are the most similar out of all of these. So we're taking those and we're going to be importing the sentence transformers library. And we want to import the sentence transformer. And then from that, we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are"", 'title': 'Sentence Similarity With Sentence-Transformers in Python', 'url': 'https://youtu.be/Ey81KfQ3PQU'}",UCv83tO5cePwHMt1952IVVHw,174,2021-05-05 15:00:09 UTC,12,"really into depth. I'm just going to show you how to actually use the library. Now, if you do want to go into a little more depth, I have another video that I'll be releasing just before this one. And that will go into what is actually happening here, how we are calculating similarity or pulling the how the BERT model that we'll be using is actually creating those embeddings and then how we're actually calculating the similarity there. So if you're interested in that, go check it out. Otherwise, if you just want to get a quick similarity score between two sentences, this is probably the way to go. So we have these six sentences up here and this one, three years later, the coffin was still full of jello. And this one, the person box was packed with jelly many dozens of months later. They're saying the same thing, but the second one is saying it in a way that most of us wouldn't normally say it. Instead of saying coffin, we're saying person box instead of jello, we're saying jelly. I think that's kind of normal, actually. And instead of years, we're saying dozens of months. So it's not really sharing the same words, but we're going to see that we can actually find that these two sentences are the most similar out of all of these. So we're taking those and we're going to be importing the sentence transformers library. And we want to import the sentence transformer. And then from that, we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are",Sentence Similarity With Sentence-Transformers in Python,https://youtu.be/Ey81KfQ3PQU
Ey81KfQ3PQU-t46.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 215, 'published': '2021-05-05 15:00:09 UTC', 'start': 46, 'text': ""Otherwise, if you just want to get a quick similarity score between two sentences, this is probably the way to go. So we have these six sentences up here and this one, three years later, the coffin was still full of jello. And this one, the person box was packed with jelly many dozens of months later. They're saying the same thing, but the second one is saying it in a way that most of us wouldn't normally say it. Instead of saying coffin, we're saying person box instead of jello, we're saying jelly. I think that's kind of normal, actually. And instead of years, we're saying dozens of months. So it's not really sharing the same words, but we're going to see that we can actually find that these two sentences are the most similar out of all of these. So we're taking those and we're going to be importing the sentence transformers library. And we want to import the sentence transformer. And then from that, we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six"", 'title': 'Sentence Similarity With Sentence-Transformers in Python', 'url': 'https://youtu.be/Ey81KfQ3PQU'}",UCv83tO5cePwHMt1952IVVHw,215,2021-05-05 15:00:09 UTC,46,"Otherwise, if you just want to get a quick similarity score between two sentences, this is probably the way to go. So we have these six sentences up here and this one, three years later, the coffin was still full of jello. And this one, the person box was packed with jelly many dozens of months later. They're saying the same thing, but the second one is saying it in a way that most of us wouldn't normally say it. Instead of saying coffin, we're saying person box instead of jello, we're saying jelly. I think that's kind of normal, actually. And instead of years, we're saying dozens of months. So it's not really sharing the same words, but we're going to see that we can actually find that these two sentences are the most similar out of all of these. So we're taking those and we're going to be importing the sentence transformers library. And we want to import the sentence transformer. And then from that, we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six",Sentence Similarity With Sentence-Transformers in Python,https://youtu.be/Ey81KfQ3PQU
Ey81KfQ3PQU-t80.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 260, 'published': '2021-05-05 15:00:09 UTC', 'start': 80, 'text': ""we're saying person box instead of jello, we're saying jelly. I think that's kind of normal, actually. And instead of years, we're saying dozens of months. So it's not really sharing the same words, but we're going to see that we can actually find that these two sentences are the most similar out of all of these. So we're taking those and we're going to be importing the sentence transformers library. And we want to import the sentence transformer. And then from that, we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics"", 'title': 'Sentence Similarity With Sentence-Transformers in Python', 'url': 'https://youtu.be/Ey81KfQ3PQU'}",UCv83tO5cePwHMt1952IVVHw,260,2021-05-05 15:00:09 UTC,80,"we're saying person box instead of jello, we're saying jelly. I think that's kind of normal, actually. And instead of years, we're saying dozens of months. So it's not really sharing the same words, but we're going to see that we can actually find that these two sentences are the most similar out of all of these. So we're taking those and we're going to be importing the sentence transformers library. And we want to import the sentence transformer. And then from that, we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics",Sentence Similarity With Sentence-Transformers in Python,https://youtu.be/Ey81KfQ3PQU
Ey81KfQ3PQU-t119.11999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 296, 'published': '2021-05-05 15:00:09 UTC', 'start': 119, 'text': ""we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics pairwise. And we import cosine similarity. And to calculate our cosine similarity, all we do is take that function. And inside here, we pass our first sentence. So this three years later, the coffin is still full of jello. I want to pass that sentence vector, which is just an index zero of our sentence Vets array. And because we are extracting that single array value. So if we just have a look at this, you see"", 'title': 'Sentence Similarity With Sentence-Transformers in Python', 'url': 'https://youtu.be/Ey81KfQ3PQU'}",UCv83tO5cePwHMt1952IVVHw,296,2021-05-05 15:00:09 UTC,119,"we want to initialize a sentence transformer model. So we write sentence transformer. And then in here, we're going to be using this model that I've already defined a model name, which is a BERT base MLI mean tokens model. So initialize that. I need to rerun that. So we have our model and I'll just show you really quickly. This model is coming from the Hugging Face Transformers library behind sentence transformers. So this is the actual model we are using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics pairwise. And we import cosine similarity. And to calculate our cosine similarity, all we do is take that function. And inside here, we pass our first sentence. So this three years later, the coffin is still full of jello. I want to pass that sentence vector, which is just an index zero of our sentence Vets array. And because we are extracting that single array value. So if we just have a look at this, you see",Sentence Similarity With Sentence-Transformers in Python,https://youtu.be/Ey81KfQ3PQU
Ey81KfQ3PQU-t155.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 338, 'published': '2021-05-05 15:00:09 UTC', 'start': 155, 'text': ""using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics pairwise. And we import cosine similarity. And to calculate our cosine similarity, all we do is take that function. And inside here, we pass our first sentence. So this three years later, the coffin is still full of jello. I want to pass that sentence vector, which is just an index zero of our sentence Vets array. And because we are extracting that single array value. So if we just have a look at this, you see that we have a almost like a list of lists here. If we just extract this, we only get a list. So what we want to do is actually keep that inside a list, otherwise we'll get dimension error. And then we can also use the same method to extract the array. So we can extract the array and then we do sentence Vets one onwards. So this will be the remaining sentences. Okay, so let's take these or let's just bring them down here."", 'title': 'Sentence Similarity With Sentence-Transformers in Python', 'url': 'https://youtu.be/Ey81KfQ3PQU'}",UCv83tO5cePwHMt1952IVVHw,338,2021-05-05 15:00:09 UTC,155,"using. Now, first thing we do here is create our sentence vectors or sentence embeddings. So we'll call this sentence Vets equals model and code. And all we need to do here is pass our sentences. So we can pass a single sentence or a list of sentences. It's completely fine. And then let's just have a quick look at what we have here. So you see that we have this big array. And if we look at the shape, we see that we have a six by 768 array. So the six refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics pairwise. And we import cosine similarity. And to calculate our cosine similarity, all we do is take that function. And inside here, we pass our first sentence. So this three years later, the coffin is still full of jello. I want to pass that sentence vector, which is just an index zero of our sentence Vets array. And because we are extracting that single array value. So if we just have a look at this, you see that we have a almost like a list of lists here. If we just extract this, we only get a list. So what we want to do is actually keep that inside a list, otherwise we'll get dimension error. And then we can also use the same method to extract the array. So we can extract the array and then we do sentence Vets one onwards. So this will be the remaining sentences. Okay, so let's take these or let's just bring them down here.",Sentence Similarity With Sentence-Transformers in Python,https://youtu.be/Ey81KfQ3PQU
Ey81KfQ3PQU-t200.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 382, 'published': '2021-05-05 15:00:09 UTC', 'start': 200, 'text': ""refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics pairwise. And we import cosine similarity. And to calculate our cosine similarity, all we do is take that function. And inside here, we pass our first sentence. So this three years later, the coffin is still full of jello. I want to pass that sentence vector, which is just an index zero of our sentence Vets array. And because we are extracting that single array value. So if we just have a look at this, you see that we have a almost like a list of lists here. If we just extract this, we only get a list. So what we want to do is actually keep that inside a list, otherwise we'll get dimension error. And then we can also use the same method to extract the array. So we can extract the array and then we do sentence Vets one onwards. So this will be the remaining sentences. Okay, so let's take these or let's just bring them down here. Calculate this. And we can see that our highest similarity by quite a bit is just 0.72. Now, that means that between this sentence and this sentence, we have a similarity score of 0.72. So clearly, it's working, it's scoring high similarity. And you can play around this and and test multiple different words and sentences and just see how it works. But that's the easy way of putting all this together. So I think it's really cool that we can do that so easily."", 'title': 'Sentence Similarity With Sentence-Transformers in Python', 'url': 'https://youtu.be/Ey81KfQ3PQU'}",UCv83tO5cePwHMt1952IVVHw,382,2021-05-05 15:00:09 UTC,200,"refers to our six sentences here. And the 768 refers to the hidden state size within the BERT model that we're using. So each one of these sentences is now being represented by a dense vector containing 768 values. And that means that we already take those and compare similarity between them. So to do that, we're going to be using the sklearn implementation of cosine similarity, which we can import like this. So sklearn pairwise or metrics pairwise. And we import cosine similarity. And to calculate our cosine similarity, all we do is take that function. And inside here, we pass our first sentence. So this three years later, the coffin is still full of jello. I want to pass that sentence vector, which is just an index zero of our sentence Vets array. And because we are extracting that single array value. So if we just have a look at this, you see that we have a almost like a list of lists here. If we just extract this, we only get a list. So what we want to do is actually keep that inside a list, otherwise we'll get dimension error. And then we can also use the same method to extract the array. So we can extract the array and then we do sentence Vets one onwards. So this will be the remaining sentences. Okay, so let's take these or let's just bring them down here. Calculate this. And we can see that our highest similarity by quite a bit is just 0.72. Now, that means that between this sentence and this sentence, we have a similarity score of 0.72. So clearly, it's working, it's scoring high similarity. And you can play around this and and test multiple different words and sentences and just see how it works. But that's the easy way of putting all this together. So I think it's really cool that we can do that so easily.",Sentence Similarity With Sentence-Transformers in Python,https://youtu.be/Ey81KfQ3PQU
Vwq7Ucp9UCw-t19.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 192, 'published': '2021-04-12 15:00:11 UTC', 'start': 19, 'text': ""to actually download and install it so I'm just going to take you through those steps now. And all we need to do is head on over to this website up here and it's elasticssearch.co and you can see the address just there. Now I'm going to follow the instructions for Windows but of course if you're on Linux or Mac just follow through it's very similar either way. So here we're going to install it on Windows using the MSI installer. So just scroll down here and we can see we can download the package from this link so download that and once you download it just open it and we'll see this window pop up. So once you see this window pop up we just go through with all of the default settings. So install as a service and continue through obviously if you do need to change anything change it but for me there's nothing here that I want to modify. Notice here we have the HTTP port and we're using 9200 we'll be using that later. We just continue through here default settings and then we click install and we just let that install. Okay so now that we've installed Elasticsearch we can go ahead and actually check that it's running. So to do that we're going to import Python requests and whenever we interact with Elasticsearch it's either going to be through Haystack or it will be through the request library and we'll just interact with the Elasticsearch API. So to check the health of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,192,2021-04-12 15:00:11 UTC,19,to actually download and install it so I'm just going to take you through those steps now. And all we need to do is head on over to this website up here and it's elasticssearch.co and you can see the address just there. Now I'm going to follow the instructions for Windows but of course if you're on Linux or Mac just follow through it's very similar either way. So here we're going to install it on Windows using the MSI installer. So just scroll down here and we can see we can download the package from this link so download that and once you download it just open it and we'll see this window pop up. So once you see this window pop up we just go through with all of the default settings. So install as a service and continue through obviously if you do need to change anything change it but for me there's nothing here that I want to modify. Notice here we have the HTTP port and we're using 9200 we'll be using that later. We just continue through here default settings and then we click install and we just let that install. Okay so now that we've installed Elasticsearch we can go ahead and actually check that it's running. So to do that we're going to import Python requests and whenever we interact with Elasticsearch it's either going to be through Haystack or it will be through the request library and we'll just interact with the Elasticsearch API. So to check the health of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t61.120000000000005,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 227, 'published': '2021-04-12 15:00:11 UTC', 'start': 61, 'text': ""So just scroll down here and we can see we can download the package from this link so download that and once you download it just open it and we'll see this window pop up. So once you see this window pop up we just go through with all of the default settings. So install as a service and continue through obviously if you do need to change anything change it but for me there's nothing here that I want to modify. Notice here we have the HTTP port and we're using 9200 we'll be using that later. We just continue through here default settings and then we click install and we just let that install. Okay so now that we've installed Elasticsearch we can go ahead and actually check that it's running. So to do that we're going to import Python requests and whenever we interact with Elasticsearch it's either going to be through Haystack or it will be through the request library and we'll just interact with the Elasticsearch API. So to check the health of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards."", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,227,2021-04-12 15:00:11 UTC,61,So just scroll down here and we can see we can download the package from this link so download that and once you download it just open it and we'll see this window pop up. So once you see this window pop up we just go through with all of the default settings. So install as a service and continue through obviously if you do need to change anything change it but for me there's nothing here that I want to modify. Notice here we have the HTTP port and we're using 9200 we'll be using that later. We just continue through here default settings and then we click install and we just let that install. Okay so now that we've installed Elasticsearch we can go ahead and actually check that it's running. So to do that we're going to import Python requests and whenever we interact with Elasticsearch it's either going to be through Haystack or it will be through the request library and we'll just interact with the Elasticsearch API. So to check the health of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards.,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t97.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 275, 'published': '2021-04-12 15:00:11 UTC', 'start': 97, 'text': ""be using that later. We just continue through here default settings and then we click install and we just let that install. Okay so now that we've installed Elasticsearch we can go ahead and actually check that it's running. So to do that we're going to import Python requests and whenever we interact with Elasticsearch it's either going to be through Haystack or it will be through the request library and we'll just interact with the Elasticsearch API. So to check the health of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards. If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,275,2021-04-12 15:00:11 UTC,97,be using that later. We just continue through here default settings and then we click install and we just let that install. Okay so now that we've installed Elasticsearch we can go ahead and actually check that it's running. So to do that we're going to import Python requests and whenever we interact with Elasticsearch it's either going to be through Haystack or it will be through the request library and we'll just interact with the Elasticsearch API. So to check the health of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards. If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t137.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 320, 'published': '2021-04-12 15:00:11 UTC', 'start': 137, 'text': ""of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards. If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,320,2021-04-12 15:00:11 UTC,137,of our cluster so essentially check that it's actually up and running. What we need to do is send a GET request to localhost and if you remember earlier we had it was port 9200. Of course if the port on yours was different modify it this is just the default value and after this we need to reach out to the cluster endpoint and we are checking the health and then we'll just format that as a JSON. So what you should see here is we have our cluster which is Elasticsearch. It may have a different name if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards. If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t176.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 352, 'published': '2021-04-12 15:00:11 UTC', 'start': 176, 'text': ""if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards. If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,352,2021-04-12 15:00:11 UTC,176,if you modified it but by default it's Elasticsearch. The status is yellow which basically just means we have one node up and running. You can have multiple nodes in Elasticsearch and for your cluster health to be green it will expect your shards of indexes to have backup shards across different nodes and obviously we can't do that if we only have one node but it's completely fine for us because we're just in development. If you're in production yes you'd probably want it to have those backup shards. If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t211.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 388, 'published': '2021-04-12 15:00:11 UTC', 'start': 211, 'text': ""If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,388,2021-04-12 15:00:11 UTC,211,If none of that made any sense don't worry about it we really don't need to know any of that for what we're doing here. Now what we can also do is we can check if we have any indices already. Now if I take a look at mine I will already have some indices set up which I've just set up prior to recording this and to check that we go to localhost again and this time we want to call the cat API which is what we would call whenever we want to see data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t258.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 434, 'published': '2021-04-12 15:00:11 UTC', 'start': 258, 'text': ""data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,434,2021-04-12 15:00:11 UTC,258,data in a table human readable format rather than JSON and what we're checking here are the indices. And we'll just add text on there so we can actually see that and this is quite messy so if we just print it instead look a bit cleaner. Okay so you can see I have these two indices you shouldn't I don't think have either of those no you won't have either of those so don't worry about that. Now what we are going to do is create a new index which will be called Aurelius and that is where we will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t296.55999999999995,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 470, 'published': '2021-04-12 15:00:11 UTC', 'start': 296, 'text': ""will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt."", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,470,2021-04-12 15:00:11 UTC,296,will put our documents. Now to actually implement that we will be going through the Haystack library which you can pip install farm Haystack and what we want to do is from Haystack dot document store elastic search import elastic search document store. So this is our document store instance and of course this is not aware of our elastic search instance we need to initialize that so we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt.,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t344.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 511, 'published': '2021-04-12 15:00:11 UTC', 'start': 344, 'text': ""we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt. I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character."", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,511,2021-04-12 15:00:11 UTC,344,we'll store it in a variable called doc store and all we write is elastic search document store. Now we need to initialize it with the parameters so it knows where to connect to our elastic search instance. So to do that we write host and this is local host. Now if you have a username and password set which you don't by default you will need to enter them in here. I don't have any set so no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt. I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character.,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t379.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 553, 'published': '2021-04-12 15:00:11 UTC', 'start': 379, 'text': ""no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt. I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character. So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph."", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,553,2021-04-12 15:00:11 UTC,379,no worries. And then we also need to specify our index and at the moment we don't have an Aurelius index and that's fine because this will initialize it for us. So we'll just call it Aurelius. Now if we go down here we can see what it actually did so it sent a put request to here. localhost 9200 Aurelius. So that's how you create a new index. After that what we want to do is first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt. I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character. So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph.,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t416.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 588, 'published': '2021-04-12 15:00:11 UTC', 'start': 416, 'text': ""first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt. I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character. So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph. So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,588,2021-04-12 15:00:11 UTC,416,first import our data. So we have the data here which I got from this website and process with this script which you can find on GitHub. I'll keep a link in the description so you can just go and copy that if you need to. Now I haven't really done much pre-process it's pretty straightforward and all you need to do here is actually open that data. So we do that with open and from here that data file is located two folders up in a data folder it's called meditations.txt. I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character. So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph. So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t461.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 623, 'published': '2021-04-12 15:00:11 UTC', 'start': 461, 'text': ""I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character. So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph. So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these."", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,623,2021-04-12 15:00:11 UTC,461,I'm going to be reading that and all we do is data equals f.read and then if we just have a quick look at the first 100 characters there we see that we have this newline character and that signifies a new paragraph from the text. So what we want to do here is split the data and then we can see that we have a newline character. So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph. So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these.,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t496.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 667, 'published': '2021-04-12 15:00:11 UTC', 'start': 496, 'text': ""So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph. So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these. So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,667,2021-04-12 15:00:11 UTC,496,So what we want to do is split the data by newline and then if we check the length of that see that we have 508 separate paragraphs in there. So what we now want to do is we want to modify this data so that it's in the correct format for Haystack and Elasticsearch. So that format looks like this so it expects a list of dictionaries where each dictionary looks like this from the text and inside here we would have our paragraph. So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these. So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t541.12,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 717, 'published': '2021-04-12 15:00:11 UTC', 'start': 541, 'text': ""So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these. So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text we have the paragraph and then in here we have this meta with a source which is always Meditations at the moment so that looks pretty good and we'll just double check the length again it should be 508 okay perfect now what we need to do is index all of these documents into our Elasticsearch instance and to do that it's super easy all we do is call docstore because we're doing this through Haystack now and we do write documents and we just pass in our data.json and that should work."", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,717,2021-04-12 15:00:11 UTC,541,So each one of these items here and then there's another optional field called meta and meta contains a dictionary and in here we can put whatever we want. So for us I don't think at the moment there's really that much to put into here other than where it came from so the book or maybe the source is probably a better word to use here and all of these are coming from Meditations. Now later on we will probably add a few other books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these. So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text we have the paragraph and then in here we have this meta with a source which is always Meditations at the moment so that looks pretty good and we'll just double check the length again it should be 508 okay perfect now what we need to do is index all of these documents into our Elasticsearch instance and to do that it's super easy all we do is call docstore because we're doing this through Haystack now and we do write documents and we just pass in our data.json and that should work.,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t577.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 759, 'published': '2021-04-12 15:00:11 UTC', 'start': 577, 'text': ""books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these. So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text we have the paragraph and then in here we have this meta with a source which is always Meditations at the moment so that looks pretty good and we'll just double check the length again it should be 508 okay perfect now what we need to do is index all of these documents into our Elasticsearch instance and to do that it's super easy all we do is call docstore because we're doing this through Haystack now and we do write documents and we just pass in our data.json and that should work. Okay cool so we can see here what it's done as it's sent a POST request to the Bulk API and sent two of them I assume because it can only send so many documents at once so that's pretty cool and now what I want to check is that we actually have 508 documents in our Elasticsearch instance so to do that we're going to revert back to requests so we'll do requests.get again go to our localhost"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,759,2021-04-12 15:00:11 UTC,577,books as well and then the source will be different and when we return that item from our retriever and our reader we'll at least be able to see which book came from him. It would also be pretty cool to maybe include like a page number or something but at the moment with this there are no page numbers included so we're not doing that at the moment. So that's the format that we need and it's going to be a list of these. So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text we have the paragraph and then in here we have this meta with a source which is always Meditations at the moment so that looks pretty good and we'll just double check the length again it should be 508 okay perfect now what we need to do is index all of these documents into our Elasticsearch instance and to do that it's super easy all we do is call docstore because we're doing this through Haystack now and we do write documents and we just pass in our data.json and that should work. Okay cool so we can see here what it's done as it's sent a POST request to the Bulk API and sent two of them I assume because it can only send so many documents at once so that's pretty cool and now what I want to check is that we actually have 508 documents in our Elasticsearch instance so to do that we're going to revert back to requests so we'll do requests.get again go to our localhost,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
Vwq7Ucp9UCw-t610.5600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 819, 'published': '2021-04-12 15:00:11 UTC', 'start': 610, 'text': ""So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text we have the paragraph and then in here we have this meta with a source which is always Meditations at the moment so that looks pretty good and we'll just double check the length again it should be 508 okay perfect now what we need to do is index all of these documents into our Elasticsearch instance and to do that it's super easy all we do is call docstore because we're doing this through Haystack now and we do write documents and we just pass in our data.json and that should work. Okay cool so we can see here what it's done as it's sent a POST request to the Bulk API and sent two of them I assume because it can only send so many documents at once so that's pretty cool and now what I want to check is that we actually have 508 documents in our Elasticsearch instance so to do that we're going to revert back to requests so we'll do requests.get again go to our localhost 9200 and here we need to specify the index that we want to count the number of entries in and then all we do is add count onto the end there and this will return a JSON object so we do this so that we can see it and sure enough we have 508 items in that document store. So if we head on back to our original plan so up here we had meditations we've now got that and we've also set up the first part of our sack over here so Elastic now has meditations in there"", 'title': 'How to Index Q&A Data With Haystack and Elasticsearch', 'url': 'https://youtu.be/Vwq7Ucp9UCw'}",UCv83tO5cePwHMt1952IVVHw,819,2021-04-12 15:00:11 UTC,610,So to do that we'll just do some list comprehension. So we're going to write this and let's just copy this I think yeah it should be fine we'll copy this and just indent that and in here we have our paragraph and sources Meditations for all of them and then we just write for paragraph in and data okay so yeah that should work and if we just and if we just check what we have here okay so that's that's what we want so we have text we have the paragraph and then in here we have this meta with a source which is always Meditations at the moment so that looks pretty good and we'll just double check the length again it should be 508 okay perfect now what we need to do is index all of these documents into our Elasticsearch instance and to do that it's super easy all we do is call docstore because we're doing this through Haystack now and we do write documents and we just pass in our data.json and that should work. Okay cool so we can see here what it's done as it's sent a POST request to the Bulk API and sent two of them I assume because it can only send so many documents at once so that's pretty cool and now what I want to check is that we actually have 508 documents in our Elasticsearch instance so to do that we're going to revert back to requests so we'll do requests.get again go to our localhost 9200 and here we need to specify the index that we want to count the number of entries in and then all we do is add count onto the end there and this will return a JSON object so we do this so that we can see it and sure enough we have 508 items in that document store. So if we head on back to our original plan so up here we had meditations we've now got that and we've also set up the first part of our sack over here so Elastic now has meditations in there,How to Index Q&A Data With Haystack and Elasticsearch,https://youtu.be/Vwq7Ucp9UCw
IC9FaVPKlYc-t22.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 197, 'published': '2021-06-15 15:00:19 UTC', 'start': 22, 'text': ""style of language in our specific use cases. So we'll jump straight into it, but what we're going to see is essentially two different methods applied together. So when we're pre-training, we're using something called mass language modeling or MLM and also net sentence prediction or NSP. Now in a few previous videos, I've covered all of these. So if you do want to go into a little more depth, then I would definitely recommend having a look at those. But in this video, we're just going to go straight into actually training a BERT model using both of those methods using the pre-training class. So we need first to import everything that we need. So I'm going to import requests because I'm going to use request download data we're using, which is from here. You find a link in the description for that. And we also need to import our tokenizer and model classes from transformers. So from transformers, we're going to import BERT tokenizer and also BERT for pre-training. Now, like I said before, this BERT for pre-training class contains both an MLM head and an NSP head. So once we have that, we also need to import torch as well. So let me import torch. Once we have that, we can initialize our tokenizer and model. So we initialize our tokenizer like this. So BERT tokenizer and from pre-trained. And we're going to be using the BERT base uncased model. Obviously, you can use whichever BERT model you'd like. And for our model, we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,197,2021-06-15 15:00:19 UTC,22,"style of language in our specific use cases. So we'll jump straight into it, but what we're going to see is essentially two different methods applied together. So when we're pre-training, we're using something called mass language modeling or MLM and also net sentence prediction or NSP. Now in a few previous videos, I've covered all of these. So if you do want to go into a little more depth, then I would definitely recommend having a look at those. But in this video, we're just going to go straight into actually training a BERT model using both of those methods using the pre-training class. So we need first to import everything that we need. So I'm going to import requests because I'm going to use request download data we're using, which is from here. You find a link in the description for that. And we also need to import our tokenizer and model classes from transformers. So from transformers, we're going to import BERT tokenizer and also BERT for pre-training. Now, like I said before, this BERT for pre-training class contains both an MLM head and an NSP head. So once we have that, we also need to import torch as well. So let me import torch. Once we have that, we can initialize our tokenizer and model. So we initialize our tokenizer like this. So BERT tokenizer and from pre-trained. And we're going to be using the BERT base uncased model. Obviously, you can use whichever BERT model you'd like. And for our model, we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t58.72,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 237, 'published': '2021-06-15 15:00:19 UTC', 'start': 58, 'text': ""going to go straight into actually training a BERT model using both of those methods using the pre-training class. So we need first to import everything that we need. So I'm going to import requests because I'm going to use request download data we're using, which is from here. You find a link in the description for that. And we also need to import our tokenizer and model classes from transformers. So from transformers, we're going to import BERT tokenizer and also BERT for pre-training. Now, like I said before, this BERT for pre-training class contains both an MLM head and an NSP head. So once we have that, we also need to import torch as well. So let me import torch. Once we have that, we can initialize our tokenizer and model. So we initialize our tokenizer like this. So BERT tokenizer and from pre-trained. And we're going to be using the BERT base uncased model. Obviously, you can use whichever BERT model you'd like. And for our model, we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute. So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,237,2021-06-15 15:00:19 UTC,58,"going to go straight into actually training a BERT model using both of those methods using the pre-training class. So we need first to import everything that we need. So I'm going to import requests because I'm going to use request download data we're using, which is from here. You find a link in the description for that. And we also need to import our tokenizer and model classes from transformers. So from transformers, we're going to import BERT tokenizer and also BERT for pre-training. Now, like I said before, this BERT for pre-training class contains both an MLM head and an NSP head. So once we have that, we also need to import torch as well. So let me import torch. Once we have that, we can initialize our tokenizer and model. So we initialize our tokenizer like this. So BERT tokenizer and from pre-trained. And we're going to be using the BERT base uncased model. Obviously, you can use whichever BERT model you'd like. And for our model, we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute. So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t93.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 273, 'published': '2021-06-15 15:00:19 UTC', 'start': 93, 'text': ""and also BERT for pre-training. Now, like I said before, this BERT for pre-training class contains both an MLM head and an NSP head. So once we have that, we also need to import torch as well. So let me import torch. Once we have that, we can initialize our tokenizer and model. So we initialize our tokenizer like this. So BERT tokenizer and from pre-trained. And we're going to be using the BERT base uncased model. Obviously, you can use whichever BERT model you'd like. And for our model, we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute. So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,273,2021-06-15 15:00:19 UTC,93,"and also BERT for pre-training. Now, like I said before, this BERT for pre-training class contains both an MLM head and an NSP head. So once we have that, we also need to import torch as well. So let me import torch. Once we have that, we can initialize our tokenizer and model. So we initialize our tokenizer like this. So BERT tokenizer and from pre-trained. And we're going to be using the BERT base uncased model. Obviously, you can use whichever BERT model you'd like. And for our model, we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute. So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t145.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 310, 'published': '2021-06-15 15:00:19 UTC', 'start': 145, 'text': ""we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute. So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence,"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,310,2021-06-15 15:00:19 UTC,145,"we have the BERT for pre-training class. So that's our tokenizer model. Now let's get our data. Don't need to worry about that warning. It's just telling us that we need to train it, basically, if we want to use it for inference predictions. So we get our data. We're going to pull it from here. So let me copy that. And it's just requests.get. And paste that in there. And we should see a 200 code. That's good. And so we just extracted data using the text attribute. So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence,",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t184.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 350, 'published': '2021-06-15 15:00:19 UTC', 'start': 184, 'text': ""So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence, we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's,"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,350,2021-06-15 15:00:19 UTC,184,"So text equals that. We also need to split it because it's a set of paragraphs that are split by a new line character. And we can see those in here. Now we need to power data both for NSP and MLM. So we'll go with NSP first. And to do that, we need to create a set of random sentences. So sentence A and B. And then we need to create a set of random sentences. So we need to create a set of random sentences. So sentence A and B, where the sentence B is not related to sentence A. We need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence, we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's,",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t222.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 394, 'published': '2021-06-15 15:00:19 UTC', 'start': 222, 'text': ""need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence, we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's, and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,394,2021-06-15 15:00:19 UTC,222,"need roughly 50% of those. And then the other 50% we want it to be sentence A is followed by sentence B. So they are more coherent. So we're basically teaching BERT to distinguish between coherence between sentences. So like long term dependencies. And we just want to be aware that within our text, so we have this one paragraph that has multiple sentences. So we split by this. We have those. So we need to create essentially a list of all of the different sentences that we have that we can just pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence, we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's, and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t261.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 422, 'published': '2021-06-15 15:00:19 UTC', 'start': 261, 'text': ""pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence, we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's, and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,422,2021-06-15 15:00:19 UTC,261,"pull from when we're creating our training data for NSP. Now to do that, we're going to use this comprehension here. And what we do is write sentence. So for each sentence, for each paragraph in the text. So this variable. For sentence in para.split. So this is where we're getting our sentence variable from. And we just want to be aware of if we have a look at this one, we see we get this empty sentence, we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's, and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t296.40000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 450, 'published': '2021-06-15 15:00:19 UTC', 'start': 296, 'text': ""we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's, and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,450,2021-06-15 15:00:19 UTC,296,"we get that for all of our paragraphs. So we want to not include those. So we say if sentence is not equal to that empty sentence. And we're also going to need to get the length of that bag for later as well. And now what we do is create our NSP training data. So we want that 50-50 split. So we're going to use the random library to create that 50-50 randomness. We want to initialize a list of sentence a's, a list of sentence b's, and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t335.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 490, 'published': '2021-06-15 15:00:19 UTC', 'start': 335, 'text': ""and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,490,2021-06-15 15:00:19 UTC,335,"and also a list of labels. And then what we do is we're going to loop through each paragraph in our text. So for paragraph in text. We want to extract each sentence from the paragraph. So we're going to use it similar to what we've done here. So write sentences. This is going to be a list of all the sentences within each paragraph. So sentence for sentence in para.split by a period character. And we also want to make sure we're not including those empty ones. So if sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t374.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 538, 'published': '2021-06-15 15:00:19 UTC', 'start': 374, 'text': ""sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,538,2021-06-15 15:00:19 UTC,374,"sentence is not equal to empty, then once we're there, what we want to do is we want to get the number of sentences within each sentence or sentences variable. So just get length. And the reason we do that is because we want to check that a couple of times in the next few lines of code. And first time we check that is now. So we check that the number of sentences is greater than one. Now this because we're concatenating two sentences to create our training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t412.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 576, 'published': '2021-06-15 15:00:19 UTC', 'start': 412, 'text': ""training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,576,2021-06-15 15:00:19 UTC,412,"training data, we don't want to get just one sentence. We need it where we have, for example, in this one, we have multiple sentences so that we can select like this sentence followed by this sentence. We can't do that with these because there's no guarantee that this paragraph here is going to be talking about the same topic as this paragraph here. So we just avoid that. And in here, first thing we want to do is set out start sentence. So this is where sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t438.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 615, 'published': '2021-06-15 15:00:19 UTC', 'start': 438, 'text': ""sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us,"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,615,2021-06-15 15:00:19 UTC,438,"sentence A is going to come from. And we're going to randomly select, say for this example, we want to randomly select any of the first one, two, three sentences. Okay, we'd want to select any of these three, but not this one, because if this sentence A, we don't have a sentence B which follows it to extract. So we write random, randint 0 up to the length of num sentences minus two. Now we can now get our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us,",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t474.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 656, 'published': '2021-06-15 15:00:19 UTC', 'start': 474, 'text': ""our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us, which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,656,2021-06-15 15:00:19 UTC,474,"our sentence A, which is append, and we just write sentences start. And then for our sentence B, 50% we want to select random one from bag up here, 50% of time we want to select the genuine next sentence. So say if random.random, so this will select a random float between 0 and 1, it's greater than 0.5. And sentence B is going to be, we'll make this our coherent version. So sentences start plus one. And that means our label will have to be zero because that means that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us, which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t522.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 698, 'published': '2021-06-15 15:00:19 UTC', 'start': 522, 'text': ""that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us, which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,698,2021-06-15 15:00:19 UTC,522,"that these two sentences are coherent. Sentence B does follow sentence A. Otherwise, we select a random sentence for sentence B. So do append, and here we would write bag, and we need to select a random one. So we do random, same as we did earlier on for the start, we do random, randint from zero to the length of the bag size minus one. So we also need to do the label, which is going to be one in this case. We can execute that. Now that will work. I go a little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us, which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t563.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 740, 'published': '2021-06-15 15:00:19 UTC', 'start': 563, 'text': ""little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us, which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,740,2021-06-15 15:00:19 UTC,563,"little more into depth on this in the previous NSP video. So I'll leave a link to that in the description if you want to go through it. And now what we can do is tokenize our data. So to do that, we just write inputs and we use a tokenizer. So this is just normal, you know, hugging face transformers. And we just write sentence A and sentence B. So hugging face transformers will know what we want to do with that. It will deal with formatting for us, which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t595.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 781, 'published': '2021-06-15 15:00:19 UTC', 'start': 595, 'text': ""which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,781,2021-06-15 15:00:19 UTC,595,"which is pretty useful. We want to return PyTorch tensors. So return tensors equals pt. And we need to set everything to a max length of 512 tokens. So max length equals 512. The truncation needs to be set to true. And we also need to set padding equal to max length. Okay. So that creates three different tensors for us. Impart IDs, token type IDs, and attention mask. Now for the pre-trained model, we need two more tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t642.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 830, 'published': '2021-06-15 15:00:19 UTC', 'start': 642, 'text': ""tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,830,2021-06-15 15:00:19 UTC,642,"tensors. We need our next sentence label tensor. So to create that, we write inputs, next sentence label. And that needs to be a long tensor containing our labels, which we created before in the correct dimensionality. So that's why we're using the list here and the transpose. And we can have a look at what that creates as well. So look at the first 10. We get that. Okay. And now what we want to do is create our mask data. So we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t684.5600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 885, 'published': '2021-06-15 15:00:19 UTC', 'start': 684, 'text': ""we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero. So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,885,2021-06-15 15:00:19 UTC,684,"we need the labels for our mask first. So when we do this, what we'll do is we're going to clone the input IDs tensor. We're going to use that clone for the labels tensor. And then we're going to go back to our input IDs and mask around 15% of the tokens in that tensor. So let's create that labels tensor. It's going to be equal to inputs, input IDs, detach, and clone. Okay. So now we've got our mask data. Okay. So now we'll see in here, we have all of the tensors we need, but we still need to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero. So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t727.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 928, 'published': '2021-06-15 15:00:19 UTC', 'start': 727, 'text': ""to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero. So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,928,2021-06-15 15:00:19 UTC,727,"to mask around 15% of these before moving on to training our model. And to do that, we'll use, we'll create a random array using the torch rend. That needs to be in the same shape as our input IDs. And that will just create a big tensor between values of zero to one. And what we want to do is mask around 15% of those. So we write something like this. Okay. And that will give us our mask here, but we also don't want to mask special tokens, which we are doing here. We're masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero. So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t764.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 970, 'published': '2021-06-15 15:00:19 UTC', 'start': 764, 'text': ""masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero. So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID. Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,970,2021-06-15 15:00:19 UTC,764,"masking classification tokens and also masking padding tokens up here. So we need to add a little bit more logic to that. So let me just add this to a variable. So we add that logic, which says, and input IDs is not equal to one zero one, which is our CLS token, which is what we get down here. See the impact. See we get faults now. And we also want to do the same for our separator tokens, which is one zero two. We can't see any of those. And our padding tokens, we use zero. So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID. Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t812.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1011, 'published': '2021-06-15 15:00:19 UTC', 'start': 812, 'text': ""So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID. Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index. And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1011,2021-06-15 15:00:19 UTC,812,"So you see these are all that will go false now, like so. So that's our masking array. And now what we want to do is loop through all of these, extract the points at which they are not false. So where we have the mask and use those indices values to mask our actual input IDs up here. To do that, we go for i in range inputs, input IDs dot shape zero. This is like iterating through each row. And what we do here is we get selection. So these are the indices where we have true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID. Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index. And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t861.44,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1058, 'published': '2021-06-15 15:00:19 UTC', 'start': 861, 'text': ""true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID. Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index. And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1058,2021-06-15 15:00:19 UTC,861,"true values and mask array. And we do that using torch flatten mask array at the given index, where they are non-zero. And we want to create a list from that. Okay. So we have that. Oh, and so I want to show you what the selection looks like quickly. So it's just a selection of indices to mask. And we want to apply that to our inputs, input IDs. So at the current index, and we select those specific items and we set them equal to one zero three, which is the masking token ID. Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index. And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t911.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1102, 'published': '2021-06-15 15:00:19 UTC', 'start': 911, 'text': ""Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index. And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay. So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1102,2021-06-15 15:00:19 UTC,911,"Okay. So that's our masking. And now what we need to do is we need to take all of our data here and load it into a PyTorch data loader. And to do that, we need to reform our data into a PyTorch data set object. And we do that here. So main thing to note is we pass our data into this initialization that assigns them to this self encodings attribute. And then here we say, okay, given a certain index, we want to extract the tensors in a dictionary format for that index. And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay. So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t954.16,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1148, 'published': '2021-06-15 15:00:19 UTC', 'start': 954, 'text': ""And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay. So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM. And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1148,2021-06-15 15:00:19 UTC,954,"And then here we're just passing the lengths of how many tensors or how many samples we have in the full data set. So run that. We initialize our data sets using that class. So right, data set equals meditations data set, pass our data in there, which is inputs. And then with that, we can create our data loader like this. So torch utils data data loader. And we have data set. Okay. So that's ready. Now we need to set up our training loop. So first thing we need to do is check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay. So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM. And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t997.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1193, 'published': '2021-06-15 15:00:19 UTC', 'start': 997, 'text': ""check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay. So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM. And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1193,2021-06-15 15:00:19 UTC,997,"check if we are on GPU or not. If we are, we use it and we do that like so. So device equals torch device cuda if torch cuda is available. Else torch device CPU. So that's saying use the GPU if we have a cuda enabled GPU, otherwise use CPU. And then what we want to do is move our model over to that device. And we also want to activate the training mode of our model. And then we need to initialize our optimizer. I'm going to be using Adam with weighted decay. So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM. And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t1038.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1222, 'published': '2021-06-15 15:00:19 UTC', 'start': 1038, 'text': ""So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM. And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels. Labels and also labels. Okay. And now we can actually process that through our model. So in here, we just need to pass all of these tensors that we have. So input IDs. And then we have token type IDs. Just copy this. Attention mask. Next sentence label."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1222,2021-06-15 15:00:19 UTC,1038,"So from transformers import Adam w. And initialize it like this. So optim equals Adam w. We pass our model parameters to that. And we also pass a learning rate. So learning rate is going to be 5e to the minus 5. Okay. And now we can create our training loop. So you're going to use TQDM to create the progress bar. And we're going to go through two epochs. So for epoch in range two, we initialize our loop by wrapping it within TQDM. And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels. Labels and also labels. Okay. And now we can actually process that through our model. So in here, we just need to pass all of these tensors that we have. So input IDs. And then we have token type IDs. Just copy this. Attention mask. Next sentence label.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t1084.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1260, 'published': '2021-06-15 15:00:19 UTC', 'start': 1084, 'text': ""And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels. Labels and also labels. Okay. And now we can actually process that through our model. So in here, we just need to pass all of these tensors that we have. So input IDs. And then we have token type IDs. Just copy this. Attention mask. Next sentence label. And labels. Okay. So that's quite a lot going into our model. And now what we want to do is extract the loss from that. Then we calculate loss for every parameter in our model. And then using that, we can update our gradients using our optimizer. And then what we want to do is print the relevant info to our progress bar that we set up using TQDM and loop. So loop set description."", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1260,2021-06-15 15:00:19 UTC,1084,"And in here we have our data loader. And we set leave equal to true so that we can see that progress bar. And then we loop through each batch within that loop. Up here, so I didn't actually set the batches. My mistake. So up here we want to set where we initialize the data loader. We want to set batch size equal to 16. And also shuffle the data set as well. Okay. So for batch in loop, here we want to initialize the gradient on our optimizer. And then we need to load in each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels. Labels and also labels. Okay. And now we can actually process that through our model. So in here, we just need to pass all of these tensors that we have. So input IDs. And then we have token type IDs. Just copy this. Attention mask. Next sentence label. And labels. Okay. So that's quite a lot going into our model. And now what we want to do is extract the loss from that. Then we calculate loss for every parameter in our model. And then using that, we can update our gradients using our optimizer. And then what we want to do is print the relevant info to our progress bar that we set up using TQDM and loop. So loop set description.",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
IC9FaVPKlYc-t1129.52,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1294, 'published': '2021-06-15 15:00:19 UTC', 'start': 1129, 'text': ""each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels. Labels and also labels. Okay. And now we can actually process that through our model. So in here, we just need to pass all of these tensors that we have. So input IDs. And then we have token type IDs. Just copy this. Attention mask. Next sentence label. And labels. Okay. So that's quite a lot going into our model. And now what we want to do is extract the loss from that. Then we calculate loss for every parameter in our model. And then using that, we can update our gradients using our optimizer. And then what we want to do is print the relevant info to our progress bar that we set up using TQDM and loop. So loop set description. And here I was going to put the epoch info. So the epoch we're currently on. And then I also want to set the post fix. Which will contain the loss information. So loss.item. Okay. We can run that. And you see that our model is now training. So we're now training a model using both assignment modeling and net sentence prediction. And we haven't needed to take any structured data"", 'title': 'Training BERT #5 - Training With BertForPretraining', 'url': 'https://youtu.be/IC9FaVPKlYc'}",UCv83tO5cePwHMt1952IVVHw,1294,2021-06-15 15:00:19 UTC,1129,"each of our tensors, which there are quite a few of them. So we have inputs.keys. We need to load in each one of these. So input IDs equals batch. We access this like a dictionary. So input IDs. We also want to move each one of those tensors that we're using to our device. So we do that for each one of those. And we have attention mask. And next sentence labels and also labels. Labels and also labels. Okay. And now we can actually process that through our model. So in here, we just need to pass all of these tensors that we have. So input IDs. And then we have token type IDs. Just copy this. Attention mask. Next sentence label. And labels. Okay. So that's quite a lot going into our model. And now what we want to do is extract the loss from that. Then we calculate loss for every parameter in our model. And then using that, we can update our gradients using our optimizer. And then what we want to do is print the relevant info to our progress bar that we set up using TQDM and loop. So loop set description. And here I was going to put the epoch info. So the epoch we're currently on. And then I also want to set the post fix. Which will contain the loss information. So loss.item. Okay. We can run that. And you see that our model is now training. So we're now training a model using both assignment modeling and net sentence prediction. And we haven't needed to take any structured data",Training BERT #5 - Training With BertForPretraining,https://youtu.be/IC9FaVPKlYc
DBsxUSUhfRg-t9.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 105, 'published': '2021-04-15 15:00:10 UTC', 'start': 9, 'text': ""paragraphs from meditations so we did that in this script here and All together we only have not that much data, 508 paragraphs or documents within our document store so What we now want to do is set up the next part of our Retriever reader stack, which is the retriever and What the retriever will do is given a query it will communicate with our Elasticsearch document store and return a Certain number of contexts which are the paragraphs in our case that it thinks are most relevant to our query So That's what we are going to be doing here and The first thing that we need to do is initialize our document store again, so I'm just going to copy these and paste them here and This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,105,2021-04-15 15:00:10 UTC,9,"paragraphs from meditations so we did that in this script here and All together we only have not that much data, 508 paragraphs or documents within our document store so What we now want to do is set up the next part of our Retriever reader stack, which is the retriever and What the retriever will do is given a query it will communicate with our Elasticsearch document store and return a Certain number of contexts which are the paragraphs in our case that it thinks are most relevant to our query So That's what we are going to be doing here and The first thing that we need to do is initialize our document store again, so I'm just going to copy these and paste them here and This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t31.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 130, 'published': '2021-04-15 15:00:10 UTC', 'start': 31, 'text': ""Retriever reader stack, which is the retriever and What the retriever will do is given a query it will communicate with our Elasticsearch document store and return a Certain number of contexts which are the paragraphs in our case that it thinks are most relevant to our query So That's what we are going to be doing here and The first thing that we need to do is initialize our document store again, so I'm just going to copy these and paste them here and This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,130,2021-04-15 15:00:10 UTC,31,"Retriever reader stack, which is the retriever and What the retriever will do is given a query it will communicate with our Elasticsearch document store and return a Certain number of contexts which are the paragraphs in our case that it thinks are most relevant to our query So That's what we are going to be doing here and The first thing that we need to do is initialize our document store again, so I'm just going to copy these and paste them here and This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t52.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 163, 'published': '2021-04-15 15:00:10 UTC', 'start': 52, 'text': ""So That's what we are going to be doing here and The first thing that we need to do is initialize our document store again, so I'm just going to copy these and paste them here and This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,163,2021-04-15 15:00:10 UTC,52,"So That's what we are going to be doing here and The first thing that we need to do is initialize our document store again, so I'm just going to copy these and paste them here and This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t74.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 195, 'published': '2021-04-15 15:00:10 UTC', 'start': 74, 'text': ""This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,195,2021-04-15 15:00:10 UTC,74,"This would just initialize it from what we've already built so it's using the same index that already exists so Just initialize that and once we have our document store. Okay, cool. We have that now Now what we want to do is set up our DPR, which is a dense passage retriever, which essentially uses dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t100.42,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 237, 'published': '2021-04-15 15:00:10 UTC', 'start': 100, 'text': ""dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,237,2021-04-15 15:00:10 UTC,100,"dense vectors and a type of efficient similarity search to embed these indexes as dense vectors and then once it comes to actually searching And finding the most similar or the most relevant Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t118.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 258, 'published': '2021-04-15 15:00:10 UTC', 'start': 118, 'text': ""Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,258,2021-04-15 15:00:10 UTC,118,"Documents later on it will use those dense vectors and find the most similar ones So I'll explain that a little bit better in a moment So first what we want to do is actually initialize that So we do from Haystack dense retriever Import dense passage retriever Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t155.14000000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 279, 'published': '2021-04-15 15:00:10 UTC', 'start': 155, 'text': ""Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,279,2021-04-15 15:00:10 UTC,155,"Sorry, it's the other way around here so retriever dense And then we'll put into a Variable called retriever which uses the dense passage retriever from up here And in here we need to pass a few parameters. So the first thing is the document store. So the document store is just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t185.38,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 299, 'published': '2021-04-15 15:00:10 UTC', 'start': 185, 'text': ""just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,299,2021-04-15 15:00:10 UTC,185,"just what we've already initialized up so and Then we need to initialize two different models so it's the query embedding model And the passage embedding model Now behind the scenes Haystack is using the Hugging Face Transformers library So what we'll do is we'll head over to the Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t214.54,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 324, 'published': '2021-04-15 15:00:10 UTC', 'start': 214, 'text': ""Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,324,2021-04-15 15:00:10 UTC,214,"Models over there and see which embedding models we can use for DPR Okay, so here let's just search for DPR and you'll find we have all of these models from Facebook AI Now with DPR the reason that it's so useful for question answering is that we have What are two different models that encode the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t252.06,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 351, 'published': '2021-04-15 15:00:10 UTC', 'start': 252, 'text': ""the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,351,2021-04-15 15:00:10 UTC,252,"the text that we pass into it so we have this sort of setup during training and What we see down here Are these two models we have this EP, BERT, and EMP models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t267.98,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 377, 'published': '2021-04-15 15:00:10 UTC', 'start': 267, 'text': ""models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,377,2021-04-15 15:00:10 UTC,267,"models we have this EP, BERT encoder And we also have this EQ, BERT encoder. Now the EP, BERT encoder encodes the passages or the context So essentially the paragraphs that we have fed into our elastic search model This is what we'll be encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t289.74,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 400, 'published': '2021-04-15 15:00:10 UTC', 'start': 289, 'text': ""encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,400,2021-04-15 15:00:10 UTC,289,encoding them into these vectors here Now this is during training this whole graph. So all we will actually see when we're encoding these vectors is we will see the EP encoder And this will create the EP vectors And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question,Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t315.98,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 423, 'published': '2021-04-15 15:00:10 UTC', 'start': 315, 'text': ""And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,423,2021-04-15 15:00:10 UTC,315,And all we're going to do is feed in all of the documents from elastic search into this Now once all of these have been encoded We then have a new set of dense vectors And then we'll have a new set of dense vectors And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context,Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t343.26,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 448, 'published': '2021-04-15 15:00:10 UTC', 'start': 343, 'text': ""And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,448,2021-04-15 15:00:10 UTC,343,And all of those Will be fed back into our document store so back into elastic Now when it comes to performing similarity search later on We're going to ask a question and that question will be processed by the EQ encoder So here we have our EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector,Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t369.82,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 477, 'published': '2021-04-15 15:00:10 UTC', 'start': 369, 'text': ""EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector So one example that I like to use Is if our question Was What is the capital of France"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,477,2021-04-15 15:00:10 UTC,369,EQ encoder And we have our question so that will go into here And that will encode our question And then send it over to elastic and say okay what are the most similar vectors to this vector that we created from a question And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector So one example that I like to use Is if our question Was What is the capital of France,Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t394.14,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 502, 'published': '2021-04-15 15:00:10 UTC', 'start': 394, 'text': ""And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector So one example that I like to use Is if our question Was What is the capital of France The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,502,2021-04-15 15:00:10 UTC,394,And the reason that we're asking this question is because We're going to be using a new set of vectors to train our question And the reason that DPR is so good is That if you look at the training down here We are creating these EP vectors and these EQ vectors that are matching so where we have a matching question to a matching context We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector So one example that I like to use Is if our question Was What is the capital of France The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic,Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t418.54,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 526, 'published': '2021-04-15 15:00:10 UTC', 'start': 418, 'text': ""We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector So one example that I like to use Is if our question Was What is the capital of France The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,526,2021-04-15 15:00:10 UTC,418,"We are training them To maximize the dot product And the alignment between those two vectors so what happens is That a relevant passage and a relevant question Will come out to have a very similar vector So one example that I like to use Is if our question Was What is the capital of France The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t438.06,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 547, 'published': '2021-04-15 15:00:10 UTC', 'start': 438, 'text': ""So one example that I like to use Is if our question Was What is the capital of France The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,547,2021-04-15 15:00:10 UTC,438,"So one example that I like to use Is if our question Was What is the capital of France The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t466.3,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 565, 'published': '2021-04-15 15:00:10 UTC', 'start': 466, 'text': ""The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,565,2021-04-15 15:00:10 UTC,466,"The embedding that I will create from that will create a context that looks something like the Capital Of France Is and you know something here we don't know what it will put because it doesn't actually know what the capital France is It's just doing linguistic Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t493.14,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 581, 'published': '2021-04-15 15:00:10 UTC', 'start': 493, 'text': ""Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,581,2021-04-15 15:00:10 UTC,493,"Transformations to try and figure out what sort of context the answer would come from and then of course when you feed this context into elastic The most similar Vector will be the one which contains the answer to our question Okay, because the answer to our question which is something like the capital France is Paris now We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t517.56,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 604, 'published': '2021-04-15 15:00:10 UTC', 'start': 517, 'text': ""We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,604,2021-04-15 15:00:10 UTC,517,"We don't have Paris here But it will be able to figure that out because it will be the most similar sequence to the context that DPR has produced Now back to Hugging face here. You can see we have these multiple DPR Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t536.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 628, 'published': '2021-04-15 15:00:10 UTC', 'start': 536, 'text': ""Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,628,2021-04-15 15:00:10 UTC,536,"Models and what we want is a pair. We want a question encoder And a ctx which is context encoder now, we'll be using this single nq base So what I'll do is just copy this And in here we just add in our model Okay, so that's a question encoder Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t559.2399999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 650, 'published': '2021-04-15 15:00:10 UTC', 'start': 559, 'text': ""Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,650,2021-04-15 15:00:10 UTC,559,"Now what we also need is the context encoder Which is instead of question here We just add ctx Now we have two other parameters that we need to add in here Which is ux and ctx So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t575.3199999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 668, 'published': '2021-04-15 15:00:10 UTC', 'start': 575, 'text': ""So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,668,2021-04-15 15:00:10 UTC,575,"So we're going to use the same parameter Two other parameters that we need to add in here, which is use gpu Which is if you're using a gpu, obviously you set this to true. If not you go with faults it will Take a little bit of time to process this if you are not using a gpu though Then we also add embed Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t598.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 696, 'published': '2021-04-15 15:00:10 UTC', 'start': 598, 'text': ""Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,696,2021-04-15 15:00:10 UTC,598,"Title equals true as well now what we should see is This will execute without error, hopefully Okay, great. And then what we need to do is update the embeddings within elastix search So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t614.6800000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 706, 'published': '2021-04-15 15:00:10 UTC', 'start': 614, 'text': ""So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,706,2021-04-15 15:00:10 UTC,614,"So what we've done here is kind of set up the process and now what we need to do is update The documents that we have in elastix search to have dpr embeddings. So to do that we go doc store update embeddings And then in here we pass our retriever Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t641.24,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 732, 'published': '2021-04-15 15:00:10 UTC', 'start': 641, 'text': ""Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,732,2021-04-15 15:00:10 UTC,641,"Okay, now this may take well this would be really quick for me We don't have that many documents and even on cpu actually with the lack of documents we have it should be pretty quick so What we see here we created these embeddings And then we posted them again to our index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t660.98,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 757, 'published': '2021-04-15 15:00:10 UTC', 'start': 660, 'text': ""index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,757,2021-04-15 15:00:10 UTC,660,"index So that is pretty cool and now what we need to do Is just test that it actually works Now let's go with retriever and this is how we Get context from our elastix search document store. So right retrieve And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t684.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 786, 'published': '2021-04-15 15:00:10 UTC', 'start': 684, 'text': ""And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,786,2021-04-15 15:00:10 UTC,684,"And then we pass in a query here so let me just find something here Like what did you learn from your Great grandfather maybe or from various Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t702.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 816, 'published': '2021-04-15 15:00:10 UTC', 'start': 702, 'text': ""Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,816,2021-04-15 15:00:10 UTC,702,"Yeah, let's go From grandfather. Let's go grandfather So What did you what did your grandfather Teach you I don't know if this is going to work, but let's see Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t727.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 828, 'published': '2021-04-15 15:00:10 UTC', 'start': 727, 'text': ""Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,828,2021-04-15 15:00:10 UTC,727,"Okay, so you see that we return quite a few Contexts here Now we haven't set up the full thing. So we're just going to pass in a query Here now we haven't set up the full thing. So we're just returning What it sees as being relevant context. We are not actually extracting an answer out yet Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t746.7600000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 843, 'published': '2021-04-15 15:00:10 UTC', 'start': 746, 'text': ""Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the Context that we wanted to return So it returns that as the fourth best context Which is fine because it's not We kind of expect that to sort those a little bit better than our Retriever model"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,843,2021-04-15 15:00:10 UTC,746,"Because that will be the job of our reader model So what we have is from my great grandfather we have that one so it's okay Some other ones here Let's type in grandfather Okay, so it's just returning that one which is fine it's not perfect but What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the Context that we wanted to return So it returns that as the fourth best context Which is fine because it's not We kind of expect that to sort those a little bit better than our Retriever model",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t778.92,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 861, 'published': '2021-04-15 15:00:10 UTC', 'start': 778, 'text': ""What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the Context that we wanted to return So it returns that as the fourth best context Which is fine because it's not We kind of expect that to sort those a little bit better than our Retriever model This is pretty cool. And I think Definitely a good start. So now what we have Retrieved for meditations Set up our document store And now we have also set up our retrieval"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,861,2021-04-15 15:00:10 UTC,778,What we would expect to do in reality is return more So let's try another one as well And let's say who taught you about freedom of will Who taught the freedom of will And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the Context that we wanted to return So it returns that as the fourth best context Which is fine because it's not We kind of expect that to sort those a little bit better than our Retriever model This is pretty cool. And I think Definitely a good start. So now what we have Retrieved for meditations Set up our document store And now we have also set up our retrieval,Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
DBsxUSUhfRg-t806.2,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 887, 'published': '2021-04-15 15:00:10 UTC', 'start': 806, 'text': ""And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the Context that we wanted to return So it returns that as the fourth best context Which is fine because it's not We kind of expect that to sort those a little bit better than our Retriever model This is pretty cool. And I think Definitely a good start. So now what we have Retrieved for meditations Set up our document store And now we have also set up our retrieval So we can see that And now we have also set up our retrieval So we can also cross that off And next thing is our reader model So, I think that's it for this video in the next one, of course, we'll move on to that reader model"", 'title': 'Q&A Document Retrieval With DPR', 'url': 'https://youtu.be/DBsxUSUhfRg'}",UCv83tO5cePwHMt1952IVVHw,887,2021-04-15 15:00:10 UTC,806,"And we see here okay in the first one we don't get the correct answer that we want or the correct context And we go down and I saw There he is. So here is the Context that we wanted to return So it returns that as the fourth best context Which is fine because it's not We kind of expect that to sort those a little bit better than our Retriever model This is pretty cool. And I think Definitely a good start. So now what we have Retrieved for meditations Set up our document store And now we have also set up our retrieval So we can see that And now we have also set up our retrieval So we can also cross that off And next thing is our reader model So, I think that's it for this video in the next one, of course, we'll move on to that reader model",Q&A Document Retrieval With DPR,https://youtu.be/DBsxUSUhfRg
ODdKC30dT8c-t13.82,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 120, 'published': '2022-09-23 14:45:22 UTC', 'start': 13, 'text': ""include data pre-processing within the data loading pipeline We can stream from a another sort of remote data source, which is pretty useful if you are using a data set where the owners of that data set want the data to be streamed from their server Which happens quite a lot or if you know, maybe you have your data set split into multiple Files or you have images in your data set or something along those lines In those cases, you always need to use one of these data set building scripts So what I'm first going to do very quickly is show you how I created a compressed file for this demo So we're going to create Let me show you so we're gonna go over here. So into this James Callum HF data sets repo on github 01 builder script we're going to here and you you will see this file here. So this data set tar gz file So this is a zip or compressed file and we're actually going to stream our data from This exact location. So if you We go on here. I see we have this download link download button. We're just going to copy that link address and we're gonna use that to Stream our data into into the data data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,120,2022-09-23 14:45:22 UTC,13,"include data pre-processing within the data loading pipeline We can stream from a another sort of remote data source, which is pretty useful if you are using a data set where the owners of that data set want the data to be streamed from their server Which happens quite a lot or if you know, maybe you have your data set split into multiple Files or you have images in your data set or something along those lines In those cases, you always need to use one of these data set building scripts So what I'm first going to do very quickly is show you how I created a compressed file for this demo So we're going to create Let me show you so we're gonna go over here. So into this James Callum HF data sets repo on github 01 builder script we're going to here and you you will see this file here. So this data set tar gz file So this is a zip or compressed file and we're actually going to stream our data from This exact location. So if you We go on here. I see we have this download link download button. We're just going to copy that link address and we're gonna use that to Stream our data into into the data data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t39.64,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 135, 'published': '2022-09-23 14:45:22 UTC', 'start': 39, 'text': ""Files or you have images in your data set or something along those lines In those cases, you always need to use one of these data set building scripts So what I'm first going to do very quickly is show you how I created a compressed file for this demo So we're going to create Let me show you so we're gonna go over here. So into this James Callum HF data sets repo on github 01 builder script we're going to here and you you will see this file here. So this data set tar gz file So this is a zip or compressed file and we're actually going to stream our data from This exact location. So if you We go on here. I see we have this download link download button. We're just going to copy that link address and we're gonna use that to Stream our data into into the data data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,135,2022-09-23 14:45:22 UTC,39,"Files or you have images in your data set or something along those lines In those cases, you always need to use one of these data set building scripts So what I'm first going to do very quickly is show you how I created a compressed file for this demo So we're going to create Let me show you so we're gonna go over here. So into this James Callum HF data sets repo on github 01 builder script we're going to here and you you will see this file here. So this data set tar gz file So this is a zip or compressed file and we're actually going to stream our data from This exact location. So if you We go on here. I see we have this download link download button. We're just going to copy that link address and we're gonna use that to Stream our data into into the data data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t66.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 158, 'published': '2022-09-23 14:45:22 UTC', 'start': 66, 'text': ""01 builder script we're going to here and you you will see this file here. So this data set tar gz file So this is a zip or compressed file and we're actually going to stream our data from This exact location. So if you We go on here. I see we have this download link download button. We're just going to copy that link address and we're gonna use that to Stream our data into into the data data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,158,2022-09-23 14:45:22 UTC,66,01 builder script we're going to here and you you will see this file here. So this data set tar gz file So this is a zip or compressed file and we're actually going to stream our data from This exact location. So if you We go on here. I see we have this download link download button. We're just going to copy that link address and we're gonna use that to Stream our data into into the data data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add,Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t96.75999999999999,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 179, 'published': '2022-09-23 14:45:22 UTC', 'start': 96, 'text': ""data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,179,2022-09-23 14:45:22 UTC,96,data set building script so very quickly How did I build that you can actually have a look at this file here? So all I'm doing is I'm taking this the reddit topics data set that I've built already very similar to the dates that we used in the last video It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to,Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t114.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 202, 'published': '2022-09-23 14:45:22 UTC', 'start': 114, 'text': ""It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,202,2022-09-23 14:45:22 UTC,114,"It's just a little bit bigger. So it's not massive 3.7 3.8 thousand rows All it converts pandas convert to a dictionary or using the the records orientation and then Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t130.28,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 223, 'published': '2022-09-23 14:45:22 UTC', 'start': 130, 'text': ""Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,223,2022-09-23 14:45:22 UTC,130,"Save that as a JSON L or JSON lines file then we compressed it using this so you'll probably if you have your own data set and you want to compress it and Kind of follow the same steps of doing here. This is what you will need. So you need to add your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t146.92000000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 241, 'published': '2022-09-23 14:45:22 UTC', 'start': 146, 'text': ""your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,241,2022-09-23 14:45:22 UTC,146,"your data set file to the Zipped compressed file here and you choose to half file that this I believe is actually installed by default With Python so you won't have to pivot install that so with all of that We can go ahead and actually look at how we build our data building script. So we start with a template first so come over to hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t172.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 264, 'published': '2022-09-23 14:45:22 UTC', 'start': 172, 'text': ""hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,264,2022-09-23 14:45:22 UTC,172,"hugging face here and go data sets and And let's go squad, okay, so go here so squad is just a very popular data set and I think among the tutorials they use it as a As like a template for building your own scripts um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t194.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 280, 'published': '2022-09-23 14:45:22 UTC', 'start': 194, 'text': ""um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,280,2022-09-23 14:45:22 UTC,194,"um And that's probably where I got this from but I just by default I go to this data set and use this my template if I'm building a new data set loading script, so Come over here. So within the build script. I have a few things here Jason lines just you can see what was in there. I can actually delete that. I don't need that anymore So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t217.8,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 303, 'published': '2022-09-23 14:45:22 UTC', 'start': 217, 'text': ""So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,303,2022-09-23 14:45:22 UTC,217,"So just remove that What I want to do is create a Python file and I'm going to name it the same as my data set So I'm I'm gonna call this the reddit topics tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t235.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 326, 'published': '2022-09-23 14:45:22 UTC', 'start': 235, 'text': ""tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,326,2022-09-23 14:45:22 UTC,235,"tar GZ And that's what I'm gonna call this this data set Okay And This all okay, we're gonna modify a lot of this but for now, I'm not going to touch too much we just want to Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t256.04,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 350, 'published': '2022-09-23 14:45:22 UTC', 'start': 256, 'text': ""Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,350,2022-09-23 14:45:22 UTC,256,"Um, once first on the essential things that we need here. So first thing we don't need this It's like added complexity that isn't necessary class We'll call it so reddit Tar GZ, I suppose it's fine already topics tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t274.84000000000003,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 370, 'published': '2022-09-23 14:45:22 UTC', 'start': 274, 'text': ""tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,370,2022-09-23 14:45:22 UTC,274,"tar GZ Builder configs this doesn't matter This does matter we will mess around that later not now So what does let's focus on what actually matters right now? So the here we have this download manager and we're gonna we're gonna look at download manager a bit more in the next video, but for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t297.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 396, 'published': '2022-09-23 14:45:22 UTC', 'start': 297, 'text': ""for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,396,2022-09-23 14:45:22 UTC,297,"for now download manager is essentially a hugging face state sets utility that allows us to given a particular file either local or on the internet we can download it and Extract the contents of it. So this is why I formatted the Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t317.68,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 416, 'published': '2022-09-23 14:45:22 UTC', 'start': 317, 'text': ""Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,416,2022-09-23 14:45:22 UTC,317,"Date sets file as a tar GZ file Because I want to use this download and extract function or method So what we need to do is in your so I'm gonna change this to URL. I'm gonna come up here where we define URL Actually move that and here I'm going to replace that with the Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t341.47999999999996,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 439, 'published': '2022-09-23 14:45:22 UTC', 'start': 341, 'text': ""Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,439,2022-09-23 14:45:22 UTC,341,"Location that I copied earlier. So See if that actually that won't work. So I need to copy it again. So if I go to here The repo again. Good zero and build a script the Location of the compressed file go there and then where it says download I'm going to copy that link and I'm gonna put in here. Okay, so with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t365.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 466, 'published': '2022-09-23 14:45:22 UTC', 'start': 365, 'text': ""with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,466,2022-09-23 14:45:22 UTC,365,"with that Description we can know demo We'll change the other things later with that We or this here will kind of almost work so this is just one thing so We're downloading this URL, but With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t386.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 483, 'published': '2022-09-23 14:45:22 UTC', 'start': 386, 'text': ""With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,483,2022-09-23 14:45:22 UTC,386,"With squad there were two URLs. Okay, so If I actually go back a little bit you see that there is these two years one for the training set one for the development set and We only have one so we actually Need to modify this a little bit to deal with just one day. So not two So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t405.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 501, 'published': '2022-09-23 14:45:22 UTC', 'start': 405, 'text': ""So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,501,2022-09-23 14:45:22 UTC,405,"So here we need to return split generator. We actually just remove this one the validation split because we just have a training split and the download files is actually Not going to be this it's going to be so this is basically going to show us a path to a particular location Let me show you exactly what it's doing. Okay, so we're going to do from Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t430.96,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 524, 'published': '2022-09-23 14:45:22 UTC', 'start': 430, 'text': ""Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,524,2022-09-23 14:45:22 UTC,430,"Transfer no data set. Sorry Import Import from data sets utils import download manager Might not be there. Maybe it's here. Let's see Okay, it was that so dl manager and i'm just going to Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t457.84,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 547, 'published': '2022-09-23 14:45:22 UTC', 'start': 457, 'text': ""Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,547,2022-09-23 14:45:22 UTC,457,"Initialize it. This is kind of happening in the background of our of our builder script So we don't actually do this in the build script. It just kind of happens so We do that and then let's just Copy what we have elsewhere. So we have the url And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t476.71999999999997,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 568, 'published': '2022-09-23 14:45:22 UTC', 'start': 476, 'text': ""And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,568,2022-09-23 14:45:22 UTC,476,"And it is this okay, that is the euro And let's just see what this output so download manager and download and extract Your up. Okay. Let's see what we get. Um We'll call this out We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t492.48,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 595, 'published': '2022-09-23 14:45:22 UTC', 'start': 492, 'text': ""We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,595,2022-09-23 14:45:22 UTC,492,"We'll call this out Okay, so we see We get a file path from that now. Okay Interesting. So what let's have a look at what is in that file path. So os list out Okay, so now we can see we actually have that json lines file that we that we put inside our our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t520.4,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 617, 'published': '2022-09-23 14:45:22 UTC', 'start': 520, 'text': ""our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,617,2022-09-23 14:45:22 UTC,520,"our compressed tar file so Okay, what what does that mean for us it means we can actually just load that From here based on what the download manager is giving us. Okay. So this is like a cached location for our particular data set So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t539.78,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 635, 'published': '2022-09-23 14:45:22 UTC', 'start': 539, 'text': ""So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set?"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,635,2022-09-23 14:45:22 UTC,539,"So return to the builder script, uh, we have download files I I don't really like the name so i'm just going to call it path um I'm gonna say path here as well remove train And if we just have a look at the path Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set?",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t560.0200000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 654, 'published': '2022-09-23 14:45:22 UTC', 'start': 560, 'text': ""Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set? method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,654,2022-09-23 14:45:22 UTC,560,"Um, it is it is just the directory that contains our data set of json Okay, or json lines file. So actually what we need to do Is we need to do like out plus? Data set dot json out Okay, this this will give us a full path to our file. So that is what we're going to do here So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set? method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t585.3,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 676, 'published': '2022-09-23 14:45:22 UTC', 'start': 585, 'text': ""So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set? method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,676,2022-09-23 14:45:22 UTC,585,"So, where are we um, so path let's I mean it's a bit easy to read okay come here Zoom out a little bit Okay, so it will be path And then here we have data set dot json out, okay And yeah, that's how That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set? method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t610.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 697, 'published': '2022-09-23 14:45:22 UTC', 'start': 610, 'text': ""That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set? method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,697,2022-09-23 14:45:22 UTC,610,"That's a split generators function here. And what that will do Is you see that we have this file path here? That is going to get passed along to this generate examples function or method and Is this method that is going to kind of output the data set? method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t628.66,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 713, 'published': '2022-09-23 14:45:22 UTC', 'start': 628, 'text': ""method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,713,2022-09-23 14:45:22 UTC,628,"method that is going to kind of output the rows of the data set to us so What we need to do is actually Just use this to kind of read our data set now It you know just kind of doing that from from stretcher seeing what's happening. It's kind of hard So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t646.02,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 744, 'published': '2022-09-23 14:45:22 UTC', 'start': 646, 'text': ""So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,744,2022-09-23 14:45:22 UTC,646,"So let's let's return to that the notebook file and see how we can do that So we're here we have our Our let's call this file path now because this is what we've created in the other file. It's a file path And what we're going to do file path is well first we need to import json because it's a json minus file so we're gonna have to read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t670.74,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 764, 'published': '2022-09-23 14:45:22 UTC', 'start': 670, 'text': ""read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,764,2022-09-23 14:45:22 UTC,670,"read that and Exactly. We actually want to do this so as fp I don't think we even need to put encoding there, but we'll put it to be safe And What we're going to do is we're going to go through that so for line In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t686.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 779, 'published': '2022-09-23 14:45:22 UTC', 'start': 686, 'text': ""In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,779,2022-09-23 14:45:22 UTC,686,"In fp it's a json lines file. So there's just lines of data Each one those lines represents a json object. So we are going to Oh, we can we can just print it from now in in here so Let's put a count on this. So we'll print out a few items, but not too many so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t707.54,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 795, 'published': '2022-09-23 14:45:22 UTC', 'start': 707, 'text': ""so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,795,2022-09-23 14:45:22 UTC,707,"so if count is 5 break Okay, let's see what we get. Okay, cool. So we can see that we we get a few items here. All right So we're just kind of going through those of red file and we're just looping through and and printing them So we can do the same over in our other So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t732.42,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 820, 'published': '2022-09-23 14:45:22 UTC', 'start': 732, 'text': ""So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,820,2022-09-23 14:45:22 UTC,732,"So we can do the same over in our other file in the builder script so let's come to here Copy this in now All of this we can see here. Okay Some of this we will need not all of it. So let's go ahead and just remove what we don't need Okay, this is all we need Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t757.86,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 845, 'published': '2022-09-23 14:45:22 UTC', 'start': 757, 'text': ""Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key?"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,845,2022-09-23 14:45:22 UTC,757,"Um this yield so because this uh generator example is creating a generator function right so Let's come here remove Remove parts of this. So the line or the we should call it a record Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key?",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t775.14,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 876, 'published': '2022-09-23 14:45:22 UTC', 'start': 775, 'text': ""Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key? Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um,"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,876,2022-09-23 14:45:22 UTC,775,"Is equal to json dot loads line Maybe we'll call it object Okay And within the object we have a few A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key? Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um,",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t788.66,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 892, 'published': '2022-09-23 14:45:22 UTC', 'start': 788, 'text': ""A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key? Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um, Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,892,2022-09-23 14:45:22 UTC,788,"A few different key value pairs, right? So what are those? We can have a look at the the make tar file File and we have we have all these here so we have subtitle self text Up vote ratio id and create utc now We can actually just pass all of these directly onto the next so we can yield all of these So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key? Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um, Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t812.02,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 913, 'published': '2022-09-23 14:45:22 UTC', 'start': 812, 'text': ""So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key? Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um, Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,913,2022-09-23 14:45:22 UTC,812,"So let me show you what I mean by that So we come here and you see that we're just yielding and then we're yielding this dictionary type structure for squad All right, so as we already have that dictionary type structure because we use a json lines file This is one of the reasons I like using them. So we can actually just do yield key object like that now Okay, what is um, what is key? Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um, Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t834.9,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 932, 'published': '2022-09-23 14:45:22 UTC', 'start': 834, 'text': ""Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um, Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,932,2022-09-23 14:45:22 UTC,834,"Key is actually the index value or id value if you if you want, but it's an index value So i'm going to rename it Index because that makes more sense to me than key And yeah, here we go. So we have We have set up here we're going to open the file located or let's let's do that and read the lines. Um, Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t866.1,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 953, 'published': '2022-09-23 14:45:22 UTC', 'start': 866, 'text': ""Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,953,2022-09-23 14:45:22 UTC,866,"Load file object or json object And yeah, we just yield them so what would that do when we are loading The function or when we are loading the data set over in hugging face data sets This is going to be the thing that generates all of those all those items so What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t888.98,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 982, 'published': '2022-09-23 14:45:22 UTC', 'start': 888, 'text': ""What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,982,2022-09-23 14:45:22 UTC,888,"What we should do now is maybe we can Maybe we can test it and see what happens. Um, it won't work straight away. We'll see But let's try so what i'm going to do is i'm just going to copy all of this then i'm going to come over to Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t904.66,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1013, 'published': '2022-09-23 14:45:22 UTC', 'start': 904, 'text': ""Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1013,2022-09-23 14:45:22 UTC,904,"Hugging face i'm going to click on my or icon right over here click new data set I'm going to call it reddit topics tar gc create that And i'm going to come to files i'm going to add a file create new file And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t923.14,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1035, 'published': '2022-09-23 14:45:22 UTC', 'start': 923, 'text': ""And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1035,2022-09-23 14:45:22 UTC,923,"And this is just going to be reddit topics Tar gc. So the exact same file we created before i'm just going to paste all that code in there Okay, so you see we have all this code. Uh, let's Let's just remove this I think it's that important. Uh, it's not squad anymore. So let's just call it reddit topics Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t946.02,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1057, 'published': '2022-09-23 14:45:22 UTC', 'start': 946, 'text': ""Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1057,2022-09-23 14:45:22 UTC,946,"Tar gc demo data set One thing we do need is we need to import json. So that's good. That's already there Um, we don't need this anymore, but let's keep it in there for now before we start removing everything creating more errors So let's commit that And then let's just try and see what happens. Okay, so i'm going to create a new file to test it. So i'm saying test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t972.42,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1076, 'published': '2022-09-23 14:45:22 UTC', 'start': 972, 'text': ""test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1076,2022-09-23 14:45:22 UTC,972,"test test data set And what we're going to do is from data sets import load data set And the data set we'll just call data load data set And we can find the data set name over here So i'm just going to copy click here copy that and there is just one split in this data set. So split equals train Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1000.82,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1096, 'published': '2022-09-23 14:45:22 UTC', 'start': 1000, 'text': ""Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1096,2022-09-23 14:45:22 UTC,1000,"Okay, let's see what happens. Okay, we download the build script so far so good downloads the data And then we get this. Okay. What is this os error come down here? cannot find data file Okay, so we had a look at this so without this Dot here. Uh, we can see that data file is there so we have our first error, um, which was not on purpose But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1026.9,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1119, 'published': '2022-09-23 14:45:22 UTC', 'start': 1026, 'text': ""But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1119,2022-09-23 14:45:22 UTC,1026,"But that's fine So the reason we have that is because here I we put a dot i'm not sure why that so let's save that And actually, let's just edit it in the in the web editor here as well So let's remove that commit changes And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1046.3400000000001,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1133, 'published': '2022-09-23 14:45:22 UTC', 'start': 1046, 'text': ""And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1133,2022-09-23 14:45:22 UTC,1046,"And then let's try again. Okay, let's come up here. Let's clear everything restart and let's go again Okay, so now we get this key error. So what does that mean key error context? Context Okay, I don't remember putting context anywhere. So let's have a look at the builder script And if we okay, let's have a look. Okay here we have this so we haven't modified this yet Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1070.58,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1167, 'published': '2022-09-23 14:45:22 UTC', 'start': 1070, 'text': ""Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1167,2022-09-23 14:45:22 UTC,1070,"Now what is this telling us? Um, it's basically telling the data set builder Which features to expect in the data set? So basically down here we're kind of feeding in these these different features. We're feeding in these these records Each record is a key value pair. So the keys are the feature names and the values are obviously values Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1092.02,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1182, 'published': '2022-09-23 14:45:22 UTC', 'start': 1092, 'text': ""Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1182,2022-09-23 14:45:22 UTC,1092,"Which have a particular data type now? here We have the the feature names. So the keys But they are not aligned to our actual data set. These are using the squad data set key value pairs So we need to come over to this file and we can Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1110.66,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1204, 'published': '2022-09-23 14:45:22 UTC', 'start': 1110, 'text': ""Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1204,2022-09-23 14:45:22 UTC,1110,"Or get those features specific to our data set from there. So let's take these i'm going to copy them across here and all i'm going to do is actually just Write those here. Okay, so we have a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1126.8200000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1229, 'published': '2022-09-23 14:45:22 UTC', 'start': 1126, 'text': ""a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1229,2022-09-23 14:45:22 UTC,1126,"a subtitle self text Upvote ratio Uh, we have oh we have id and we also have another one so let's create another Well, actually, let's make this one more normal first the id Is this and then we have one more which is created utc Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1151.88,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1253, 'published': '2022-09-23 14:45:22 UTC', 'start': 1151, 'text': ""Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1253,2022-09-23 14:45:22 UTC,1151,"Okay, now we can try this it's not going to work again, but let's try Yeah, let's rerun this See what happens Okay. So actually it does work, but it's not working in the way that we might expect so If we have a look at data and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1178.7600000000002,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1276, 'published': '2022-09-23 14:45:22 UTC', 'start': 1178, 'text': ""and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1276,2022-09-23 14:45:22 UTC,1178,"and zero Okay, we have Subtitle self text and then we come down here. There's a lot in this self text Um, but so just look at this. So the upvote ratio Which is a floating point number is now a string The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1196.4399999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1296, 'published': '2022-09-23 14:45:22 UTC', 'start': 1196, 'text': ""The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1296,2022-09-23 14:45:22 UTC,1196,"The id that's fine We should expect that and the created utc which is also a floating point number is now a string as well. So So there's a bit of an issue here. Basically if we if we go back to our script When we are feeding the features through this feature specification It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1218.76,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1317, 'published': '2022-09-23 14:45:22 UTC', 'start': 1218, 'text': ""It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1317,2022-09-23 14:45:22 UTC,1218,"It's seeing that we're saying everything should be a string and it's converting everything into a string We don't actually want everything to be a string. So what we need to do here is use a specific apache arrow Data type identifiers for different things. So for example flow that we have here. Okay, so let's go ahead and have a look at how What that might be so to find that i'm just going to type like apache arrow data types Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1247.32,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1336, 'published': '2022-09-23 14:45:22 UTC', 'start': 1247, 'text': ""Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1336,2022-09-23 14:45:22 UTC,1247,"Here so apache arrow data types and schemas schemas maybe Uh, we come here and we can see we can see a load of these so we have we have integer values unsigned integers And then we have floats. So I'm going to say okay single precision floating point type is perfect Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1267.4799999999998,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1354, 'published': '2022-09-23 14:45:22 UTC', 'start': 1267, 'text': ""Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes Now let's have a look at what happens if we load the data set so Come back over here test data set Uh, let's run this Let's see what happens Okay, it loaded well it loaded correctly. That's a good sign come down here"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1354,2022-09-23 14:45:22 UTC,1267,"Okay, so i'm just going to copy that float 32 and i'm going to put that for created utc and also the operator ratio Okay, i'm going to save that I'm going to change a few things that we don't we don't actually need so i'm going to remove this task template because we can't Do question answering with this data set. Um Um, no, we can't at least not extractive question answering or can't train with that for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes Now let's have a look at what happens if we load the data set so Come back over here test data set Uh, let's run this Let's see what happens Okay, it loaded well it loaded correctly. That's a good sign come down here",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1291.08,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1376, 'published': '2022-09-23 14:45:22 UTC', 'start': 1291, 'text': ""for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes Now let's have a look at what happens if we load the data set so Come back over here test data set Uh, let's run this Let's see what happens Okay, it loaded well it loaded correctly. That's a good sign come down here And now we can see that these are no longer strings, but they're actually flowing point numbers. Okay, so That is That's everything there are maybe a few aesthetic things to change here. So the Like the citation We'll change that up here. I can change this as well, but we're not going to go through that in this"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1376,2022-09-23 14:45:22 UTC,1291,"for the home page Let's let's put this I suppose Okay, supervised keys is none And what else do we have here? So subscription Uh, so it's a demo. We know that okay Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes Now let's have a look at what happens if we load the data set so Come back over here test data set Uh, let's run this Let's see what happens Okay, it loaded well it loaded correctly. That's a good sign come down here And now we can see that these are no longer strings, but they're actually flowing point numbers. Okay, so That is That's everything there are maybe a few aesthetic things to change here. So the Like the citation We'll change that up here. I can change this as well, but we're not going to go through that in this",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
ODdKC30dT8c-t1310.6,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1396, 'published': '2022-09-23 14:45:22 UTC', 'start': 1310, 'text': ""Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes Now let's have a look at what happens if we load the data set so Come back over here test data set Uh, let's run this Let's see what happens Okay, it loaded well it loaded correctly. That's a good sign come down here And now we can see that these are no longer strings, but they're actually flowing point numbers. Okay, so That is That's everything there are maybe a few aesthetic things to change here. So the Like the citation We'll change that up here. I can change this as well, but we're not going to go through that in this Uh in this video, but I think you want to watch me change citations. So yeah, that's everything For this video in the next video What we're going to do is take a look at taking this a a little bit further And adding more advanced data types like images into our data sets so"", 'title': 'Hugging Face Datasets #2 - Dataset Builder Scripts', 'url': 'https://youtu.be/ODdKC30dT8c'}",UCv83tO5cePwHMt1952IVVHw,1396,2022-09-23 14:45:22 UTC,1310,"Okay, let's save this and And try again. Okay, so i'm going to copy this over into home face come here Uh not here here edit And come here select all paste and I am going to commit those changes Now let's have a look at what happens if we load the data set so Come back over here test data set Uh, let's run this Let's see what happens Okay, it loaded well it loaded correctly. That's a good sign come down here And now we can see that these are no longer strings, but they're actually flowing point numbers. Okay, so That is That's everything there are maybe a few aesthetic things to change here. So the Like the citation We'll change that up here. I can change this as well, but we're not going to go through that in this Uh in this video, but I think you want to watch me change citations. So yeah, that's everything For this video in the next video What we're going to do is take a look at taking this a a little bit further And adding more advanced data types like images into our data sets so",Hugging Face Datasets #2 - Dataset Builder Scripts,https://youtu.be/ODdKC30dT8c
989aKUVBfbk-t23.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 166, 'published': '2022-08-11 13:03:08 UTC', 'start': 23, 'text': ""In this implementation we're going to be using a vision transformer that will embed images. And we're going to use a normal text transformer that will embed text. During pre-training OpenAI trained the model on pairs of images and text and it trained them to both output embedding vectors that are as close as possible to each other. So the text transformer was trained to output a single embedding 512 dimensional embedding that was as close as possible to the vision transformer's image embedding for the image text pair. So what that means is that clip is able to take both images and text and embed them both into a similar vector space. And with that we can do a lot of things. You can do image and text classification. You can do image and text search and a huge number of things. Anything to do with images and text there's a good chance we can do it with clip. So let's have a look at how we actually use clip. OpenAI released a GitHub repository OpenAI clip here. This contains clip but we're not going to use this implementation. We're actually going to use this implementation of clip. So this is on Hugging Face. So we're going to be using Hugging Face transformers and this is still from OpenAI. It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,166,2022-08-11 13:03:08 UTC,23,In this implementation we're going to be using a vision transformer that will embed images. And we're going to use a normal text transformer that will embed text. During pre-training OpenAI trained the model on pairs of images and text and it trained them to both output embedding vectors that are as close as possible to each other. So the text transformer was trained to output a single embedding 512 dimensional embedding that was as close as possible to the vision transformer's image embedding for the image text pair. So what that means is that clip is able to take both images and text and embed them both into a similar vector space. And with that we can do a lot of things. You can do image and text classification. You can do image and text search and a huge number of things. Anything to do with images and text there's a good chance we can do it with clip. So let's have a look at how we actually use clip. OpenAI released a GitHub repository OpenAI clip here. This contains clip but we're not going to use this implementation. We're actually going to use this implementation of clip. So this is on Hugging Face. So we're going to be using Hugging Face transformers and this is still from OpenAI. It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t79.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 196, 'published': '2022-08-11 13:03:08 UTC', 'start': 79, 'text': ""And with that we can do a lot of things. You can do image and text classification. You can do image and text search and a huge number of things. Anything to do with images and text there's a good chance we can do it with clip. So let's have a look at how we actually use clip. OpenAI released a GitHub repository OpenAI clip here. This contains clip but we're not going to use this implementation. We're actually going to use this implementation of clip. So this is on Hugging Face. So we're going to be using Hugging Face transformers and this is still from OpenAI. It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here. And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,196,2022-08-11 13:03:08 UTC,79,"And with that we can do a lot of things. You can do image and text classification. You can do image and text search and a huge number of things. Anything to do with images and text there's a good chance we can do it with clip. So let's have a look at how we actually use clip. OpenAI released a GitHub repository OpenAI clip here. This contains clip but we're not going to use this implementation. We're actually going to use this implementation of clip. So this is on Hugging Face. So we're going to be using Hugging Face transformers and this is still from OpenAI. It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here. And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here.",Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t98.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 220, 'published': '2022-08-11 13:03:08 UTC', 'start': 98, 'text': ""OpenAI released a GitHub repository OpenAI clip here. This contains clip but we're not going to use this implementation. We're actually going to use this implementation of clip. So this is on Hugging Face. So we're going to be using Hugging Face transformers and this is still from OpenAI. It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here. And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here. So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,220,2022-08-11 13:03:08 UTC,98,"OpenAI released a GitHub repository OpenAI clip here. This contains clip but we're not going to use this implementation. We're actually going to use this implementation of clip. So this is on Hugging Face. So we're going to be using Hugging Face transformers and this is still from OpenAI. It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here. And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here. So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog.",Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t118.0,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 246, 'published': '2022-08-11 13:03:08 UTC', 'start': 118, 'text': ""It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here. And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here. So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog. OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,246,2022-08-11 13:03:08 UTC,118,"It's still clip. It's just an easy to use implementation of it through the Hugging Face transformers library which is a more standard library for actually doing anything with NLP and also now computer vision and some other things as well. So to get started I'd recommend you install these libraries. To install Torch you should probably go through the PyTorch.org instructions rather than following this here. So go to PyTorch.org and just install PyTorch using the specific install command they use for your platform or your iOS from here. And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here. So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog. OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer.",Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t157.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 285, 'published': '2022-08-11 13:03:08 UTC', 'start': 157, 'text': ""And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here. So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog. OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer. So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,285,2022-08-11 13:03:08 UTC,157,"And then pip install transformers and datasets. You can still just use this command I'd recommend installing PyTorch from here instead. Now after that we're going to need our dataset. So this is just a very simple dataset. It contains I think just under 10,000 images and we only care about the images here. So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog. OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer. So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this.",Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t183.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 308, 'published': '2022-08-11 13:03:08 UTC', 'start': 183, 'text': ""So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog. OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer. So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this. OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,308,2022-08-11 13:03:08 UTC,183,So if we have a look we have ImageNet we'll go the first item and we'll just have a look at image. And we have this Sony radio and we have other things as well. So if we go ImageNet. It's 6494. There's another image here of a dog. OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer. So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this. OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t206.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 337, 'published': '2022-08-11 13:03:08 UTC', 'start': 206, 'text': ""OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer. So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this. OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already. OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,337,2022-08-11 13:03:08 UTC,206,OK just to point out that we have a lot of images in here in the dataset that cover a range of things. There's not a huge number of different categories here but they have dogs they have radios and a few other things. Now I'm just going to go ahead and initialize everything. So there's a few things here. From transformers we're importing the clip tokenizer. So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this. OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already. OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t231.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 363, 'published': '2022-08-11 13:03:08 UTC', 'start': 231, 'text': ""So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this. OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already. OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained. OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,363,2022-08-11 13:03:08 UTC,231,So the tokenizer is what's going to handle the pre-processing of our text into token ID tensors and other tensors. We have the clip processor that's like the tokenizer but for images. So this is actually just going to resize our images into the size that clip expects and also modify the pixel values as well. And then we have clip model. Clip model is clip itself. OK so if you have CUDA or MPS if you're on M1 Mac you just set that with this. OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already. OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained. OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t274.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 397, 'published': '2022-08-11 13:03:08 UTC', 'start': 274, 'text': ""OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already. OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained. OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some. And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,397,2022-08-11 13:03:08 UTC,274,OK and then we're ready to actually initialize all of this. So the model ID is going to be what we saw before. So you come over here we have the tokenizer clip VIT base patch 32 copy that. And here we go. OK. And now we just need to look I'm being told what to do already. OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained. OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some. And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t301.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 437, 'published': '2022-08-11 13:03:08 UTC', 'start': 301, 'text': ""OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained. OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some. And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs. OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,437,2022-08-11 13:03:08 UTC,301,OK so model clip model from pre-trained model ID. I'm going to I don't normally set device like that. I don't know if you can. I am going to do it like this. OK and tokenizer. OK good job. And processor. Cool. Almost there. It's from pre-trained. OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some. And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs. OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t325.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 462, 'published': '2022-08-11 13:03:08 UTC', 'start': 325, 'text': ""OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some. And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs. OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt. And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,462,2022-08-11 13:03:08 UTC,325,OK and you got a little bit confused. So model ID. OK that looks good. Let's run that. OK cool. So now what we're going to do is take a look how we actually create the text embeddings through clip. So we start with a prompt. I'm going to go with a dog in the snow. There's not many pictures of dogs in the snow in the dataset but there are some. And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs. OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt. And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t356.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 487, 'published': '2022-08-11 13:03:08 UTC', 'start': 356, 'text': ""And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs. OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt. And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things. OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,487,2022-08-11 13:03:08 UTC,356,And what we need to do is is tokenize the prompt. Yeah that's true. OK. I'm not going to do it like that. We're going to go with tokenize prompt and the we need to return tensors using Pytorch. So we're using we're going to be using Pytorch behind the scenes here. So make sure we do that. And let's just have a look at what is actually in inputs. OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt. And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things. OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t383.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 511, 'published': '2022-08-11 13:03:08 UTC', 'start': 383, 'text': ""OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt. And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things. OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch. OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,511,2022-08-11 13:03:08 UTC,383,OK so we get the this input ID is tensor so you'll you'll recognize this if you if you use a face transformers before. These are just the ID token IDs that represent the words from this. OK. And these this is the attention mask. Now for us it is going to all be ones but if we had padding in here anything beyond the length of our prompt would become a zero. Telling the model to not pay attention to to that part of the prompt. And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things. OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch. OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t415.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 544, 'published': '2022-08-11 13:03:08 UTC', 'start': 415, 'text': ""And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things. OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch. OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well. I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,544,2022-08-11 13:03:08 UTC,415,And from there we can process this through clips so we do model get text features I think. And we pass in those inputs. OK. And let's have a look at the shape of that. OK so we have a five hundred and twelve dimensional vector. OK. So that's the text embedding side of things. Now we need to go ahead and do the image embedding side of things. OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch. OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well. I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t451.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 574, 'published': '2022-08-11 13:03:08 UTC', 'start': 451, 'text': ""OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch. OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well. I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib. Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,574,2022-08-11 13:03:08 UTC,451,OK. So we're going to resize the image first with the processor. We're not adding any text in here so you can also process text through this processor. I'm just keeping it separate because it makes more sense to me. The image should be images actually. Again we want to return tensors using PyTorch. OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well. I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib. Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t476.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 607, 'published': '2022-08-11 13:03:08 UTC', 'start': 476, 'text': ""OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well. I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib. Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back. This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,607,2022-08-11 13:03:08 UTC,476,OK. And then we can have a look at the I'm going to I'm going to show you the image. First we have a look at the shape and as well one thing. So OK I can show you. OK. OK. In here we actually have this pixel values so we actually need to extract that. So we're going to put it here. I'm going to move those to the device as well. I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib. Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back. This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t500.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 641, 'published': '2022-08-11 13:03:08 UTC', 'start': 500, 'text': ""I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib. Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back. This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized. You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,641,2022-08-11 13:03:08 UTC,500,I think the device I have set up right now is actually CPU so it doesn't make a difference for me but it's fine. So let's have a look at the shape. OK. So you see that we have this 224 by 224 image with three color channels. So this is just the expected shape that will be consumed by the vision transformer of CLIP. OK. And to import my PLOTlib. Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back. This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized. You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t530.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 669, 'published': '2022-08-11 13:03:08 UTC', 'start': 530, 'text': ""Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back. This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized. You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector. OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,669,2022-08-11 13:03:08 UTC,530,Pyplot.plt. And I want to show you this image. So this resize image. So PLT. I'm show image. And I need to so I need to resize it. Let me show you what I'm actually doing here. So image.squeeze. Zero. So I'm going to remove that first dimension. Now I'm going to transpose it. So we put the three color channels at the back. This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized. You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector. OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t562.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 703, 'published': '2022-08-11 13:03:08 UTC', 'start': 562, 'text': ""This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized. You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector. OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy. So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,703,2022-08-11 13:03:08 UTC,562,This is for this is for my PLOTlib to be able to actually show us this. So I'm going to take that. I'm going to put it here. OK. And you can see so the minimum maximum color values are all of the color values. Pixel values are modified when we do this process it through the processor. So the colors are kind of messed up. But you can see that this is like a resized. You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector. OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy. So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t594.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 733, 'published': '2022-08-11 13:03:08 UTC', 'start': 594, 'text': ""You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector. OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy. So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random. So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,733,2022-08-11 13:03:08 UTC,594,You know what we saw before. OK. So it's a Sony. Just kind of backwards now and flipped. We can sort it see that it is that Sony radio. So with that we can go ahead and get the image features. I think it just showed me. Model. Get image features. So an image. OK. And then let's have a look at the shape. Cool. OK. So similar to before we have that 512 dimensional embedding vector. OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy. So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random. So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t626.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 770, 'published': '2022-08-11 13:03:08 UTC', 'start': 626, 'text': ""OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy. So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random. So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there. OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,770,2022-08-11 13:03:08 UTC,626,OK. So that's cool. And from here we can we can do a lot of things. What I'm going to show you how to do is how to kind of search through this or at least compare a small number of images against our prompt so that we can actually see which one of those images is the most similar to a dog in the snow. OK. So to do that we're going to want to embed more of these images. I'm not going to embed loads of them just going to embed 100 images. Nothing nothing crazy. So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random. So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there. OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t658.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 813, 'published': '2022-08-11 13:03:08 UTC', 'start': 658, 'text': ""So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random. So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there. OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set. And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,813,2022-08-11 13:03:08 UTC,658,So we're going to import NumPy as NP. NP random seed. So this is just so you can replicate what I am doing. And so this will this will randomly generate a set set set of random numbers. OK. So the reason I'm doing this is because we want to take a sample out of the data set. We don't want to have the whole data set. I want it to be at least somewhat random. So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there. OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set. And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t687.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 856, 'published': '2022-08-11 13:03:08 UTC', 'start': 687, 'text': ""So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there. OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set. And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop. OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,856,2022-08-11 13:03:08 UTC,687,So to do that we want to go. So sample. Indices are going to be equal to NumPy random dot random from zero up to the length of image net. It's actually plus one. And we need 100 of those. And then we're going to convert that into a list. OK. I can just have a quick look at what is in there. OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set. And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop. OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t717.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 901, 'published': '2022-08-11 13:03:08 UTC', 'start': 717, 'text': ""OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set. And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop. OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here. So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,901,2022-08-11 13:03:08 UTC,717,OK. So just all of these all these numbers here. OK. So yeah. Cool. And if we run it again because we have that random seed set the random set of numbers doesn't change. And what I'm going to do is just create a list of images using that using those values. So I for I in sample IDX. OK. Check. OK. So now 100 images from our data set. And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop. OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here. So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t753.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 963, 'published': '2022-08-11 13:03:08 UTC', 'start': 753, 'text': ""And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop. OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here. So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code. So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,963,2022-08-11 13:03:08 UTC,753,And now we want to just go ahead and literally take everything we've just done and put into a for loop to create the embeddings for all of these images. OK. So that will look something like this. I'm using TQDM here. This is just a progress bar so we can see where we are. Batch size saying how many images to perform this for in any one go. You can increase this if you're on a if you're using a bigger GPU or or whatever else. Image array I'm setting that to none for now. We initialize that in the first loop. OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here. So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code. So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t795.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 995, 'published': '2022-08-11 13:03:08 UTC', 'start': 795, 'text': ""OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here. So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code. So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle. So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,995,2022-08-11 13:03:08 UTC,795,OK. And then we're just in the same thing as before. So. So from this. So I'm selecting a batch of images based on the on the batch size. And then where we are processing and resizing the images from that batch we're getting the image features look exactly the same thing. I think before I actually didn't include pixel values but it's the same thing. It's just a default argument. Converting into a NumPy array. Did I show you this before? I don't actually think so. No maybe not. But here the squeeze is very similar. It's the same thing as what I showed you up here. So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code. So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle. So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t834.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1041, 'published': '2022-08-11 13:03:08 UTC', 'start': 834, 'text': ""So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code. So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle. So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy. Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1041,2022-08-11 13:03:08 UTC,834,So we squeeze the first dimension out of that like we did here. And then we are moving that batch of embeddings to the CPU. If it's not already on the CPU we're detaching it from the gradient like the training graph of PyTorch. The PyTorch model e.g. clip. And then we're converting into a NumPy array. OK. And then I'm going to add that batch of embeddings to a larger array of all image embeddings. OK. And that's why the image array comes in. OK. So let's run that. OK. So we come up here. I made a mistake in the code. So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle. So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy. Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t881.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1076, 'published': '2022-08-11 13:03:08 UTC', 'start': 881, 'text': ""So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle. So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy. Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that. So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1076,2022-08-11 13:03:08 UTC,881,So here I'm actually pulling in the full row or record at any one time. We don't do that. We want the image itself. OK. So run that again. OK. And now if we check the type of images. Zero. We should see it's a pill image. Yeah. Cool. Yeah. Pill here. Now we can run this. OK. It won't take long. And now we have one hundred five hundred twelve dimensional image embeddings from our data set and we can now use them to compare to our initial text embedding and see which one of these matches most closely to that text embedding. OK. So I'm going to be using dot product similarity. So there's just one thing to be aware of with that. And that is that it considers both the magnitude of the vector and also the angle. So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy. Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that. So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t940.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1114, 'published': '2022-08-11 13:03:08 UTC', 'start': 940, 'text': ""So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy. Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that. So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum. So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1114,2022-08-11 13:03:08 UTC,940,So in this case that will that can throw off our results. So we should normalize all of the image embeddings so that we are not looking at the magnitude of vectors. And we're only focusing on the angular similarity between our text embedding and these image embeddings. So to do that we need to. I'll just show you quickly. So look at the minimum maximum. You know that kind of all over the place. So to normalize we need to do this. So do image array divided by do numpy. Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that. So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum. So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t984.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1153, 'published': '2022-08-11 13:03:08 UTC', 'start': 984, 'text': ""Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that. So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum. So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot. And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1153,2022-08-11 13:03:08 UTC,984,Linouge dot norm. And here we have the image array. OK. Axis equals one. And let me show you what that is. So we have all these numbers and these are basically telling us for each one of these vectors of what should we divide it by in order to bring each of them to within a within a set set. Within a set magnitude pretty much. So. Take a look at the shape will be 100. So yeah we do that. So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum. So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot. And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t1029.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1196, 'published': '2022-08-11 13:03:08 UTC', 'start': 1029, 'text': ""So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum. So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot. And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top. So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1196,2022-08-11 13:03:08 UTC,1029,So I think I need to. Transpose this. OK. And then so the image array the shape is going to be transposed now so I'm going to transpose it again. Yeah. Image array equals image array transpose. OK. Cool. And now if we have a look at the minimum and maximum. So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot. And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top. So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t1062.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1239, 'published': '2022-08-11 13:03:08 UTC', 'start': 1062, 'text': ""So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot. And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top. So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0. Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1239,2022-08-11 13:03:08 UTC,1062,So minimum and maximum we get these values which are more reasonable. OK. So now what we can do is use dot product similarity to actually compare compare these. So text embedding I'm going to take the text embedding and similar to before what we did is we need to move it to the CPU. Detach it from the PyTorch graph and then convert to a numpy array. OK. Yeah. And then for the scores all we need to do is a numpy dot. And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top. So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0. Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t1100.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1282, 'published': '2022-08-11 13:03:08 UTC', 'start': 1100, 'text': ""And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top. So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0. Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i. And actually that would be 0 i and then I'm going to I am going to show and I'm going to PLT show. OK. Cool. So yeah I mean that's it. So the first first item as we would expect is a dog in the snow. So after that we get dogs and we get like these snowy areas. The reason for that is that we just don't have any more images of dogs in the snow. This one I don't know what this is. It's like a toy that maybe it's a dog. Maybe it's a bear. I'm not sure. But I suppose technically that's like a dog in the snow."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1282,2022-08-11 13:03:08 UTC,1100,And we are going to put the text embedding followed by the image array. And actually I think I need to transpose this again. So. Maybe we could have avoided transposing up here. OK. Yeah. So the scores that we get here we get a single score for every single vector. As we can see shape 100 and they are the dot product similarity scores. So what we can now do is sort based on this scores array and just return like the top. So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0. Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i. And actually that would be 0 i and then I'm going to I am going to show and I'm going to PLT show. OK. Cool. So yeah I mean that's it. So the first first item as we would expect is a dog in the snow. So after that we get dogs and we get like these snowy areas. The reason for that is that we just don't have any more images of dogs in the snow. This one I don't know what this is. It's like a toy that maybe it's a dog. Maybe it's a bear. I'm not sure. But I suppose technically that's like a dog in the snow.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t1134.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1325, 'published': '2022-08-11 13:03:08 UTC', 'start': 1134, 'text': ""So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0. Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i. And actually that would be 0 i and then I'm going to I am going to show and I'm going to PLT show. OK. Cool. So yeah I mean that's it. So the first first item as we would expect is a dog in the snow. So after that we get dogs and we get like these snowy areas. The reason for that is that we just don't have any more images of dogs in the snow. This one I don't know what this is. It's like a toy that maybe it's a dog. Maybe it's a bear. I'm not sure. But I suppose technically that's like a dog in the snow. So we have that. So yeah obviously the model is performing pretty well and I think that's very cool that we can do that so easily. And yeah I mean CLIP is I think an amazing model that we can use to do a load of cool things across both the text and image domain which is super interesting. And it's definitely like if you think just a couple of years ago this sort of thing was impossible and didn't seem like at least not to this sort of degree of accuracy like it was going to be happening anytime soon. So this is this is really cool. Here we've obviously shown I showed you how to do like a text to image search. You can do this like the deep. In reality what we're doing is kind of searching through the vectors."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1325,2022-08-11 13:03:08 UTC,1134,So the top five images and see what the top five most similar images are for our particular query. OK. So we're going to return the top k. So top k is going to be the five most similar or the five items with the highest score. And then we want to take the index values using np.org sort. We're going to add the negative of the scores there and just make sure we take because scores has this here. So we're actually just taking the let me show you scores 0. Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i. And actually that would be 0 i and then I'm going to I am going to show and I'm going to PLT show. OK. Cool. So yeah I mean that's it. So the first first item as we would expect is a dog in the snow. So after that we get dogs and we get like these snowy areas. The reason for that is that we just don't have any more images of dogs in the snow. This one I don't know what this is. It's like a toy that maybe it's a dog. Maybe it's a bear. I'm not sure. But I suppose technically that's like a dog in the snow. So we have that. So yeah obviously the model is performing pretty well and I think that's very cool that we can do that so easily. And yeah I mean CLIP is I think an amazing model that we can use to do a load of cool things across both the text and image domain which is super interesting. And it's definitely like if you think just a couple of years ago this sort of thing was impossible and didn't seem like at least not to this sort of degree of accuracy like it was going to be happening anytime soon. So this is this is really cool. Here we've obviously shown I showed you how to do like a text to image search. You can do this like the deep. In reality what we're doing is kind of searching through the vectors.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
989aKUVBfbk-t1175.5,"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1364, 'published': '2022-08-11 13:03:08 UTC', 'start': 1175, 'text': ""Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i. And actually that would be 0 i and then I'm going to I am going to show and I'm going to PLT show. OK. Cool. So yeah I mean that's it. So the first first item as we would expect is a dog in the snow. So after that we get dogs and we get like these snowy areas. The reason for that is that we just don't have any more images of dogs in the snow. This one I don't know what this is. It's like a toy that maybe it's a dog. Maybe it's a bear. I'm not sure. But I suppose technically that's like a dog in the snow. So we have that. So yeah obviously the model is performing pretty well and I think that's very cool that we can do that so easily. And yeah I mean CLIP is I think an amazing model that we can use to do a load of cool things across both the text and image domain which is super interesting. And it's definitely like if you think just a couple of years ago this sort of thing was impossible and didn't seem like at least not to this sort of degree of accuracy like it was going to be happening anytime soon. So this is this is really cool. Here we've obviously shown I showed you how to do like a text to image search. You can do this like the deep. In reality what we're doing is kind of searching through the vectors. So it doesn't matter you know which direction you're doing that search. The vectors are all the same. So if you want to do a text to text search with with CLIP you could. You want to do image to image search you could. If you want to do image to text or all of those things all at once you could. It is not as you're searching through vectors. So what is behind those vectors doesn't really matter so much. OK. So I think that's it for this video. I think CLIP is super interesting and I hope that you do as well in the future or very soon actually."", 'title': ""Fast intro to multi-modal ML with OpenAI's CLIP"", 'url': 'https://youtu.be/989aKUVBfbk'}",UCv83tO5cePwHMt1952IVVHw,1364,2022-08-11 13:03:08 UTC,1175,Shape. OK. So it's taking the 100 values there and then I want to take the top k from that. OK. So what we're left with is these five index values which are essentially indexes of the image embeddings and therefore both the. And therefore the images that are the most similar to our query. So we use matplotlib again to to visualize those. So we do for i in ID8. Let's print the let's print the score first. So scores i. And actually that would be 0 i and then I'm going to I am going to show and I'm going to PLT show. OK. Cool. So yeah I mean that's it. So the first first item as we would expect is a dog in the snow. So after that we get dogs and we get like these snowy areas. The reason for that is that we just don't have any more images of dogs in the snow. This one I don't know what this is. It's like a toy that maybe it's a dog. Maybe it's a bear. I'm not sure. But I suppose technically that's like a dog in the snow. So we have that. So yeah obviously the model is performing pretty well and I think that's very cool that we can do that so easily. And yeah I mean CLIP is I think an amazing model that we can use to do a load of cool things across both the text and image domain which is super interesting. And it's definitely like if you think just a couple of years ago this sort of thing was impossible and didn't seem like at least not to this sort of degree of accuracy like it was going to be happening anytime soon. So this is this is really cool. Here we've obviously shown I showed you how to do like a text to image search. You can do this like the deep. In reality what we're doing is kind of searching through the vectors. So it doesn't matter you know which direction you're doing that search. The vectors are all the same. So if you want to do a text to text search with with CLIP you could. You want to do image to image search you could. If you want to do image to text or all of those things all at once you could. It is not as you're searching through vectors. So what is behind those vectors doesn't really matter so much. OK. So I think that's it for this video. I think CLIP is super interesting and I hope that you do as well in the future or very soon actually.,Fast intro to multi-modal ML with OpenAI's CLIP,https://youtu.be/989aKUVBfbk
